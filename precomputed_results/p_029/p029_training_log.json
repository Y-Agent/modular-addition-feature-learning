{"standard": {"config": {"prime": 29, "d_mlp": 814, "act_type": "ReLU", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 5e-05, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=29, run=standard\n======================================================================\n\nConfiguration:\n  prime (p)       = 29\n  d_mlp           = 814\n  activation      = ReLU\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 5e-05\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.367505      3.367505      0.0357      0.0357      0.0166       4.0534\n      50      3.361738      3.361738      0.1046      0.1046      0.0164       4.1020\n     100      3.355029      3.355029      0.2687      0.2687      0.0173       4.2812\n     150      3.347543      3.347543      0.4899      0.4899      0.0189       4.5558\n     200      3.338959      3.338959      0.6813      0.6813      0.0213       4.8950\n     250      3.328995      3.328995      0.8026      0.8026      0.0242       5.2769\n     300      3.317429      3.317429      0.8823      0.8823      0.0276       5.6870\n     350      3.304145      3.304145      0.9251      0.9251      0.0313       6.1151\n     400      3.289083      3.289083      0.9584      0.9584      0.0351       6.5556\n     450      3.272178      3.272178      0.9786      0.9786      0.0392       7.0055\n     500      3.253420      3.253420      0.9786      0.9786      0.0432       7.4626\n     550      3.232780      3.232780      0.9786      0.9786      0.0474       7.9253\n     600      3.210243      3.210243      0.9774      0.9774      0.0517       8.3926\n     650      3.185796      3.185796      0.9774      0.9774      0.0560       8.8637\n     700      3.159448      3.159448      0.9786      0.9786      0.0603       9.3380\n     750      3.131183      3.131183      0.9810      0.9810      0.0646       9.8153\n     800      3.101013      3.101013      0.9810      0.9810      0.0690      10.2950\n     850      3.068944      3.068944      0.9810      0.9810      0.0733      10.7767\n     900      3.034985      3.034985      0.9810      0.9810      0.0776      11.2601\n     950      2.999133      2.999133      0.9822      0.9822      0.0819      11.7451\n    1000      2.961427      2.961427      0.9822      0.9822      0.0862      12.2312\n    1050      2.921860      2.921860      0.9834      0.9834      0.0906      12.7184\n    1100      2.880416      2.880416      0.9822      0.9822      0.0949      13.2065\n    1150      2.837116      2.837116      0.9822      0.9822      0.0991      13.6955\n    1200      2.791992      2.791992      0.9822      0.9822      0.1033      14.1854\n    1250      2.745075      2.745075      0.9822      0.9822      0.1075      14.6757\n    1300      2.696375      2.696375      0.9834      0.9834      0.1116      15.1665\n    1350      2.645943      2.645943      0.9834      0.9834      0.1157      15.6579\n    1400      2.593854      2.593854      0.9834      0.9834      0.1197      16.1498\n    1450      2.540108      2.540108      0.9834      0.9834      0.1236      16.6417\n    1500      2.484703      2.484703      0.9834      0.9834      0.1275      17.1334\n    1550      2.427702      2.427702      0.9834      0.9834      0.1312      17.6248\n    1600      2.369196      2.369196      0.9845      0.9845      0.1348      18.1163\n    1650      2.309238      2.309238      0.9845      0.9845      0.1382      18.6079\n    1700      2.247915      2.247915      0.9845      0.9845      0.1416      19.0997\n    1750      2.185241      2.185241      0.9845      0.9845      0.1448      19.5917\n    1800      2.121281      2.121281      0.9845      0.9845      0.1478      20.0834\n    1850      2.056149      2.056149      0.9845      0.9845      0.1506      20.5746\n    1900      1.989894      1.989894      0.9845      0.9845      0.1533      21.0652\n    1950      1.922660      1.922660      0.9857      0.9857      0.1556      21.5554\n    2000      1.854597      1.854597      0.9857      0.9857      0.1578      22.0449\n    2050      1.785812      1.785812      0.9869      0.9869      0.1596      22.5334\n    2100      1.716459      1.716459      0.9869      0.9869      0.1611      23.0212\n    2150      1.646701      1.646701      0.9869      0.9869      0.1622      23.5083\n    2200      1.576685      1.576685      0.9869      0.9869      0.1631      23.9943\n    2250      1.506534      1.506534      0.9893      0.9893      0.1635      24.4790\n    2300      1.436450      1.436450      0.9905      0.9905      0.1635      24.9623\n    2350      1.366620      1.366620      0.9929      0.9929      0.1633      25.4446\n    2400      1.297222      1.297222      0.9929      0.9929      0.1624      25.9257\n    2450      1.228483      1.228483      0.9941      0.9941      0.1610      26.4056\n    2500      1.160577      1.160577      0.9964      0.9964      0.1594      26.8837\n    2550      1.093745      1.093745      0.9964      0.9964      0.1570      27.3597\n    2600      1.028183      1.028183      0.9976      0.9976      0.1543      27.8338\n    2650      0.964075      0.964075      0.9976      0.9976      0.1511      28.3057\n    2700      0.901629      0.901629      0.9988      0.9988      0.1475      28.7753\n    2750      0.841027      0.841027      1.0000      1.0000      0.1433      29.2428\n    2800      0.782433      0.782433      1.0000      1.0000      0.1389      29.7077\n    2850      0.725965      0.725965      1.0000      1.0000      0.1341      30.1698\n    2900      0.671749      0.671749      1.0000      1.0000      0.1289      30.6289\n    2950      0.619901      0.619901      1.0000      1.0000      0.1236      31.0849\n    3000      0.570509      0.570509      1.0000      1.0000      0.1179      31.5378\n    3050      0.523629      0.523629      1.0000      1.0000      0.1122      31.9872\n    3100      0.479275      0.479275      1.0000      1.0000      0.1064      32.4329\n    3150      0.437442      0.437442      1.0000      1.0000      0.1005      32.8749\n    3200      0.398135      0.398135      1.0000      1.0000      0.0945      33.3133\n    3250      0.361314      0.361314      1.0000      1.0000      0.0887      33.7482\n    3300      0.326939      0.326939      1.0000      1.0000      0.0829      34.1803\n    3350      0.294984      0.294984      1.0000      1.0000      0.0773      34.6097\n    3400      0.265379      0.265379      1.0000      1.0000      0.0717      35.0374\n    3450      0.238073      0.238073      1.0000      1.0000      0.0662      35.4639\n    3500      0.212973      0.212973      1.0000      1.0000      0.0610      35.8894\n    3550      0.189995      0.189995      1.0000      1.0000      0.0560      36.3140\n    3600      0.169050      0.169050      1.0000      1.0000      0.0512      36.7383\n    3650      0.150039      0.150039      1.0000      1.0000      0.0466      37.1625\n    3700      0.132851      0.132851      1.0000      1.0000      0.0423      37.5870\n    3750      0.117366      0.117366      1.0000      1.0000      0.0382      38.0117\n    3800      0.103470      0.103470      1.0000      1.0000      0.0344      38.4365\n    3850      0.091040      0.091040      1.0000      1.0000      0.0308      38.8612\n    3900      0.079950      0.079950      1.0000      1.0000      0.0276      39.2856\n    3950      0.070085      0.070085      1.0000      1.0000      0.0246      39.7095\n    4000      0.061329      0.061329      1.0000      1.0000      0.0219      40.1328\n    4050      0.053576      0.053576      1.0000      1.0000      0.0194      40.5554\n    4100      0.046726      0.046726      1.0000      1.0000      0.0172      40.9771\n    4150      0.040690      0.040690      1.0000      1.0000      0.0152      41.3979\n    4200      0.035380      0.035380      1.0000      1.0000      0.0134      41.8177\n    4250      0.030719      0.030719      1.0000      1.0000      0.0118      42.2365\n    4300      0.026635      0.026635      1.0000      1.0000      0.0104      42.6544\n    4350      0.023062      0.023062      1.0000      1.0000      0.0091      43.0715\n    4400      0.019942      0.019942      1.0000      1.0000      0.0080      43.4877\n    4450      0.017220      0.017220      1.0000      1.0000      0.0070      43.9030\n    4500      0.014851      0.014851      1.0000      1.0000      0.0061      44.3173\n    4550      0.012791      0.012791      1.0000      1.0000      0.0053      44.7306\n    4600      0.011003      0.011003      1.0000      1.0000      0.0046      45.1431\n    4650      0.009453      0.009453      1.0000      1.0000      0.0040      45.5546\n    4700      0.008112      0.008112      1.0000      1.0000      0.0035      45.9649\n    4750      0.006952      0.006952      1.0000      1.0000      0.0030      46.3744\n    4800      0.005952      0.005952      1.0000      1.0000      0.0026      46.7830\n    4850      0.005089      0.005089      1.0000      1.0000      0.0022      47.1909\n    4900      0.004347      0.004347      1.0000      1.0000      0.0019      47.5978\n    4950      0.003709      0.003709      1.0000      1.0000      0.0017      48.0039\n    4999      0.003171      0.003171      1.0000      1.0000      0.0014      48.4010\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.003171\n  Test Loss   = 0.003171\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 48.4010\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.367505, "test_losses": 3.367505, "train_accs": 0.035672, "test_accs": 0.035672, "grad_norms": 0.016627, "param_norms": 4.053447}, {"epoch": 50, "train_losses": 3.361738, "test_losses": 3.361738, "train_accs": 0.104637, "test_accs": 0.104637, "grad_norms": 0.016413, "param_norms": 4.102048}, {"epoch": 100, "train_losses": 3.355029, "test_losses": 3.355029, "train_accs": 0.268728, "test_accs": 0.268728, "grad_norms": 0.017257, "param_norms": 4.281169}, {"epoch": 150, "train_losses": 3.347543, "test_losses": 3.347543, "train_accs": 0.489893, "test_accs": 0.489893, "grad_norms": 0.018883, "param_norms": 4.555753}, {"epoch": 200, "train_losses": 3.338959, "test_losses": 3.338959, "train_accs": 0.681332, "test_accs": 0.681332, "grad_norms": 0.021279, "param_norms": 4.894986}, {"epoch": 250, "train_losses": 3.328995, "test_losses": 3.328995, "train_accs": 0.802616, "test_accs": 0.802616, "grad_norms": 0.024191, "param_norms": 5.276889}, {"epoch": 300, "train_losses": 3.317429, "test_losses": 3.317429, "train_accs": 0.882283, "test_accs": 0.882283, "grad_norms": 0.027594, "param_norms": 5.686975}, {"epoch": 350, "train_losses": 3.304145, "test_losses": 3.304145, "train_accs": 0.925089, "test_accs": 0.925089, "grad_norms": 0.031253, "param_norms": 6.115091}, {"epoch": 400, "train_losses": 3.289083, "test_losses": 3.289083, "train_accs": 0.958383, "test_accs": 0.958383, "grad_norms": 0.035109, "param_norms": 6.555643}, {"epoch": 450, "train_losses": 3.272178, "test_losses": 3.272178, "train_accs": 0.978597, "test_accs": 0.978597, "grad_norms": 0.039156, "param_norms": 7.00551}, {"epoch": 500, "train_losses": 3.25342, "test_losses": 3.25342, "train_accs": 0.978597, "test_accs": 0.978597, "grad_norms": 0.043227, "param_norms": 7.462559}, {"epoch": 550, "train_losses": 3.23278, "test_losses": 3.23278, "train_accs": 0.978597, "test_accs": 0.978597, "grad_norms": 0.047427, "param_norms": 7.925322}, {"epoch": 600, "train_losses": 3.210243, "test_losses": 3.210243, "train_accs": 0.977408, "test_accs": 0.977408, "grad_norms": 0.05166, "param_norms": 8.392596}, {"epoch": 650, "train_losses": 3.185796, "test_losses": 3.185796, "train_accs": 0.977408, "test_accs": 0.977408, "grad_norms": 0.055959, "param_norms": 8.863669}, {"epoch": 700, "train_losses": 3.159448, "test_losses": 3.159448, "train_accs": 0.978597, "test_accs": 0.978597, "grad_norms": 0.060269, "param_norms": 9.338024}, {"epoch": 750, "train_losses": 3.131183, "test_losses": 3.131183, "train_accs": 0.980975, "test_accs": 0.980975, "grad_norms": 0.06462, "param_norms": 9.815335}, {"epoch": 800, "train_losses": 3.101013, "test_losses": 3.101013, "train_accs": 0.980975, "test_accs": 0.980975, "grad_norms": 0.068975, "param_norms": 10.294977}, {"epoch": 850, "train_losses": 3.068944, "test_losses": 3.068944, "train_accs": 0.980975, "test_accs": 0.980975, "grad_norms": 0.073276, "param_norms": 10.776652}, {"epoch": 900, "train_losses": 3.034985, "test_losses": 3.034985, "train_accs": 0.980975, "test_accs": 0.980975, "grad_norms": 0.077638, "param_norms": 11.260073}, {"epoch": 950, "train_losses": 2.999133, "test_losses": 2.999133, "train_accs": 0.982164, "test_accs": 0.982164, "grad_norms": 0.081895, "param_norms": 11.745094}, {"epoch": 1000, "train_losses": 2.961427, "test_losses": 2.961427, "train_accs": 0.982164, "test_accs": 0.982164, "grad_norms": 0.086224, "param_norms": 12.231181}, {"epoch": 1050, "train_losses": 2.92186, "test_losses": 2.92186, "train_accs": 0.983353, "test_accs": 0.983353, "grad_norms": 0.090566, "param_norms": 12.718413}, {"epoch": 1100, "train_losses": 2.880416, "test_losses": 2.880416, "train_accs": 0.982164, "test_accs": 0.982164, "grad_norms": 0.094896, "param_norms": 13.206483}, {"epoch": 1150, "train_losses": 2.837116, "test_losses": 2.837116, "train_accs": 0.982164, "test_accs": 0.982164, "grad_norms": 0.099148, "param_norms": 13.695522}, {"epoch": 1200, "train_losses": 2.791992, "test_losses": 2.791992, "train_accs": 0.982164, "test_accs": 0.982164, "grad_norms": 0.103331, "param_norms": 14.185393}, {"epoch": 1250, "train_losses": 2.745075, "test_losses": 2.745075, "train_accs": 0.982164, "test_accs": 0.982164, "grad_norms": 0.107518, "param_norms": 14.675688}, {"epoch": 1300, "train_losses": 2.696375, "test_losses": 2.696375, "train_accs": 0.983353, "test_accs": 0.983353, "grad_norms": 0.111649, "param_norms": 15.166519}, {"epoch": 1350, "train_losses": 2.645943, "test_losses": 2.645943, "train_accs": 0.983353, "test_accs": 0.983353, "grad_norms": 0.115714, "param_norms": 15.657917}, {"epoch": 1400, "train_losses": 2.593854, "test_losses": 2.593854, "train_accs": 0.983353, "test_accs": 0.983353, "grad_norms": 0.119696, "param_norms": 16.149777}, {"epoch": 1450, "train_losses": 2.540108, "test_losses": 2.540108, "train_accs": 0.983353, "test_accs": 0.983353, "grad_norms": 0.123578, "param_norms": 16.641719}, {"epoch": 1500, "train_losses": 2.484703, "test_losses": 2.484703, "train_accs": 0.983353, "test_accs": 0.983353, "grad_norms": 0.12748, "param_norms": 17.133399}, {"epoch": 1550, "train_losses": 2.427702, "test_losses": 2.427702, "train_accs": 0.983353, "test_accs": 0.983353, "grad_norms": 0.131176, "param_norms": 17.624847}, {"epoch": 1600, "train_losses": 2.369196, "test_losses": 2.369196, "train_accs": 0.984542, "test_accs": 0.984542, "grad_norms": 0.13479, "param_norms": 18.116275}, {"epoch": 1650, "train_losses": 2.309238, "test_losses": 2.309238, "train_accs": 0.984542, "test_accs": 0.984542, "grad_norms": 0.138158, "param_norms": 18.607883}, {"epoch": 1700, "train_losses": 2.247915, "test_losses": 2.247915, "train_accs": 0.984542, "test_accs": 0.984542, "grad_norms": 0.141582, "param_norms": 19.09972}, {"epoch": 1750, "train_losses": 2.185241, "test_losses": 2.185241, "train_accs": 0.984542, "test_accs": 0.984542, "grad_norms": 0.144766, "param_norms": 19.59167}, {"epoch": 1800, "train_losses": 2.121281, "test_losses": 2.121281, "train_accs": 0.984542, "test_accs": 0.984542, "grad_norms": 0.147788, "param_norms": 20.08337}, {"epoch": 1850, "train_losses": 2.056149, "test_losses": 2.056149, "train_accs": 0.984542, "test_accs": 0.984542, "grad_norms": 0.150603, "param_norms": 20.574563}, {"epoch": 1900, "train_losses": 1.989894, "test_losses": 1.989894, "train_accs": 0.984542, "test_accs": 0.984542, "grad_norms": 0.153295, "param_norms": 21.065234}, {"epoch": 1950, "train_losses": 1.92266, "test_losses": 1.92266, "train_accs": 0.985731, "test_accs": 0.985731, "grad_norms": 0.155564, "param_norms": 21.555389}, {"epoch": 2000, "train_losses": 1.854597, "test_losses": 1.854597, "train_accs": 0.985731, "test_accs": 0.985731, "grad_norms": 0.157795, "param_norms": 22.04486}, {"epoch": 2050, "train_losses": 1.785812, "test_losses": 1.785812, "train_accs": 0.98692, "test_accs": 0.98692, "grad_norms": 0.15961, "param_norms": 22.533439}, {"epoch": 2100, "train_losses": 1.716459, "test_losses": 1.716459, "train_accs": 0.98692, "test_accs": 0.98692, "grad_norms": 0.1611, "param_norms": 23.0212}, {"epoch": 2150, "train_losses": 1.646701, "test_losses": 1.646701, "train_accs": 0.98692, "test_accs": 0.98692, "grad_norms": 0.162246, "param_norms": 23.508266}, {"epoch": 2200, "train_losses": 1.576685, "test_losses": 1.576685, "train_accs": 0.98692, "test_accs": 0.98692, "grad_norms": 0.163122, "param_norms": 23.994281}, {"epoch": 2250, "train_losses": 1.506534, "test_losses": 1.506534, "train_accs": 0.989298, "test_accs": 0.989298, "grad_norms": 0.163517, "param_norms": 24.478961}, {"epoch": 2300, "train_losses": 1.43645, "test_losses": 1.43645, "train_accs": 0.990488, "test_accs": 0.990488, "grad_norms": 0.163548, "param_norms": 24.962318}, {"epoch": 2350, "train_losses": 1.36662, "test_losses": 1.36662, "train_accs": 0.992866, "test_accs": 0.992866, "grad_norms": 0.163271, "param_norms": 25.444614}, {"epoch": 2400, "train_losses": 1.297222, "test_losses": 1.297222, "train_accs": 0.992866, "test_accs": 0.992866, "grad_norms": 0.162406, "param_norms": 25.925744}, {"epoch": 2450, "train_losses": 1.228483, "test_losses": 1.228483, "train_accs": 0.994055, "test_accs": 0.994055, "grad_norms": 0.161048, "param_norms": 26.405599}, {"epoch": 2500, "train_losses": 1.160577, "test_losses": 1.160577, "train_accs": 0.996433, "test_accs": 0.996433, "grad_norms": 0.159359, "param_norms": 26.883685}, {"epoch": 2550, "train_losses": 1.093745, "test_losses": 1.093745, "train_accs": 0.996433, "test_accs": 0.996433, "grad_norms": 0.156982, "param_norms": 27.359709}, {"epoch": 2600, "train_losses": 1.028183, "test_losses": 1.028183, "train_accs": 0.997622, "test_accs": 0.997622, "grad_norms": 0.154269, "param_norms": 27.833837}, {"epoch": 2650, "train_losses": 0.964075, "test_losses": 0.964075, "train_accs": 0.997622, "test_accs": 0.997622, "grad_norms": 0.151145, "param_norms": 28.305677}, {"epoch": 2700, "train_losses": 0.901629, "test_losses": 0.901629, "train_accs": 0.998811, "test_accs": 0.998811, "grad_norms": 0.147481, "param_norms": 28.775272}, {"epoch": 2750, "train_losses": 0.841027, "test_losses": 0.841027, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.143279, "param_norms": 29.242797}, {"epoch": 2800, "train_losses": 0.782433, "test_losses": 0.782433, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.138924, "param_norms": 29.707729}, {"epoch": 2850, "train_losses": 0.725965, "test_losses": 0.725965, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.13412, "param_norms": 30.169822}, {"epoch": 2900, "train_losses": 0.671749, "test_losses": 0.671749, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.12887, "param_norms": 30.628906}, {"epoch": 2950, "train_losses": 0.619901, "test_losses": 0.619901, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.123586, "param_norms": 31.084933}, {"epoch": 3000, "train_losses": 0.570509, "test_losses": 0.570509, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.117921, "param_norms": 31.537791}, {"epoch": 3050, "train_losses": 0.523629, "test_losses": 0.523629, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.112178, "param_norms": 31.987181}, {"epoch": 3100, "train_losses": 0.479275, "test_losses": 0.479275, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.10643, "param_norms": 32.432933}, {"epoch": 3150, "train_losses": 0.437442, "test_losses": 0.437442, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.100462, "param_norms": 32.874937}, {"epoch": 3200, "train_losses": 0.398135, "test_losses": 0.398135, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.094523, "param_norms": 33.313291}, {"epoch": 3250, "train_losses": 0.361314, "test_losses": 0.361314, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.088718, "param_norms": 33.748249}, {"epoch": 3300, "train_losses": 0.326939, "test_losses": 0.326939, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.082889, "param_norms": 34.180285}, {"epoch": 3350, "train_losses": 0.294984, "test_losses": 0.294984, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.077254, "param_norms": 34.609723}, {"epoch": 3400, "train_losses": 0.265379, "test_losses": 0.265379, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.071734, "param_norms": 35.037397}, {"epoch": 3450, "train_losses": 0.238073, "test_losses": 0.238073, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.066248, "param_norms": 35.463946}, {"epoch": 3500, "train_losses": 0.212973, "test_losses": 0.212973, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.061034, "param_norms": 35.889379}, {"epoch": 3550, "train_losses": 0.189995, "test_losses": 0.189995, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.056006, "param_norms": 36.31395}, {"epoch": 3600, "train_losses": 0.16905, "test_losses": 0.16905, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.051229, "param_norms": 36.738256}, {"epoch": 3650, "train_losses": 0.150039, "test_losses": 0.150039, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.046562, "param_norms": 37.162518}, {"epoch": 3700, "train_losses": 0.132851, "test_losses": 0.132851, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.042253, "param_norms": 37.586962}, {"epoch": 3750, "train_losses": 0.117366, "test_losses": 0.117366, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.038152, "param_norms": 38.011717}, {"epoch": 3800, "train_losses": 0.10347, "test_losses": 0.10347, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.034359, "param_norms": 38.436526}, {"epoch": 3850, "train_losses": 0.09104, "test_losses": 0.09104, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.030829, "param_norms": 38.86118}, {"epoch": 3900, "train_losses": 0.07995, "test_losses": 0.07995, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.027577, "param_norms": 39.285577}, {"epoch": 3950, "train_losses": 0.070085, "test_losses": 0.070085, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.024606, "param_norms": 39.709469}, {"epoch": 4000, "train_losses": 0.061329, "test_losses": 0.061329, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.021894, "param_norms": 40.132753}, {"epoch": 4050, "train_losses": 0.053576, "test_losses": 0.053576, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.019446, "param_norms": 40.555375}, {"epoch": 4100, "train_losses": 0.046726, "test_losses": 0.046726, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.017218, "param_norms": 40.977098}, {"epoch": 4150, "train_losses": 0.04069, "test_losses": 0.04069, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.015206, "param_norms": 41.397898}, {"epoch": 4200, "train_losses": 0.03538, "test_losses": 0.03538, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013404, "param_norms": 41.81767}, {"epoch": 4250, "train_losses": 0.030719, "test_losses": 0.030719, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01181, "param_norms": 42.236501}, {"epoch": 4300, "train_losses": 0.026635, "test_losses": 0.026635, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010364, "param_norms": 42.654425}, {"epoch": 4350, "train_losses": 0.023062, "test_losses": 0.023062, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009086, "param_norms": 43.071497}, {"epoch": 4400, "train_losses": 0.019942, "test_losses": 0.019942, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007952, "param_norms": 43.487724}, {"epoch": 4450, "train_losses": 0.01722, "test_losses": 0.01722, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006951, "param_norms": 43.90305}, {"epoch": 4500, "train_losses": 0.014851, "test_losses": 0.014851, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006064, "param_norms": 44.317346}, {"epoch": 4550, "train_losses": 0.012791, "test_losses": 0.012791, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005284, "param_norms": 44.730608}, {"epoch": 4600, "train_losses": 0.011003, "test_losses": 0.011003, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004596, "param_norms": 45.1431}, {"epoch": 4650, "train_losses": 0.009453, "test_losses": 0.009453, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003995, "param_norms": 45.554558}, {"epoch": 4700, "train_losses": 0.008112, "test_losses": 0.008112, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003463, "param_norms": 45.96491}, {"epoch": 4750, "train_losses": 0.006952, "test_losses": 0.006952, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003, "param_norms": 46.374393}, {"epoch": 4800, "train_losses": 0.005952, "test_losses": 0.005952, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002594, "param_norms": 46.783021}, {"epoch": 4850, "train_losses": 0.005089, "test_losses": 0.005089, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002243, "param_norms": 47.190886}, {"epoch": 4900, "train_losses": 0.004347, "test_losses": 0.004347, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001931, "param_norms": 47.597779}, {"epoch": 4950, "train_losses": 0.003709, "test_losses": 0.003709, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001666, "param_norms": 48.003885}, {"epoch": 4999, "train_losses": 0.003171, "test_losses": 0.003171, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001437, "param_norms": 48.400995}], "total_epochs": 5000}, "grokking": {"config": {"prime": 29, "d_mlp": 814, "act_type": "ReLU", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 0.0001, "weight_decay": 2.0, "frac_train": 0.75, "num_epochs": 50000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=29, run=grokking\n======================================================================\n\nConfiguration:\n  prime (p)       = 29\n  d_mlp           = 814\n  activation      = ReLU\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 0.0001\n  weight_decay    = 2.0\n  frac_train      = 0.75\n  num_epochs      = 50000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.367376      3.367888      0.0381      0.0284      0.0219       4.0534\n     190      3.281096      3.431323      0.2492      0.0000      0.0371       6.1431\n     380      3.107883      3.487839      0.6032      0.0047      0.0646       9.5421\n     570      2.855770      3.502432      0.7143      0.0047      0.0912      12.9209\n     760      2.541790      3.471561      0.7238      0.0095      0.1141      16.1827\n     950      2.185595      3.385047      0.7444      0.0664      0.1320      19.2812\n    1140      1.809192      3.229661      0.8159      0.2559      0.1429      22.1849\n    1330      1.436823      2.994188      0.9333      0.5972      0.1447      24.8782\n    1520      1.090341      2.674394      0.9889      0.7536      0.1379      27.3554\n    1710      0.784443      2.294713      1.0000      0.7867      0.1240      29.6381\n    1900      0.530470      1.912153      1.0000      0.7867      0.1042      31.7775\n    2090      0.336674      1.589061      1.0000      0.7867      0.0804      33.8164\n    2280      0.202804      1.355842      1.0000      0.7867      0.0566      35.7641\n    2470      0.118128      1.202932      1.0000      0.7867      0.0371      37.6204\n    2660      0.067507      1.101313      1.0000      0.7867      0.0232      39.3814\n    2850      0.038166      1.028516      1.0000      0.7867      0.0140      41.0535\n    3040      0.021472      0.971217      1.0000      0.7867      0.0084      42.6481\n    3230      0.012060      0.920178      1.0000      0.7962      0.0049      44.1748\n    3420      0.006776      0.871972      1.0000      0.8057      0.0029      45.6388\n    3610      0.003812      0.823837      1.0000      0.8152      0.0017      47.0444\n    3800      0.002151      0.775736      1.0000      0.8246      0.0010      48.3944\n    3990      0.001219      0.727454      1.0000      0.8626      0.0006      49.6897\n    4180      0.000694      0.679658      1.0000      0.8815      0.0003      50.9311\n    4370      0.000399      0.634287      1.0000      0.9005      0.0002      52.1157\n    4560      0.000232      0.591218      1.0000      0.9100      0.0001      53.2376\n    4750      0.000137      0.550757      1.0000      0.9100      0.0001      54.2853\n    4940      0.000082      0.511985      1.0000      0.9194      0.0000      55.2458\n    5130      0.000051      0.475704      1.0000      0.9194      0.0000      56.1029\n    5320      0.000033      0.442504      1.0000      0.9289      0.0000      56.8382\n    5510      0.000022      0.412204      1.0000      0.9289      0.0000      57.4383\n    5700      0.000016      0.384426      1.0000      0.9289      0.0000      57.8950\n    5890      0.000012      0.359085      1.0000      0.9384      0.0000      58.2114\n    6080      0.000010      0.335802      1.0000      0.9479      0.0000      58.4026\n    6270      0.000008      0.314425      1.0000      0.9479      0.0000      58.4918\n    6460      0.000007      0.294882      1.0000      0.9573      0.0000      58.5011\n    6650      0.000007      0.277182      1.0000      0.9573      0.0000      58.4534\n    6840      0.000006      0.261376      1.0000      0.9573      0.0000      58.3668\n    7030      0.000006      0.247019      1.0000      0.9573      0.0000      58.2544\n    7220      0.000006      0.234050      1.0000      0.9573      0.0000      58.1276\n    7410      0.000006      0.222153      1.0000      0.9573      0.0000      57.9933\n    7600      0.000006      0.211290      1.0000      0.9573      0.0000      57.8558\n    7790      0.000006      0.201216      1.0000      0.9573      0.0000      57.7194\n    7980      0.000006      0.191915      1.0000      0.9573      0.0000      57.5866\n    8170      0.000006      0.183473      1.0000      0.9573      0.0000      57.4577\n    8360      0.000006      0.175750      1.0000      0.9573      0.0000      57.3339\n    8550      0.000005      0.168739      1.0000      0.9573      0.0000      57.2154\n    8740      0.000005      0.162270      1.0000      0.9621      0.0000      57.1028\n    8930      0.000005      0.156270      1.0000      0.9621      0.0000      56.9954\n    9120      0.000005      0.150708      1.0000      0.9621      0.0000      56.8937\n    9310      0.000005      0.145627      1.0000      0.9621      0.0000      56.7976\n    9500      0.000005      0.141017      1.0000      0.9621      0.0000      56.7057\n    9690      0.000005      0.136640      1.0000      0.9621      0.0000      56.6183\n    9880      0.000005      0.132576      1.0000      0.9668      0.0000      56.5360\n   10070      0.000005      0.128895      1.0000      0.9668      0.0000      56.4577\n   10260      0.000005      0.125481      1.0000      0.9668      0.0000      56.3833\n   10450      0.000005      0.122465      1.0000      0.9668      0.0000      56.3122\n   10640      0.000005      0.119711      1.0000      0.9716      0.0000      56.2442\n   10830      0.000005      0.117191      1.0000      0.9716      0.0000      56.1796\n   11020      0.000005      0.114887      1.0000      0.9763      0.0000      56.1183\n   11210      0.000005      0.112762      1.0000      0.9810      0.0000      56.0597\n   11400      0.000005      0.110717      1.0000      0.9810      0.0000      56.0035\n   11590      0.000005      0.108785      1.0000      0.9810      0.0000      55.9498\n   11780      0.000005      0.107000      1.0000      0.9810      0.0000      55.8989\n   11970      0.000005      0.105327      1.0000      0.9810      0.0000      55.8497\n   12160      0.000005      0.103726      1.0000      0.9810      0.0000      55.8029\n   12350      0.000005      0.102243      1.0000      0.9810      0.0000      55.7584\n   12540      0.000005      0.100856      1.0000      0.9858      0.0000      55.7164\n   12730      0.000005      0.099661      1.0000      0.9858      0.0000      55.6762\n   12920      0.000005      0.098542      1.0000      0.9858      0.0000      55.6379\n   13110      0.000005      0.097495      1.0000      0.9858      0.0000      55.6008\n   13300      0.000005      0.096530      1.0000      0.9858      0.0000      55.5652\n   13490      0.000005      0.095564      1.0000      0.9858      0.0000      55.5313\n   13680      0.000005      0.094632      1.0000      0.9858      0.0000      55.4991\n   13870      0.000005      0.093796      1.0000      0.9858      0.0000      55.4684\n   14060      0.000005      0.093026      1.0000      0.9905      0.0000      55.4391\n   14250      0.000005      0.092337      1.0000      0.9905      0.0000      55.4110\n   14440      0.000005      0.091740      1.0000      0.9905      0.0000      55.3840\n   14630      0.000005      0.091157      1.0000      0.9905      0.0000      55.3581\n   14820      0.000005      0.090624      1.0000      0.9905      0.0000      55.3332\n   15010      0.000005      0.090147      1.0000      0.9905      0.0000      55.3095\n   15200      0.000005      0.089728      1.0000      0.9905      0.0000      55.2866\n   15390      0.000005      0.089341      1.0000      0.9905      0.0000      55.2646\n   15580      0.000005      0.088971      1.0000      0.9905      0.0000      55.2435\n   15770      0.000005      0.088640      1.0000      0.9905      0.0000      55.2234\n   15960      0.000005      0.088346      1.0000      0.9905      0.0000      55.2042\n   16150      0.000005      0.088087      1.0000      0.9905      0.0000      55.1856\n   16340      0.000005      0.087855      1.0000      0.9905      0.0000      55.1680\n   16530      0.000005      0.087659      1.0000      0.9905      0.0000      55.1509\n   16720      0.000005      0.087462      1.0000      0.9905      0.0000      55.1347\n   16910      0.000005      0.087248      1.0000      0.9905      0.0000      55.1193\n   17100      0.000005      0.087038      1.0000      0.9905      0.0000      55.1044\n   17290      0.000005      0.086839      1.0000      0.9905      0.0000      55.0902\n   17480      0.000005      0.086660      1.0000      0.9905      0.0000      55.0764\n   17670      0.000005      0.086484      1.0000      0.9905      0.0000      55.0631\n   17860      0.000005      0.086303      1.0000      0.9905      0.0000      55.0503\n   18050      0.000005      0.086100      1.0000      0.9905      0.0000      55.0380\n   18240      0.000005      0.085926      1.0000      0.9905      0.0000      55.0260\n   18430      0.000005      0.085791      1.0000      0.9905      0.0000      55.0147\n   18620      0.000005      0.085658      1.0000      0.9953      0.0000      55.0037\n   18810      0.000005      0.085558      1.0000      0.9953      0.0000      54.9933\n   19000      0.000005      0.085461      1.0000      0.9953      0.0000      54.9834\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.000005\n  Test Loss   = 0.085461\n  Train Acc   = 1.0000\n  Test Acc    = 0.9953\n  Param Norm  = 54.9834\n\nTotal epochs trained: 19001\n", "table": [{"epoch": 0, "train_losses": 3.367376, "test_losses": 3.367888, "train_accs": 0.038095, "test_accs": 0.028436, "grad_norms": 0.02187, "param_norms": 4.053447}, {"epoch": 190, "train_losses": 3.281096, "test_losses": 3.431323, "train_accs": 0.249206, "test_accs": 0.0, "grad_norms": 0.037125, "param_norms": 6.14308}, {"epoch": 380, "train_losses": 3.107883, "test_losses": 3.487839, "train_accs": 0.603175, "test_accs": 0.004739, "grad_norms": 0.064605, "param_norms": 9.542093}, {"epoch": 570, "train_losses": 2.85577, "test_losses": 3.502432, "train_accs": 0.714286, "test_accs": 0.004739, "grad_norms": 0.091237, "param_norms": 12.920923}, {"epoch": 760, "train_losses": 2.54179, "test_losses": 3.471561, "train_accs": 0.72381, "test_accs": 0.009479, "grad_norms": 0.114138, "param_norms": 16.182679}, {"epoch": 950, "train_losses": 2.185595, "test_losses": 3.385047, "train_accs": 0.744444, "test_accs": 0.066351, "grad_norms": 0.131975, "param_norms": 19.281205}, {"epoch": 1140, "train_losses": 1.809192, "test_losses": 3.229661, "train_accs": 0.815873, "test_accs": 0.255924, "grad_norms": 0.142862, "param_norms": 22.184939}, {"epoch": 1330, "train_losses": 1.436823, "test_losses": 2.994188, "train_accs": 0.933333, "test_accs": 0.597156, "grad_norms": 0.1447, "param_norms": 24.878169}, {"epoch": 1520, "train_losses": 1.090341, "test_losses": 2.674394, "train_accs": 0.988889, "test_accs": 0.753555, "grad_norms": 0.137906, "param_norms": 27.355367}, {"epoch": 1710, "train_losses": 0.784443, "test_losses": 2.294713, "train_accs": 1.0, "test_accs": 0.78673, "grad_norms": 0.123985, "param_norms": 29.638122}, {"epoch": 1900, "train_losses": 0.53047, "test_losses": 1.912153, "train_accs": 1.0, "test_accs": 0.78673, "grad_norms": 0.104216, "param_norms": 31.777502}, {"epoch": 2090, "train_losses": 0.336674, "test_losses": 1.589061, "train_accs": 1.0, "test_accs": 0.78673, "grad_norms": 0.080421, "param_norms": 33.816393}, {"epoch": 2280, "train_losses": 0.202804, "test_losses": 1.355842, "train_accs": 1.0, "test_accs": 0.78673, "grad_norms": 0.056642, "param_norms": 35.76415}, {"epoch": 2470, "train_losses": 0.118128, "test_losses": 1.202932, "train_accs": 1.0, "test_accs": 0.78673, "grad_norms": 0.037061, "param_norms": 37.620356}, {"epoch": 2660, "train_losses": 0.067507, "test_losses": 1.101313, "train_accs": 1.0, "test_accs": 0.78673, "grad_norms": 0.023165, "param_norms": 39.381382}, {"epoch": 2850, "train_losses": 0.038166, "test_losses": 1.028516, "train_accs": 1.0, "test_accs": 0.78673, "grad_norms": 0.014042, "param_norms": 41.053514}, {"epoch": 3040, "train_losses": 0.021472, "test_losses": 0.971217, "train_accs": 1.0, "test_accs": 0.78673, "grad_norms": 0.00836, "param_norms": 42.648117}, {"epoch": 3230, "train_losses": 0.01206, "test_losses": 0.920178, "train_accs": 1.0, "test_accs": 0.796209, "grad_norms": 0.00494, "param_norms": 44.174753}, {"epoch": 3420, "train_losses": 0.006776, "test_losses": 0.871972, "train_accs": 1.0, "test_accs": 0.805687, "grad_norms": 0.002895, "param_norms": 45.638799}, {"epoch": 3610, "train_losses": 0.003812, "test_losses": 0.823837, "train_accs": 1.0, "test_accs": 0.815166, "grad_norms": 0.001696, "param_norms": 47.044387}, {"epoch": 3800, "train_losses": 0.002151, "test_losses": 0.775736, "train_accs": 1.0, "test_accs": 0.824645, "grad_norms": 0.000992, "param_norms": 48.394406}, {"epoch": 3990, "train_losses": 0.001219, "test_losses": 0.727454, "train_accs": 1.0, "test_accs": 0.862559, "grad_norms": 0.000582, "param_norms": 49.689728}, {"epoch": 4180, "train_losses": 0.000694, "test_losses": 0.679658, "train_accs": 1.0, "test_accs": 0.881517, "grad_norms": 0.000342, "param_norms": 50.931114}, {"epoch": 4370, "train_losses": 0.000399, "test_losses": 0.634287, "train_accs": 1.0, "test_accs": 0.900474, "grad_norms": 0.000202, "param_norms": 52.11566}, {"epoch": 4560, "train_losses": 0.000232, "test_losses": 0.591218, "train_accs": 1.0, "test_accs": 0.909953, "grad_norms": 0.000121, "param_norms": 53.237622}, {"epoch": 4750, "train_losses": 0.000137, "test_losses": 0.550757, "train_accs": 1.0, "test_accs": 0.909953, "grad_norms": 7.3e-05, "param_norms": 54.285328}, {"epoch": 4940, "train_losses": 8.2e-05, "test_losses": 0.511985, "train_accs": 1.0, "test_accs": 0.919431, "grad_norms": 4.5e-05, "param_norms": 55.245827}, {"epoch": 5130, "train_losses": 5.1e-05, "test_losses": 0.475704, "train_accs": 1.0, "test_accs": 0.919431, "grad_norms": 2.8e-05, "param_norms": 56.10292}, {"epoch": 5320, "train_losses": 3.3e-05, "test_losses": 0.442504, "train_accs": 1.0, "test_accs": 0.92891, "grad_norms": 1.9e-05, "param_norms": 56.838191}, {"epoch": 5510, "train_losses": 2.2e-05, "test_losses": 0.412204, "train_accs": 1.0, "test_accs": 0.92891, "grad_norms": 1.3e-05, "param_norms": 57.43832}, {"epoch": 5700, "train_losses": 1.6e-05, "test_losses": 0.384426, "train_accs": 1.0, "test_accs": 0.92891, "grad_norms": 9e-06, "param_norms": 57.895038}, {"epoch": 5890, "train_losses": 1.2e-05, "test_losses": 0.359085, "train_accs": 1.0, "test_accs": 0.938389, "grad_norms": 7e-06, "param_norms": 58.211354}, {"epoch": 6080, "train_losses": 1e-05, "test_losses": 0.335802, "train_accs": 1.0, "test_accs": 0.947867, "grad_norms": 6e-06, "param_norms": 58.402605}, {"epoch": 6270, "train_losses": 8e-06, "test_losses": 0.314425, "train_accs": 1.0, "test_accs": 0.947867, "grad_norms": 5e-06, "param_norms": 58.491774}, {"epoch": 6460, "train_losses": 7e-06, "test_losses": 0.294882, "train_accs": 1.0, "test_accs": 0.957346, "grad_norms": 4e-06, "param_norms": 58.50114}, {"epoch": 6650, "train_losses": 7e-06, "test_losses": 0.277182, "train_accs": 1.0, "test_accs": 0.957346, "grad_norms": 4e-06, "param_norms": 58.453416}, {"epoch": 6840, "train_losses": 6e-06, "test_losses": 0.261376, "train_accs": 1.0, "test_accs": 0.957346, "grad_norms": 4e-06, "param_norms": 58.366818}, {"epoch": 7030, "train_losses": 6e-06, "test_losses": 0.247019, "train_accs": 1.0, "test_accs": 0.957346, "grad_norms": 4e-06, "param_norms": 58.254409}, {"epoch": 7220, "train_losses": 6e-06, "test_losses": 0.23405, "train_accs": 1.0, "test_accs": 0.957346, "grad_norms": 4e-06, "param_norms": 58.127552}, {"epoch": 7410, "train_losses": 6e-06, "test_losses": 0.222153, "train_accs": 1.0, "test_accs": 0.957346, "grad_norms": 3e-06, "param_norms": 57.993279}, {"epoch": 7600, "train_losses": 6e-06, "test_losses": 0.21129, "train_accs": 1.0, "test_accs": 0.957346, "grad_norms": 3e-06, "param_norms": 57.855781}, {"epoch": 7790, "train_losses": 6e-06, "test_losses": 0.201216, "train_accs": 1.0, "test_accs": 0.957346, "grad_norms": 3e-06, "param_norms": 57.719381}, {"epoch": 7980, "train_losses": 6e-06, "test_losses": 0.191915, "train_accs": 1.0, "test_accs": 0.957346, "grad_norms": 3e-06, "param_norms": 57.586563}, {"epoch": 8170, "train_losses": 6e-06, "test_losses": 0.183473, "train_accs": 1.0, "test_accs": 0.957346, "grad_norms": 3e-06, "param_norms": 57.45774}, {"epoch": 8360, "train_losses": 6e-06, "test_losses": 0.17575, "train_accs": 1.0, "test_accs": 0.957346, "grad_norms": 3e-06, "param_norms": 57.33387}, {"epoch": 8550, "train_losses": 5e-06, "test_losses": 0.168739, "train_accs": 1.0, "test_accs": 0.957346, "grad_norms": 3e-06, "param_norms": 57.215444}, {"epoch": 8740, "train_losses": 5e-06, "test_losses": 0.16227, "train_accs": 1.0, "test_accs": 0.962085, "grad_norms": 3e-06, "param_norms": 57.102775}, {"epoch": 8930, "train_losses": 5e-06, "test_losses": 0.15627, "train_accs": 1.0, "test_accs": 0.962085, "grad_norms": 3e-06, "param_norms": 56.995371}, {"epoch": 9120, "train_losses": 5e-06, "test_losses": 0.150708, "train_accs": 1.0, "test_accs": 0.962085, "grad_norms": 3e-06, "param_norms": 56.893703}, {"epoch": 9310, "train_losses": 5e-06, "test_losses": 0.145627, "train_accs": 1.0, "test_accs": 0.962085, "grad_norms": 3e-06, "param_norms": 56.797611}, {"epoch": 9500, "train_losses": 5e-06, "test_losses": 0.141017, "train_accs": 1.0, "test_accs": 0.962085, "grad_norms": 3e-06, "param_norms": 56.70572}, {"epoch": 9690, "train_losses": 5e-06, "test_losses": 0.13664, "train_accs": 1.0, "test_accs": 0.962085, "grad_norms": 3e-06, "param_norms": 56.618314}, {"epoch": 9880, "train_losses": 5e-06, "test_losses": 0.132576, "train_accs": 1.0, "test_accs": 0.966825, "grad_norms": 3e-06, "param_norms": 56.535994}, {"epoch": 10070, "train_losses": 5e-06, "test_losses": 0.128895, "train_accs": 1.0, "test_accs": 0.966825, "grad_norms": 3e-06, "param_norms": 56.457741}, {"epoch": 10260, "train_losses": 5e-06, "test_losses": 0.125481, "train_accs": 1.0, "test_accs": 0.966825, "grad_norms": 3e-06, "param_norms": 56.383347}, {"epoch": 10450, "train_losses": 5e-06, "test_losses": 0.122465, "train_accs": 1.0, "test_accs": 0.966825, "grad_norms": 3e-06, "param_norms": 56.312208}, {"epoch": 10640, "train_losses": 5e-06, "test_losses": 0.119711, "train_accs": 1.0, "test_accs": 0.971564, "grad_norms": 3e-06, "param_norms": 56.244173}, {"epoch": 10830, "train_losses": 5e-06, "test_losses": 0.117191, "train_accs": 1.0, "test_accs": 0.971564, "grad_norms": 3e-06, "param_norms": 56.179601}, {"epoch": 11020, "train_losses": 5e-06, "test_losses": 0.114887, "train_accs": 1.0, "test_accs": 0.976303, "grad_norms": 3e-06, "param_norms": 56.118259}, {"epoch": 11210, "train_losses": 5e-06, "test_losses": 0.112762, "train_accs": 1.0, "test_accs": 0.981043, "grad_norms": 3e-06, "param_norms": 56.05966}, {"epoch": 11400, "train_losses": 5e-06, "test_losses": 0.110717, "train_accs": 1.0, "test_accs": 0.981043, "grad_norms": 3e-06, "param_norms": 56.003538}, {"epoch": 11590, "train_losses": 5e-06, "test_losses": 0.108785, "train_accs": 1.0, "test_accs": 0.981043, "grad_norms": 3e-06, "param_norms": 55.94984}, {"epoch": 11780, "train_losses": 5e-06, "test_losses": 0.107, "train_accs": 1.0, "test_accs": 0.981043, "grad_norms": 3e-06, "param_norms": 55.898886}, {"epoch": 11970, "train_losses": 5e-06, "test_losses": 0.105327, "train_accs": 1.0, "test_accs": 0.981043, "grad_norms": 3e-06, "param_norms": 55.849734}, {"epoch": 12160, "train_losses": 5e-06, "test_losses": 0.103726, "train_accs": 1.0, "test_accs": 0.981043, "grad_norms": 3e-06, "param_norms": 55.80293}, {"epoch": 12350, "train_losses": 5e-06, "test_losses": 0.102243, "train_accs": 1.0, "test_accs": 0.981043, "grad_norms": 3e-06, "param_norms": 55.758441}, {"epoch": 12540, "train_losses": 5e-06, "test_losses": 0.100856, "train_accs": 1.0, "test_accs": 0.985782, "grad_norms": 3e-06, "param_norms": 55.71637}, {"epoch": 12730, "train_losses": 5e-06, "test_losses": 0.099661, "train_accs": 1.0, "test_accs": 0.985782, "grad_norms": 3e-06, "param_norms": 55.676196}, {"epoch": 12920, "train_losses": 5e-06, "test_losses": 0.098542, "train_accs": 1.0, "test_accs": 0.985782, "grad_norms": 3e-06, "param_norms": 55.637877}, {"epoch": 13110, "train_losses": 5e-06, "test_losses": 0.097495, "train_accs": 1.0, "test_accs": 0.985782, "grad_norms": 3e-06, "param_norms": 55.600794}, {"epoch": 13300, "train_losses": 5e-06, "test_losses": 0.09653, "train_accs": 1.0, "test_accs": 0.985782, "grad_norms": 3e-06, "param_norms": 55.565189}, {"epoch": 13490, "train_losses": 5e-06, "test_losses": 0.095564, "train_accs": 1.0, "test_accs": 0.985782, "grad_norms": 3e-06, "param_norms": 55.531273}, {"epoch": 13680, "train_losses": 5e-06, "test_losses": 0.094632, "train_accs": 1.0, "test_accs": 0.985782, "grad_norms": 3e-06, "param_norms": 55.499089}, {"epoch": 13870, "train_losses": 5e-06, "test_losses": 0.093796, "train_accs": 1.0, "test_accs": 0.985782, "grad_norms": 3e-06, "param_norms": 55.468364}, {"epoch": 14060, "train_losses": 5e-06, "test_losses": 0.093026, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.439111}, {"epoch": 14250, "train_losses": 5e-06, "test_losses": 0.092337, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.410984}, {"epoch": 14440, "train_losses": 5e-06, "test_losses": 0.09174, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.384002}, {"epoch": 14630, "train_losses": 5e-06, "test_losses": 0.091157, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.358127}, {"epoch": 14820, "train_losses": 5e-06, "test_losses": 0.090624, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.333227}, {"epoch": 15010, "train_losses": 5e-06, "test_losses": 0.090147, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.309529}, {"epoch": 15200, "train_losses": 5e-06, "test_losses": 0.089728, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.286608}, {"epoch": 15390, "train_losses": 5e-06, "test_losses": 0.089341, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.264627}, {"epoch": 15580, "train_losses": 5e-06, "test_losses": 0.088971, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.243467}, {"epoch": 15770, "train_losses": 5e-06, "test_losses": 0.08864, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.223379}, {"epoch": 15960, "train_losses": 5e-06, "test_losses": 0.088346, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.204192}, {"epoch": 16150, "train_losses": 5e-06, "test_losses": 0.088087, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.185628}, {"epoch": 16340, "train_losses": 5e-06, "test_losses": 0.087855, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.167985}, {"epoch": 16530, "train_losses": 5e-06, "test_losses": 0.087659, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.150926}, {"epoch": 16720, "train_losses": 5e-06, "test_losses": 0.087462, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.134734}, {"epoch": 16910, "train_losses": 5e-06, "test_losses": 0.087248, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.119344}, {"epoch": 17100, "train_losses": 5e-06, "test_losses": 0.087038, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.104436}, {"epoch": 17290, "train_losses": 5e-06, "test_losses": 0.086839, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.090166}, {"epoch": 17480, "train_losses": 5e-06, "test_losses": 0.08666, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.076396}, {"epoch": 17670, "train_losses": 5e-06, "test_losses": 0.086484, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.063136}, {"epoch": 17860, "train_losses": 5e-06, "test_losses": 0.086303, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.050291}, {"epoch": 18050, "train_losses": 5e-06, "test_losses": 0.0861, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.037958}, {"epoch": 18240, "train_losses": 5e-06, "test_losses": 0.085926, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.026027}, {"epoch": 18430, "train_losses": 5e-06, "test_losses": 0.085791, "train_accs": 1.0, "test_accs": 0.990521, "grad_norms": 3e-06, "param_norms": 55.014658}, {"epoch": 18620, "train_losses": 5e-06, "test_losses": 0.085658, "train_accs": 1.0, "test_accs": 0.995261, "grad_norms": 3e-06, "param_norms": 55.003722}, {"epoch": 18810, "train_losses": 5e-06, "test_losses": 0.085558, "train_accs": 1.0, "test_accs": 0.995261, "grad_norms": 3e-06, "param_norms": 54.993272}, {"epoch": 19000, "train_losses": 5e-06, "test_losses": 0.085461, "train_accs": 1.0, "test_accs": 0.995261, "grad_norms": 3e-06, "param_norms": 54.983381}], "total_epochs": 19001}, "quad_random": {"config": {"prime": 29, "d_mlp": 814, "act_type": "Quad", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 5e-05, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=29, run=quad_random\n======================================================================\n\nConfiguration:\n  prime (p)       = 29\n  d_mlp           = 814\n  activation      = Quad\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 5e-05\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.367312      3.367312      0.0345      0.0345      0.0017       4.0534\n      50      3.366676      3.366676      0.1403      0.1403      0.0018       4.1199\n     100      3.365811      3.365811      0.4031      0.4031      0.0021       4.3479\n     150      3.364646      3.364646      0.7206      0.7206      0.0027       4.6835\n     200      3.363065      3.363065      0.8775      0.8775      0.0036       5.0842\n     250      3.360972      3.360972      0.9631      0.9631      0.0046       5.5232\n     300      3.358275      3.358275      0.9869      0.9869      0.0059       5.9846\n     350      3.354892      3.354892      0.9917      0.9917      0.0074       6.4595\n     400      3.350739      3.350739      0.9941      0.9941      0.0090       6.9431\n     450      3.345732      3.345732      0.9941      0.9941      0.0109       7.4328\n     500      3.339788      3.339788      0.9941      0.9941      0.0129       7.9270\n     550      3.332824      3.332824      0.9941      0.9941      0.0151       8.4248\n     600      3.324757      3.324757      0.9941      0.9941      0.0175       8.9256\n     650      3.315507      3.315507      0.9941      0.9941      0.0201       9.4287\n     700      3.304992      3.304992      0.9941      0.9941      0.0229       9.9341\n     750      3.293136      3.293136      0.9941      0.9941      0.0258      10.4417\n     800      3.279860      3.279860      0.9941      0.9941      0.0289      10.9511\n     850      3.265085      3.265085      0.9952      0.9952      0.0322      11.4623\n     900      3.248737      3.248737      0.9952      0.9952      0.0357      11.9753\n     950      3.230739      3.230739      0.9964      0.9964      0.0393      12.4897\n    1000      3.211017      3.211017      0.9964      0.9964      0.0431      13.0055\n    1050      3.189499      3.189499      0.9964      0.9964      0.0470      13.5223\n    1100      3.166111      3.166111      0.9964      0.9964      0.0511      14.0401\n    1150      3.140784      3.140784      0.9964      0.9964      0.0554      14.5590\n    1200      3.113450      3.113450      0.9976      0.9976      0.0598      15.0789\n    1250      3.084040      3.084040      0.9976      0.9976      0.0644      15.5997\n    1300      3.052491      3.052491      0.9976      0.9976      0.0691      16.1214\n    1350      3.018743      3.018743      0.9976      0.9976      0.0740      16.6439\n    1400      2.982734      2.982734      0.9976      0.9976      0.0790      17.1671\n    1450      2.944407      2.944407      0.9976      0.9976      0.0841      17.6910\n    1500      2.903711      2.903711      0.9976      0.9976      0.0894      18.2153\n    1550      2.860594      2.860594      0.9976      0.9976      0.0947      18.7400\n    1600      2.815011      2.815011      0.9988      0.9988      0.1002      19.2652\n    1650      2.766922      2.766922      0.9988      0.9988      0.1058      19.7908\n    1700      2.716292      2.716292      0.9988      0.9988      0.1114      20.3167\n    1750      2.663092      2.663092      0.9988      0.9988      0.1172      20.8429\n    1800      2.607300      2.607300      1.0000      1.0000      0.1229      21.3693\n    1850      2.548902      2.548902      1.0000      1.0000      0.1288      21.8958\n    1900      2.487896      2.487896      1.0000      1.0000      0.1346      22.4224\n    1950      2.424289      2.424289      1.0000      1.0000      0.1404      22.9491\n    2000      2.358101      2.358101      1.0000      1.0000      0.1462      23.4757\n    2050      2.289367      2.289367      1.0000      1.0000      0.1520      24.0021\n    2100      2.218138      2.218138      1.0000      1.0000      0.1576      24.5283\n    2150      2.144484      2.144484      1.0000      1.0000      0.1631      25.0542\n    2200      2.068497      2.068497      1.0000      1.0000      0.1684      25.5797\n    2250      1.990291      1.990291      1.0000      1.0000      0.1735      26.1047\n    2300      1.910011      1.910011      1.0000      1.0000      0.1783      26.6292\n    2350      1.827830      1.827830      1.0000      1.0000      0.1827      27.1532\n    2400      1.743955      1.743955      1.0000      1.0000      0.1867      27.6762\n    2450      1.658627      1.658627      1.0000      1.0000      0.1902      28.1981\n    2500      1.572129      1.572129      1.0000      1.0000      0.1932      28.7185\n    2550      1.484781      1.484781      1.0000      1.0000      0.1954      29.2373\n    2600      1.396946      1.396946      1.0000      1.0000      0.1969      29.7543\n    2650      1.309026      1.309026      1.0000      1.0000      0.1976      30.2690\n    2700      1.221460      1.221460      1.0000      1.0000      0.1973      30.7812\n    2750      1.134720      1.134720      1.0000      1.0000      0.1960      31.2906\n    2800      1.049303      1.049303      1.0000      1.0000      0.1937      31.7968\n    2850      0.965717      0.965717      1.0000      1.0000      0.1904      32.2993\n    2900      0.884471      0.884471      1.0000      1.0000      0.1859      32.7978\n    2950      0.806056      0.806056      1.0000      1.0000      0.1804      33.2917\n    3000      0.730927      0.730927      1.0000      1.0000      0.1738      33.7803\n    3050      0.659489      0.659489      1.0000      1.0000      0.1664      34.2630\n    3100      0.592074      0.592074      1.0000      1.0000      0.1583      34.7390\n    3150      0.528933      0.528933      1.0000      1.0000      0.1495      35.2076\n    3200      0.470220      0.470220      1.0000      1.0000      0.1403      35.6679\n    3250      0.416000      0.416000      1.0000      1.0000      0.1308      36.1192\n    3300      0.366247      0.366247      1.0000      1.0000      0.1212      36.5608\n    3350      0.320864      0.320864      1.0000      1.0000      0.1117      36.9926\n    3400      0.279700      0.279700      1.0000      1.0000      0.1023      37.4149\n    3450      0.242571      0.242571      1.0000      1.0000      0.0930      37.8284\n    3500      0.209275      0.209275      1.0000      1.0000      0.0841      38.2344\n    3550      0.179607      0.179607      1.0000      1.0000      0.0755      38.6348\n    3600      0.153355      0.153355      1.0000      1.0000      0.0672      39.0318\n    3650      0.130297      0.130297      1.0000      1.0000      0.0594      39.4273\n    3700      0.110193      0.110193      1.0000      1.0000      0.0520      39.8234\n    3750      0.092786      0.092786      1.0000      1.0000      0.0453      40.2214\n    3800      0.077810      0.077810      1.0000      1.0000      0.0391      40.6215\n    3850      0.065000      0.065000      1.0000      1.0000      0.0336      41.0234\n    3900      0.054100      0.054100      1.0000      1.0000      0.0288      41.4263\n    3950      0.044872      0.044872      1.0000      1.0000      0.0245      41.8298\n    4000      0.037093      0.037093      1.0000      1.0000      0.0207      42.2335\n    4050      0.030564      0.030564      1.0000      1.0000      0.0175      42.6370\n    4100      0.025106      0.025106      1.0000      1.0000      0.0147      43.0401\n    4150      0.020561      0.020561      1.0000      1.0000      0.0123      43.4425\n    4200      0.016789      0.016789      1.0000      1.0000      0.0102      43.8442\n    4250      0.013670      0.013670      1.0000      1.0000      0.0085      44.2449\n    4300      0.011100      0.011100      1.0000      1.0000      0.0070      44.6448\n    4350      0.008989      0.008989      1.0000      1.0000      0.0058      45.0436\n    4400      0.007260      0.007260      1.0000      1.0000      0.0047      45.4413\n    4450      0.005848      0.005848      1.0000      1.0000      0.0039      45.8378\n    4500      0.004699      0.004699      1.0000      1.0000      0.0032      46.2331\n    4550      0.003766      0.003766      1.0000      1.0000      0.0026      46.6271\n    4600      0.003011      0.003011      1.0000      1.0000      0.0021      47.0197\n    4650      0.002402      0.002402      1.0000      1.0000      0.0017      47.4110\n    4700      0.001911      0.001911      1.0000      1.0000      0.0014      47.8010\n    4750      0.001517      0.001517      1.0000      1.0000      0.0011      48.1895\n    4800      0.001202      0.001202      1.0000      1.0000      0.0009      48.5766\n    4850      0.000950      0.000950      1.0000      1.0000      0.0007      48.9622\n    4900      0.000749      0.000749      1.0000      1.0000      0.0006      49.3463\n    4950      0.000589      0.000589      1.0000      1.0000      0.0005      49.7287\n    4999      0.000465      0.000465      1.0000      1.0000      0.0004      50.1018\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.000465\n  Test Loss   = 0.000465\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 50.1018\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.367312, "test_losses": 3.367312, "train_accs": 0.034483, "test_accs": 0.034483, "grad_norms": 0.001651, "param_norms": 4.053447}, {"epoch": 50, "train_losses": 3.366676, "test_losses": 3.366676, "train_accs": 0.140309, "test_accs": 0.140309, "grad_norms": 0.001752, "param_norms": 4.119938}, {"epoch": 100, "train_losses": 3.365811, "test_losses": 3.365811, "train_accs": 0.403092, "test_accs": 0.403092, "grad_norms": 0.002123, "param_norms": 4.347888}, {"epoch": 150, "train_losses": 3.364646, "test_losses": 3.364646, "train_accs": 0.720571, "test_accs": 0.720571, "grad_norms": 0.002737, "param_norms": 4.683508}, {"epoch": 200, "train_losses": 3.363065, "test_losses": 3.363065, "train_accs": 0.877527, "test_accs": 0.877527, "grad_norms": 0.00358, "param_norms": 5.084209}, {"epoch": 250, "train_losses": 3.360972, "test_losses": 3.360972, "train_accs": 0.963139, "test_accs": 0.963139, "grad_norms": 0.004641, "param_norms": 5.523195}, {"epoch": 300, "train_losses": 3.358275, "test_losses": 3.358275, "train_accs": 0.98692, "test_accs": 0.98692, "grad_norms": 0.005907, "param_norms": 5.984594}, {"epoch": 350, "train_losses": 3.354892, "test_losses": 3.354892, "train_accs": 0.991677, "test_accs": 0.991677, "grad_norms": 0.007371, "param_norms": 6.459511}, {"epoch": 400, "train_losses": 3.350739, "test_losses": 3.350739, "train_accs": 0.994055, "test_accs": 0.994055, "grad_norms": 0.009029, "param_norms": 6.943051}, {"epoch": 450, "train_losses": 3.345732, "test_losses": 3.345732, "train_accs": 0.994055, "test_accs": 0.994055, "grad_norms": 0.010876, "param_norms": 7.432758}, {"epoch": 500, "train_losses": 3.339788, "test_losses": 3.339788, "train_accs": 0.994055, "test_accs": 0.994055, "grad_norms": 0.01291, "param_norms": 7.92702}, {"epoch": 550, "train_losses": 3.332824, "test_losses": 3.332824, "train_accs": 0.994055, "test_accs": 0.994055, "grad_norms": 0.015128, "param_norms": 8.424826}, {"epoch": 600, "train_losses": 3.324757, "test_losses": 3.324757, "train_accs": 0.994055, "test_accs": 0.994055, "grad_norms": 0.017529, "param_norms": 8.925561}, {"epoch": 650, "train_losses": 3.315507, "test_losses": 3.315507, "train_accs": 0.994055, "test_accs": 0.994055, "grad_norms": 0.020111, "param_norms": 9.428715}, {"epoch": 700, "train_losses": 3.304992, "test_losses": 3.304992, "train_accs": 0.994055, "test_accs": 0.994055, "grad_norms": 0.022872, "param_norms": 9.934137}, {"epoch": 750, "train_losses": 3.293136, "test_losses": 3.293136, "train_accs": 0.994055, "test_accs": 0.994055, "grad_norms": 0.025809, "param_norms": 10.441678}, {"epoch": 800, "train_losses": 3.27986, "test_losses": 3.27986, "train_accs": 0.994055, "test_accs": 0.994055, "grad_norms": 0.02892, "param_norms": 10.951102}, {"epoch": 850, "train_losses": 3.265085, "test_losses": 3.265085, "train_accs": 0.995244, "test_accs": 0.995244, "grad_norms": 0.032204, "param_norms": 11.462339}, {"epoch": 900, "train_losses": 3.248737, "test_losses": 3.248737, "train_accs": 0.995244, "test_accs": 0.995244, "grad_norms": 0.035658, "param_norms": 11.975261}, {"epoch": 950, "train_losses": 3.230739, "test_losses": 3.230739, "train_accs": 0.996433, "test_accs": 0.996433, "grad_norms": 0.039281, "param_norms": 12.489707}, {"epoch": 1000, "train_losses": 3.211017, "test_losses": 3.211017, "train_accs": 0.996433, "test_accs": 0.996433, "grad_norms": 0.043069, "param_norms": 13.005458}, {"epoch": 1050, "train_losses": 3.189499, "test_losses": 3.189499, "train_accs": 0.996433, "test_accs": 0.996433, "grad_norms": 0.047021, "param_norms": 13.522286}, {"epoch": 1100, "train_losses": 3.166111, "test_losses": 3.166111, "train_accs": 0.996433, "test_accs": 0.996433, "grad_norms": 0.051134, "param_norms": 14.04014}, {"epoch": 1150, "train_losses": 3.140784, "test_losses": 3.140784, "train_accs": 0.996433, "test_accs": 0.996433, "grad_norms": 0.055405, "param_norms": 14.559034}, {"epoch": 1200, "train_losses": 3.11345, "test_losses": 3.11345, "train_accs": 0.997622, "test_accs": 0.997622, "grad_norms": 0.05983, "param_norms": 15.078856}, {"epoch": 1250, "train_losses": 3.08404, "test_losses": 3.08404, "train_accs": 0.997622, "test_accs": 0.997622, "grad_norms": 0.064406, "param_norms": 15.599656}, {"epoch": 1300, "train_losses": 3.052491, "test_losses": 3.052491, "train_accs": 0.997622, "test_accs": 0.997622, "grad_norms": 0.069128, "param_norms": 16.121397}, {"epoch": 1350, "train_losses": 3.018743, "test_losses": 3.018743, "train_accs": 0.997622, "test_accs": 0.997622, "grad_norms": 0.073991, "param_norms": 16.643909}, {"epoch": 1400, "train_losses": 2.982734, "test_losses": 2.982734, "train_accs": 0.997622, "test_accs": 0.997622, "grad_norms": 0.07899, "param_norms": 17.167141}, {"epoch": 1450, "train_losses": 2.944407, "test_losses": 2.944407, "train_accs": 0.997622, "test_accs": 0.997622, "grad_norms": 0.08412, "param_norms": 17.690988}, {"epoch": 1500, "train_losses": 2.903711, "test_losses": 2.903711, "train_accs": 0.997622, "test_accs": 0.997622, "grad_norms": 0.089373, "param_norms": 18.215293}, {"epoch": 1550, "train_losses": 2.860594, "test_losses": 2.860594, "train_accs": 0.997622, "test_accs": 0.997622, "grad_norms": 0.094741, "param_norms": 18.740041}, {"epoch": 1600, "train_losses": 2.815011, "test_losses": 2.815011, "train_accs": 0.998811, "test_accs": 0.998811, "grad_norms": 0.100215, "param_norms": 19.265203}, {"epoch": 1650, "train_losses": 2.766922, "test_losses": 2.766922, "train_accs": 0.998811, "test_accs": 0.998811, "grad_norms": 0.105785, "param_norms": 19.790762}, {"epoch": 1700, "train_losses": 2.716292, "test_losses": 2.716292, "train_accs": 0.998811, "test_accs": 0.998811, "grad_norms": 0.111439, "param_norms": 20.31667}, {"epoch": 1750, "train_losses": 2.663092, "test_losses": 2.663092, "train_accs": 0.998811, "test_accs": 0.998811, "grad_norms": 0.117165, "param_norms": 20.842872}, {"epoch": 1800, "train_losses": 2.6073, "test_losses": 2.6073, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.122946, "param_norms": 21.369259}, {"epoch": 1850, "train_losses": 2.548902, "test_losses": 2.548902, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.128765, "param_norms": 21.895792}, {"epoch": 1900, "train_losses": 2.487896, "test_losses": 2.487896, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.134601, "param_norms": 22.422449}, {"epoch": 1950, "train_losses": 2.424289, "test_losses": 2.424289, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.140432, "param_norms": 22.949135}, {"epoch": 2000, "train_losses": 2.358101, "test_losses": 2.358101, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.146231, "param_norms": 23.475718}, {"epoch": 2050, "train_losses": 2.289367, "test_losses": 2.289367, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.151967, "param_norms": 24.002123}, {"epoch": 2100, "train_losses": 2.218138, "test_losses": 2.218138, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.157604, "param_norms": 24.528313}, {"epoch": 2150, "train_losses": 2.144484, "test_losses": 2.144484, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.163105, "param_norms": 25.054166}, {"epoch": 2200, "train_losses": 2.068497, "test_losses": 2.068497, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.168422, "param_norms": 25.579669}, {"epoch": 2250, "train_losses": 1.990291, "test_losses": 1.990291, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.173504, "param_norms": 26.104716}, {"epoch": 2300, "train_losses": 1.910011, "test_losses": 1.910011, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.178296, "param_norms": 26.629238}, {"epoch": 2350, "train_losses": 1.82783, "test_losses": 1.82783, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.182731, "param_norms": 27.153157}, {"epoch": 2400, "train_losses": 1.743955, "test_losses": 1.743955, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.186741, "param_norms": 27.676205}, {"epoch": 2450, "train_losses": 1.658627, "test_losses": 1.658627, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.190248, "param_norms": 28.198055}, {"epoch": 2500, "train_losses": 1.572129, "test_losses": 1.572129, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.193171, "param_norms": 28.718498}, {"epoch": 2550, "train_losses": 1.484781, "test_losses": 1.484781, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.195425, "param_norms": 29.237341}, {"epoch": 2600, "train_losses": 1.396946, "test_losses": 1.396946, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.196921, "param_norms": 29.75431}, {"epoch": 2650, "train_losses": 1.309026, "test_losses": 1.309026, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.197576, "param_norms": 30.269028}, {"epoch": 2700, "train_losses": 1.22146, "test_losses": 1.22146, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.197308, "param_norms": 30.781204}, {"epoch": 2750, "train_losses": 1.13472, "test_losses": 1.13472, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.196047, "param_norms": 31.290568}, {"epoch": 2800, "train_losses": 1.049303, "test_losses": 1.049303, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.193739, "param_norms": 31.796751}, {"epoch": 2850, "train_losses": 0.965717, "test_losses": 0.965717, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.190354, "param_norms": 32.29935}, {"epoch": 2900, "train_losses": 0.884471, "test_losses": 0.884471, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.185886, "param_norms": 32.797846}, {"epoch": 2950, "train_losses": 0.806056, "test_losses": 0.806056, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.180363, "param_norms": 33.291709}, {"epoch": 3000, "train_losses": 0.730927, "test_losses": 0.730927, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.173849, "param_norms": 33.780324}, {"epoch": 3050, "train_losses": 0.659489, "test_losses": 0.659489, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.166442, "param_norms": 34.263008}, {"epoch": 3100, "train_losses": 0.592074, "test_losses": 0.592074, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.158271, "param_norms": 34.739039}, {"epoch": 3150, "train_losses": 0.528933, "test_losses": 0.528933, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.149493, "param_norms": 35.207596}, {"epoch": 3200, "train_losses": 0.47022, "test_losses": 0.47022, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.140279, "param_norms": 35.667877}, {"epoch": 3250, "train_losses": 0.416, "test_losses": 0.416, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.130799, "param_norms": 36.11916}, {"epoch": 3300, "train_losses": 0.366247, "test_losses": 0.366247, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.121215, "param_norms": 36.5608}, {"epoch": 3350, "train_losses": 0.320864, "test_losses": 0.320864, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.111665, "param_norms": 36.992648}, {"epoch": 3400, "train_losses": 0.2797, "test_losses": 0.2797, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.102251, "param_norms": 37.414899}, {"epoch": 3450, "train_losses": 0.242571, "test_losses": 0.242571, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.093048, "param_norms": 37.828355}, {"epoch": 3500, "train_losses": 0.209275, "test_losses": 0.209275, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.084107, "param_norms": 38.234395}, {"epoch": 3550, "train_losses": 0.179607, "test_losses": 0.179607, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.075471, "param_norms": 38.634835}, {"epoch": 3600, "train_losses": 0.153355, "test_losses": 0.153355, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.067196, "param_norms": 39.03176}, {"epoch": 3650, "train_losses": 0.130297, "test_losses": 0.130297, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.059352, "param_norms": 39.427304}, {"epoch": 3700, "train_losses": 0.110193, "test_losses": 0.110193, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.05202, "param_norms": 39.823402}, {"epoch": 3750, "train_losses": 0.092786, "test_losses": 0.092786, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.04527, "param_norms": 40.221371}, {"epoch": 3800, "train_losses": 0.07781, "test_losses": 0.07781, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.039143, "param_norms": 40.621504}, {"epoch": 3850, "train_losses": 0.065, "test_losses": 0.065, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.033647, "param_norms": 41.023381}, {"epoch": 3900, "train_losses": 0.0541, "test_losses": 0.0541, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.028765, "param_norms": 41.426293}, {"epoch": 3950, "train_losses": 0.044872, "test_losses": 0.044872, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.024468, "param_norms": 41.829819}, {"epoch": 4000, "train_losses": 0.037093, "test_losses": 0.037093, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.020714, "param_norms": 42.233488}, {"epoch": 4050, "train_losses": 0.030564, "test_losses": 0.030564, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.017459, "param_norms": 42.637006}, {"epoch": 4100, "train_losses": 0.025106, "test_losses": 0.025106, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014654, "param_norms": 43.040109}, {"epoch": 4150, "train_losses": 0.020561, "test_losses": 0.020561, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012251, "param_norms": 43.442548}, {"epoch": 4200, "train_losses": 0.016789, "test_losses": 0.016789, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010204, "param_norms": 43.844158}, {"epoch": 4250, "train_losses": 0.01367, "test_losses": 0.01367, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008468, "param_norms": 44.244941}, {"epoch": 4300, "train_losses": 0.0111, "test_losses": 0.0111, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007004, "param_norms": 44.644762}, {"epoch": 4350, "train_losses": 0.008989, "test_losses": 0.008989, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005773, "param_norms": 45.043578}, {"epoch": 4400, "train_losses": 0.00726, "test_losses": 0.00726, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004744, "param_norms": 45.441285}, {"epoch": 4450, "train_losses": 0.005848, "test_losses": 0.005848, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003886, "param_norms": 45.837809}, {"epoch": 4500, "train_losses": 0.004699, "test_losses": 0.004699, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003174, "param_norms": 46.233088}, {"epoch": 4550, "train_losses": 0.003766, "test_losses": 0.003766, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002584, "param_norms": 46.62707}, {"epoch": 4600, "train_losses": 0.003011, "test_losses": 0.003011, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002099, "param_norms": 47.019714}, {"epoch": 4650, "train_losses": 0.002402, "test_losses": 0.002402, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001699, "param_norms": 47.41102}, {"epoch": 4700, "train_losses": 0.001911, "test_losses": 0.001911, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001372, "param_norms": 47.800957}, {"epoch": 4750, "train_losses": 0.001517, "test_losses": 0.001517, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001105, "param_norms": 48.189495}, {"epoch": 4800, "train_losses": 0.001202, "test_losses": 0.001202, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000888, "param_norms": 48.5766}, {"epoch": 4850, "train_losses": 0.00095, "test_losses": 0.00095, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000712, "param_norms": 48.962215}, {"epoch": 4900, "train_losses": 0.000749, "test_losses": 0.000749, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000569, "param_norms": 49.346302}, {"epoch": 4950, "train_losses": 0.000589, "test_losses": 0.000589, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000454, "param_norms": 49.728697}, {"epoch": 4999, "train_losses": 0.000465, "test_losses": 0.000465, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000363, "param_norms": 50.101772}], "total_epochs": 5000}, "quad_single_freq": {"config": {"prime": 29, "d_mlp": 814, "act_type": "Quad", "init_type": "single-freq", "init_scale": 0.02, "optimizer": "SGD", "lr": 0.1, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=29, run=quad_single_freq\n======================================================================\n\nConfiguration:\n  prime (p)       = 29\n  d_mlp           = 814\n  activation      = Quad\n  init_type       = single-freq\n  init_scale      = 0.02\n  optimizer       = SGD\n  learning_rate   = 0.1\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.367408      3.367408      0.0000      0.0000      0.0034       3.0728\n      50      3.367358      3.367358      0.0024      0.0024      0.0033       3.0725\n     100      3.367302      3.367302      0.0024      0.0024      0.0033       3.0723\n     150      3.367246      3.367246      0.0214      0.0214      0.0034       3.0724\n     200      3.367190      3.367190      0.0476      0.0476      0.0034       3.0728\n     250      3.367134      3.367134      0.0939      0.0939      0.0034       3.0734\n     300      3.367077      3.367077      0.1332      0.1332      0.0034       3.0744\n     350      3.367021      3.367021      0.2093      0.2093      0.0034       3.0756\n     400      3.366964      3.366964      0.2735      0.2735      0.0034       3.0770\n     450      3.366907      3.366907      0.3627      0.3627      0.0034       3.0788\n     500      3.366850      3.366850      0.4388      0.4388      0.0034       3.0808\n     550      3.366793      3.366793      0.5422      0.5422      0.0034       3.0831\n     600      3.366736      3.366736      0.6338      0.6338      0.0034       3.0857\n     650      3.366678      3.366678      0.7039      0.7039      0.0034       3.0886\n     700      3.366620      3.366620      0.7741      0.7741      0.0034       3.0917\n     750      3.366562      3.366562      0.8395      0.8395      0.0034       3.0951\n     800      3.366503      3.366503      0.8775      0.8775      0.0034       3.0988\n     850      3.366443      3.366443      0.9215      0.9215      0.0034       3.1028\n     900      3.366384      3.366384      0.9548      0.9548      0.0035       3.1071\n     950      3.366323      3.366323      0.9762      0.9762      0.0035       3.1116\n    1000      3.366263      3.366263      0.9905      0.9905      0.0035       3.1164\n    1050      3.366202      3.366202      0.9929      0.9929      0.0035       3.1215\n    1100      3.366140      3.366140      0.9976      0.9976      0.0035       3.1269\n    1150      3.366077      3.366077      0.9976      0.9976      0.0035       3.1326\n    1200      3.366015      3.366015      1.0000      1.0000      0.0036       3.1386\n    1250      3.365951      3.365951      1.0000      1.0000      0.0036       3.1449\n    1300      3.365886      3.365886      1.0000      1.0000      0.0036       3.1514\n    1350      3.365821      3.365821      1.0000      1.0000      0.0036       3.1583\n    1400      3.365755      3.365755      1.0000      1.0000      0.0036       3.1654\n    1450      3.365688      3.365688      1.0000      1.0000      0.0037       3.1729\n    1500      3.365620      3.365620      1.0000      1.0000      0.0037       3.1806\n    1550      3.365552      3.365552      1.0000      1.0000      0.0037       3.1887\n    1600      3.365482      3.365482      1.0000      1.0000      0.0037       3.1970\n    1650      3.365412      3.365412      1.0000      1.0000      0.0038       3.2057\n    1700      3.365340      3.365340      1.0000      1.0000      0.0038       3.2147\n    1750      3.365268      3.365268      1.0000      1.0000      0.0038       3.2239\n    1800      3.365194      3.365194      1.0000      1.0000      0.0039       3.2335\n    1850      3.365119      3.365119      1.0000      1.0000      0.0039       3.2434\n    1900      3.365042      3.365042      1.0000      1.0000      0.0039       3.2537\n    1950      3.364964      3.364964      1.0000      1.0000      0.0040       3.2642\n    2000      3.364885      3.364885      1.0000      1.0000      0.0040       3.2751\n    2050      3.364805      3.364805      1.0000      1.0000      0.0040       3.2863\n    2100      3.364723      3.364723      1.0000      1.0000      0.0041       3.2978\n    2150      3.364640      3.364640      1.0000      1.0000      0.0041       3.3097\n    2200      3.364554      3.364554      1.0000      1.0000      0.0041       3.3219\n    2250      3.364468      3.364468      1.0000      1.0000      0.0042       3.3344\n    2300      3.364379      3.364379      1.0000      1.0000      0.0042       3.3473\n    2350      3.364289      3.364289      1.0000      1.0000      0.0043       3.3606\n    2400      3.364197      3.364197      1.0000      1.0000      0.0043       3.3742\n    2450      3.364103      3.364103      1.0000      1.0000      0.0044       3.3881\n    2500      3.364007      3.364007      1.0000      1.0000      0.0044       3.4024\n    2550      3.363908      3.363908      1.0000      1.0000      0.0045       3.4171\n    2600      3.363808      3.363808      1.0000      1.0000      0.0045       3.4322\n    2650      3.363705      3.363705      1.0000      1.0000      0.0046       3.4476\n    2700      3.363600      3.363600      1.0000      1.0000      0.0046       3.4634\n    2750      3.363492      3.363492      1.0000      1.0000      0.0047       3.4796\n    2800      3.363382      3.363382      1.0000      1.0000      0.0047       3.4962\n    2850      3.363269      3.363269      1.0000      1.0000      0.0048       3.5132\n    2900      3.363153      3.363153      1.0000      1.0000      0.0048       3.5306\n    2950      3.363034      3.363034      1.0000      1.0000      0.0049       3.5484\n    3000      3.362913      3.362913      1.0000      1.0000      0.0050       3.5666\n    3050      3.362788      3.362788      1.0000      1.0000      0.0050       3.5852\n    3100      3.362659      3.362659      1.0000      1.0000      0.0051       3.6043\n    3150      3.362528      3.362528      1.0000      1.0000      0.0052       3.6238\n    3200      3.362392      3.362392      1.0000      1.0000      0.0052       3.6438\n    3250      3.362254      3.362254      1.0000      1.0000      0.0053       3.6642\n    3300      3.362111      3.362111      1.0000      1.0000      0.0054       3.6850\n    3350      3.361964      3.361964      1.0000      1.0000      0.0055       3.7064\n    3400      3.361813      3.361813      1.0000      1.0000      0.0055       3.7282\n    3450      3.361657      3.361657      1.0000      1.0000      0.0056       3.7505\n    3500      3.361497      3.361497      1.0000      1.0000      0.0057       3.7733\n    3550      3.361332      3.361332      1.0000      1.0000      0.0058       3.7966\n    3600      3.361162      3.361162      1.0000      1.0000      0.0059       3.8204\n    3650      3.360987      3.360987      1.0000      1.0000      0.0060       3.8447\n    3700      3.360806      3.360806      1.0000      1.0000      0.0061       3.8696\n    3750      3.360620      3.360620      1.0000      1.0000      0.0062       3.8950\n    3800      3.360427      3.360427      1.0000      1.0000      0.0063       3.9210\n    3850      3.360229      3.360229      1.0000      1.0000      0.0064       3.9475\n    3900      3.360023      3.360023      1.0000      1.0000      0.0065       3.9747\n    3950      3.359811      3.359811      1.0000      1.0000      0.0066       4.0024\n    4000      3.359592      3.359592      1.0000      1.0000      0.0067       4.0308\n    4050      3.359365      3.359365      1.0000      1.0000      0.0068       4.0597\n    4100      3.359131      3.359131      1.0000      1.0000      0.0069       4.0893\n    4150      3.358888      3.358888      1.0000      1.0000      0.0070       4.1196\n    4200      3.358637      3.358637      1.0000      1.0000      0.0072       4.1505\n    4250      3.358376      3.358376      1.0000      1.0000      0.0073       4.1822\n    4300      3.358106      3.358106      1.0000      1.0000      0.0074       4.2145\n    4350      3.357826      3.357826      1.0000      1.0000      0.0076       4.2476\n    4400      3.357536      3.357536      1.0000      1.0000      0.0077       4.2814\n    4450      3.357235      3.357235      1.0000      1.0000      0.0078       4.3159\n    4500      3.356922      3.356922      1.0000      1.0000      0.0080       4.3513\n    4550      3.356597      3.356597      1.0000      1.0000      0.0081       4.3874\n    4600      3.356259      3.356259      1.0000      1.0000      0.0083       4.4244\n    4650      3.355908      3.355908      1.0000      1.0000      0.0085       4.4622\n    4700      3.355542      3.355542      1.0000      1.0000      0.0086       4.5009\n    4750      3.355162      3.355162      1.0000      1.0000      0.0088       4.5405\n    4800      3.354766      3.354766      1.0000      1.0000      0.0090       4.5810\n    4850      3.354353      3.354353      1.0000      1.0000      0.0092       4.6225\n    4900      3.353923      3.353923      1.0000      1.0000      0.0094       4.6650\n    4950      3.353474      3.353474      1.0000      1.0000      0.0096       4.7085\n    4999      3.353015      3.353015      1.0000      1.0000      0.0098       4.7521\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 3.353015\n  Test Loss   = 3.353015\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 4.7521\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.367408, "test_losses": 3.367408, "train_accs": 0.0, "test_accs": 0.0, "grad_norms": 0.003351, "param_norms": 3.072849}, {"epoch": 50, "train_losses": 3.367358, "test_losses": 3.367358, "train_accs": 0.002378, "test_accs": 0.002378, "grad_norms": 0.00335, "param_norms": 3.072467}, {"epoch": 100, "train_losses": 3.367302, "test_losses": 3.367302, "train_accs": 0.002378, "test_accs": 0.002378, "grad_norms": 0.00335, "param_norms": 3.072299}, {"epoch": 150, "train_losses": 3.367246, "test_losses": 3.367246, "train_accs": 0.021403, "test_accs": 0.021403, "grad_norms": 0.003351, "param_norms": 3.072404}, {"epoch": 200, "train_losses": 3.36719, "test_losses": 3.36719, "train_accs": 0.047562, "test_accs": 0.047562, "grad_norms": 0.003352, "param_norms": 3.072784}, {"epoch": 250, "train_losses": 3.367134, "test_losses": 3.367134, "train_accs": 0.093936, "test_accs": 0.093936, "grad_norms": 0.003354, "param_norms": 3.073438}, {"epoch": 300, "train_losses": 3.367077, "test_losses": 3.367077, "train_accs": 0.133175, "test_accs": 0.133175, "grad_norms": 0.003358, "param_norms": 3.074365}, {"epoch": 350, "train_losses": 3.367021, "test_losses": 3.367021, "train_accs": 0.209275, "test_accs": 0.209275, "grad_norms": 0.003362, "param_norms": 3.075569}, {"epoch": 400, "train_losses": 3.366964, "test_losses": 3.366964, "train_accs": 0.273484, "test_accs": 0.273484, "grad_norms": 0.003366, "param_norms": 3.077045}, {"epoch": 450, "train_losses": 3.366907, "test_losses": 3.366907, "train_accs": 0.362663, "test_accs": 0.362663, "grad_norms": 0.003372, "param_norms": 3.078798}, {"epoch": 500, "train_losses": 3.36685, "test_losses": 3.36685, "train_accs": 0.438763, "test_accs": 0.438763, "grad_norms": 0.003379, "param_norms": 3.080827}, {"epoch": 550, "train_losses": 3.366793, "test_losses": 3.366793, "train_accs": 0.542212, "test_accs": 0.542212, "grad_norms": 0.003386, "param_norms": 3.083132}, {"epoch": 600, "train_losses": 3.366736, "test_losses": 3.366736, "train_accs": 0.633769, "test_accs": 0.633769, "grad_norms": 0.003394, "param_norms": 3.085714}, {"epoch": 650, "train_losses": 3.366678, "test_losses": 3.366678, "train_accs": 0.703924, "test_accs": 0.703924, "grad_norms": 0.003403, "param_norms": 3.088572}, {"epoch": 700, "train_losses": 3.36662, "test_losses": 3.36662, "train_accs": 0.774078, "test_accs": 0.774078, "grad_norms": 0.003413, "param_norms": 3.091711}, {"epoch": 750, "train_losses": 3.366562, "test_losses": 3.366562, "train_accs": 0.839477, "test_accs": 0.839477, "grad_norms": 0.003424, "param_norms": 3.095127}, {"epoch": 800, "train_losses": 3.366503, "test_losses": 3.366503, "train_accs": 0.877527, "test_accs": 0.877527, "grad_norms": 0.003436, "param_norms": 3.098824}, {"epoch": 850, "train_losses": 3.366443, "test_losses": 3.366443, "train_accs": 0.921522, "test_accs": 0.921522, "grad_norms": 0.003448, "param_norms": 3.102802}, {"epoch": 900, "train_losses": 3.366384, "test_losses": 3.366384, "train_accs": 0.954816, "test_accs": 0.954816, "grad_norms": 0.003461, "param_norms": 3.107061}, {"epoch": 950, "train_losses": 3.366323, "test_losses": 3.366323, "train_accs": 0.976219, "test_accs": 0.976219, "grad_norms": 0.003476, "param_norms": 3.111604}, {"epoch": 1000, "train_losses": 3.366263, "test_losses": 3.366263, "train_accs": 0.990488, "test_accs": 0.990488, "grad_norms": 0.003491, "param_norms": 3.116432}, {"epoch": 1050, "train_losses": 3.366202, "test_losses": 3.366202, "train_accs": 0.992866, "test_accs": 0.992866, "grad_norms": 0.003507, "param_norms": 3.121543}, {"epoch": 1100, "train_losses": 3.36614, "test_losses": 3.36614, "train_accs": 0.997622, "test_accs": 0.997622, "grad_norms": 0.003524, "param_norms": 3.126941}, {"epoch": 1150, "train_losses": 3.366077, "test_losses": 3.366077, "train_accs": 0.997622, "test_accs": 0.997622, "grad_norms": 0.003541, "param_norms": 3.132627}, {"epoch": 1200, "train_losses": 3.366015, "test_losses": 3.366015, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00356, "param_norms": 3.138603}, {"epoch": 1250, "train_losses": 3.365951, "test_losses": 3.365951, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00358, "param_norms": 3.14487}, {"epoch": 1300, "train_losses": 3.365886, "test_losses": 3.365886, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003601, "param_norms": 3.151429}, {"epoch": 1350, "train_losses": 3.365821, "test_losses": 3.365821, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003622, "param_norms": 3.158282}, {"epoch": 1400, "train_losses": 3.365755, "test_losses": 3.365755, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003645, "param_norms": 3.165432}, {"epoch": 1450, "train_losses": 3.365688, "test_losses": 3.365688, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003668, "param_norms": 3.172878}, {"epoch": 1500, "train_losses": 3.36562, "test_losses": 3.36562, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003692, "param_norms": 3.180625}, {"epoch": 1550, "train_losses": 3.365552, "test_losses": 3.365552, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003718, "param_norms": 3.188673}, {"epoch": 1600, "train_losses": 3.365482, "test_losses": 3.365482, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003744, "param_norms": 3.197028}, {"epoch": 1650, "train_losses": 3.365412, "test_losses": 3.365412, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003772, "param_norms": 3.205686}, {"epoch": 1700, "train_losses": 3.36534, "test_losses": 3.36534, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0038, "param_norms": 3.214653}, {"epoch": 1750, "train_losses": 3.365268, "test_losses": 3.365268, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00383, "param_norms": 3.223931}, {"epoch": 1800, "train_losses": 3.365194, "test_losses": 3.365194, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00386, "param_norms": 3.233522}, {"epoch": 1850, "train_losses": 3.365119, "test_losses": 3.365119, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003892, "param_norms": 3.24343}, {"epoch": 1900, "train_losses": 3.365042, "test_losses": 3.365042, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003925, "param_norms": 3.253655}, {"epoch": 1950, "train_losses": 3.364964, "test_losses": 3.364964, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003958, "param_norms": 3.264206}, {"epoch": 2000, "train_losses": 3.364885, "test_losses": 3.364885, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003993, "param_norms": 3.27508}, {"epoch": 2050, "train_losses": 3.364805, "test_losses": 3.364805, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004029, "param_norms": 3.28628}, {"epoch": 2100, "train_losses": 3.364723, "test_losses": 3.364723, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004067, "param_norms": 3.297812}, {"epoch": 2150, "train_losses": 3.36464, "test_losses": 3.36464, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004105, "param_norms": 3.309679}, {"epoch": 2200, "train_losses": 3.364554, "test_losses": 3.364554, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004145, "param_norms": 3.321885}, {"epoch": 2250, "train_losses": 3.364468, "test_losses": 3.364468, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004186, "param_norms": 3.334431}, {"epoch": 2300, "train_losses": 3.364379, "test_losses": 3.364379, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004228, "param_norms": 3.347321}, {"epoch": 2350, "train_losses": 3.364289, "test_losses": 3.364289, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004271, "param_norms": 3.360564}, {"epoch": 2400, "train_losses": 3.364197, "test_losses": 3.364197, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004316, "param_norms": 3.374158}, {"epoch": 2450, "train_losses": 3.364103, "test_losses": 3.364103, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004362, "param_norms": 3.388111}, {"epoch": 2500, "train_losses": 3.364007, "test_losses": 3.364007, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00441, "param_norms": 3.402425}, {"epoch": 2550, "train_losses": 3.363908, "test_losses": 3.363908, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004458, "param_norms": 3.417105}, {"epoch": 2600, "train_losses": 3.363808, "test_losses": 3.363808, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004509, "param_norms": 3.432159}, {"epoch": 2650, "train_losses": 3.363705, "test_losses": 3.363705, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00456, "param_norms": 3.447587}, {"epoch": 2700, "train_losses": 3.3636, "test_losses": 3.3636, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004614, "param_norms": 3.463399}, {"epoch": 2750, "train_losses": 3.363492, "test_losses": 3.363492, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004668, "param_norms": 3.479594}, {"epoch": 2800, "train_losses": 3.363382, "test_losses": 3.363382, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004725, "param_norms": 3.496184}, {"epoch": 2850, "train_losses": 3.363269, "test_losses": 3.363269, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004782, "param_norms": 3.513171}, {"epoch": 2900, "train_losses": 3.363153, "test_losses": 3.363153, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004842, "param_norms": 3.530562}, {"epoch": 2950, "train_losses": 3.363034, "test_losses": 3.363034, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004903, "param_norms": 3.548364}, {"epoch": 3000, "train_losses": 3.362913, "test_losses": 3.362913, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004966, "param_norms": 3.566582}, {"epoch": 3050, "train_losses": 3.362788, "test_losses": 3.362788, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005031, "param_norms": 3.585223}, {"epoch": 3100, "train_losses": 3.362659, "test_losses": 3.362659, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005097, "param_norms": 3.604294}, {"epoch": 3150, "train_losses": 3.362528, "test_losses": 3.362528, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005166, "param_norms": 3.623801}, {"epoch": 3200, "train_losses": 3.362392, "test_losses": 3.362392, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005236, "param_norms": 3.643753}, {"epoch": 3250, "train_losses": 3.362254, "test_losses": 3.362254, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005309, "param_norms": 3.664161}, {"epoch": 3300, "train_losses": 3.362111, "test_losses": 3.362111, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005383, "param_norms": 3.685025}, {"epoch": 3350, "train_losses": 3.361964, "test_losses": 3.361964, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005459, "param_norms": 3.706359}, {"epoch": 3400, "train_losses": 3.361813, "test_losses": 3.361813, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005538, "param_norms": 3.72817}, {"epoch": 3450, "train_losses": 3.361657, "test_losses": 3.361657, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005619, "param_norms": 3.750467}, {"epoch": 3500, "train_losses": 3.361497, "test_losses": 3.361497, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005702, "param_norms": 3.773261}, {"epoch": 3550, "train_losses": 3.361332, "test_losses": 3.361332, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005788, "param_norms": 3.796558}, {"epoch": 3600, "train_losses": 3.361162, "test_losses": 3.361162, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005875, "param_norms": 3.820371}, {"epoch": 3650, "train_losses": 3.360987, "test_losses": 3.360987, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005966, "param_norms": 3.844709}, {"epoch": 3700, "train_losses": 3.360806, "test_losses": 3.360806, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006059, "param_norms": 3.869584}, {"epoch": 3750, "train_losses": 3.36062, "test_losses": 3.36062, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006155, "param_norms": 3.895006}, {"epoch": 3800, "train_losses": 3.360427, "test_losses": 3.360427, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006253, "param_norms": 3.92099}, {"epoch": 3850, "train_losses": 3.360229, "test_losses": 3.360229, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006355, "param_norms": 3.94754}, {"epoch": 3900, "train_losses": 3.360023, "test_losses": 3.360023, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006459, "param_norms": 3.974678}, {"epoch": 3950, "train_losses": 3.359811, "test_losses": 3.359811, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006566, "param_norms": 4.002414}, {"epoch": 4000, "train_losses": 3.359592, "test_losses": 3.359592, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006677, "param_norms": 4.030758}, {"epoch": 4050, "train_losses": 3.359365, "test_losses": 3.359365, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006791, "param_norms": 4.059731}, {"epoch": 4100, "train_losses": 3.359131, "test_losses": 3.359131, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006908, "param_norms": 4.08934}, {"epoch": 4150, "train_losses": 3.358888, "test_losses": 3.358888, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007029, "param_norms": 4.119607}, {"epoch": 4200, "train_losses": 3.358637, "test_losses": 3.358637, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007154, "param_norms": 4.150544}, {"epoch": 4250, "train_losses": 3.358376, "test_losses": 3.358376, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007282, "param_norms": 4.182171}, {"epoch": 4300, "train_losses": 3.358106, "test_losses": 3.358106, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007414, "param_norms": 4.2145}, {"epoch": 4350, "train_losses": 3.357826, "test_losses": 3.357826, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007551, "param_norms": 4.247556}, {"epoch": 4400, "train_losses": 3.357536, "test_losses": 3.357536, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007692, "param_norms": 4.281354}, {"epoch": 4450, "train_losses": 3.357235, "test_losses": 3.357235, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007837, "param_norms": 4.315916}, {"epoch": 4500, "train_losses": 3.356922, "test_losses": 3.356922, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007986, "param_norms": 4.351258}, {"epoch": 4550, "train_losses": 3.356597, "test_losses": 3.356597, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008141, "param_norms": 4.387407}, {"epoch": 4600, "train_losses": 3.356259, "test_losses": 3.356259, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008301, "param_norms": 4.424382}, {"epoch": 4650, "train_losses": 3.355908, "test_losses": 3.355908, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008465, "param_norms": 4.46221}, {"epoch": 4700, "train_losses": 3.355542, "test_losses": 3.355542, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008636, "param_norms": 4.500909}, {"epoch": 4750, "train_losses": 3.355162, "test_losses": 3.355162, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008811, "param_norms": 4.540513}, {"epoch": 4800, "train_losses": 3.354766, "test_losses": 3.354766, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008993, "param_norms": 4.58104}, {"epoch": 4850, "train_losses": 3.354353, "test_losses": 3.354353, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009181, "param_norms": 4.622528}, {"epoch": 4900, "train_losses": 3.353923, "test_losses": 3.353923, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009376, "param_norms": 4.664998}, {"epoch": 4950, "train_losses": 3.353474, "test_losses": 3.353474, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009577, "param_norms": 4.708483}, {"epoch": 4999, "train_losses": 3.353015, "test_losses": 3.353015, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009781, "param_norms": 4.752114}], "total_epochs": 5000}, "relu_single_freq": {"config": {"prime": 29, "d_mlp": 814, "act_type": "ReLU", "init_type": "single-freq", "init_scale": 0.002, "optimizer": "SGD", "lr": 0.01, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=29, run=relu_single_freq\n======================================================================\n\nConfiguration:\n  prime (p)       = 29\n  d_mlp           = 814\n  activation      = ReLU\n  init_type       = single-freq\n  init_scale      = 0.002\n  optimizer       = SGD\n  learning_rate   = 0.01\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.367311      3.367311      0.0166      0.0166      0.0040       0.3073\n      50      3.367304      3.367304      0.0190      0.0190      0.0040       0.3073\n     100      3.367296      3.367296      0.0309      0.0309      0.0040       0.3072\n     150      3.367289      3.367289      0.0476      0.0476      0.0040       0.3072\n     200      3.367280      3.367280      0.0606      0.0606      0.0040       0.3073\n     250      3.367273      3.367273      0.0785      0.0785      0.0040       0.3073\n     300      3.367265      3.367265      0.1023      0.1023      0.0040       0.3074\n     350      3.367257      3.367257      0.1391      0.1391      0.0040       0.3076\n     400      3.367249      3.367249      0.1795      0.1795      0.0040       0.3077\n     450      3.367241      3.367241      0.2081      0.2081      0.0040       0.3079\n     500      3.367233      3.367233      0.2497      0.2497      0.0040       0.3080\n     550      3.367225      3.367225      0.2973      0.2973      0.0040       0.3083\n     600      3.367218      3.367218      0.3579      0.3579      0.0040       0.3085\n     650      3.367210      3.367210      0.4078      0.4078      0.0040       0.3088\n     700      3.367202      3.367202      0.4768      0.4768      0.0040       0.3091\n     750      3.367194      3.367194      0.5161      0.5161      0.0040       0.3094\n     800      3.367186      3.367186      0.5731      0.5731      0.0040       0.3097\n     850      3.367178      3.367178      0.6385      0.6385      0.0040       0.3101\n     900      3.367171      3.367171      0.6861      0.6861      0.0040       0.3105\n     950      3.367162      3.367162      0.7301      0.7301      0.0040       0.3109\n    1000      3.367155      3.367155      0.7646      0.7646      0.0040       0.3113\n    1050      3.367146      3.367146      0.8062      0.8062      0.0040       0.3118\n    1100      3.367139      3.367139      0.8288      0.8288      0.0040       0.3123\n    1150      3.367131      3.367131      0.8609      0.8609      0.0040       0.3128\n    1200      3.367123      3.367123      0.8954      0.8954      0.0040       0.3133\n    1250      3.367115      3.367115      0.9180      0.9180      0.0040       0.3139\n    1300      3.367107      3.367107      0.9394      0.9394      0.0040       0.3145\n    1350      3.367100      3.367100      0.9548      0.9548      0.0040       0.3151\n    1400      3.367092      3.367092      0.9679      0.9679      0.0040       0.3157\n    1450      3.367084      3.367084      0.9774      0.9774      0.0040       0.3164\n    1500      3.367076      3.367076      0.9822      0.9822      0.0040       0.3171\n    1550      3.367069      3.367069      0.9869      0.9869      0.0040       0.3178\n    1600      3.367060      3.367060      0.9964      0.9964      0.0040       0.3185\n    1650      3.367053      3.367053      0.9988      0.9988      0.0040       0.3193\n    1700      3.367045      3.367045      0.9988      0.9988      0.0040       0.3200\n    1750      3.367037      3.367037      0.9988      0.9988      0.0040       0.3208\n    1800      3.367029      3.367029      1.0000      1.0000      0.0040       0.3216\n    1850      3.367021      3.367021      1.0000      1.0000      0.0040       0.3225\n    1900      3.367013      3.367013      1.0000      1.0000      0.0040       0.3234\n    1950      3.367005      3.367005      1.0000      1.0000      0.0040       0.3242\n    2000      3.366997      3.366997      1.0000      1.0000      0.0040       0.3251\n    2050      3.366990      3.366990      1.0000      1.0000      0.0040       0.3261\n    2100      3.366982      3.366982      1.0000      1.0000      0.0040       0.3270\n    2150      3.366974      3.366974      1.0000      1.0000      0.0040       0.3280\n    2200      3.366966      3.366966      1.0000      1.0000      0.0040       0.3290\n    2250      3.366958      3.366958      1.0000      1.0000      0.0040       0.3300\n    2300      3.366950      3.366950      1.0000      1.0000      0.0040       0.3310\n    2350      3.366942      3.366942      1.0000      1.0000      0.0041       0.3321\n    2400      3.366934      3.366934      1.0000      1.0000      0.0041       0.3332\n    2450      3.366926      3.366926      1.0000      1.0000      0.0041       0.3343\n    2500      3.366918      3.366918      1.0000      1.0000      0.0041       0.3354\n    2550      3.366910      3.366910      1.0000      1.0000      0.0041       0.3365\n    2600      3.366902      3.366902      1.0000      1.0000      0.0041       0.3377\n    2650      3.366894      3.366894      1.0000      1.0000      0.0041       0.3388\n    2700      3.366886      3.366886      1.0000      1.0000      0.0041       0.3400\n    2750      3.366878      3.366878      1.0000      1.0000      0.0041       0.3413\n    2800      3.366870      3.366870      1.0000      1.0000      0.0041       0.3425\n    2850      3.366862      3.366862      1.0000      1.0000      0.0041       0.3437\n    2900      3.366854      3.366854      1.0000      1.0000      0.0041       0.3450\n    2950      3.366846      3.366846      1.0000      1.0000      0.0041       0.3463\n    3000      3.366838      3.366838      1.0000      1.0000      0.0042       0.3476\n    3050      3.366830      3.366830      1.0000      1.0000      0.0042       0.3489\n    3100      3.366822      3.366822      1.0000      1.0000      0.0042       0.3503\n    3150      3.366814      3.366814      1.0000      1.0000      0.0042       0.3516\n    3200      3.366805      3.366805      1.0000      1.0000      0.0042       0.3530\n    3250      3.366797      3.366797      1.0000      1.0000      0.0042       0.3544\n    3300      3.366789      3.366789      1.0000      1.0000      0.0042       0.3558\n    3350      3.366781      3.366781      1.0000      1.0000      0.0042       0.3573\n    3400      3.366772      3.366772      1.0000      1.0000      0.0042       0.3587\n    3450      3.366764      3.366764      1.0000      1.0000      0.0043       0.3602\n    3500      3.366755      3.366755      1.0000      1.0000      0.0043       0.3617\n    3550      3.366747      3.366747      1.0000      1.0000      0.0043       0.3632\n    3600      3.366739      3.366739      1.0000      1.0000      0.0043       0.3647\n    3650      3.366730      3.366730      1.0000      1.0000      0.0043       0.3662\n    3700      3.366722      3.366722      1.0000      1.0000      0.0043       0.3678\n    3750      3.366713      3.366713      1.0000      1.0000      0.0043       0.3694\n    3800      3.366705      3.366705      1.0000      1.0000      0.0044       0.3709\n    3850      3.366696      3.366696      1.0000      1.0000      0.0044       0.3725\n    3900      3.366688      3.366688      1.0000      1.0000      0.0044       0.3742\n    3950      3.366679      3.366679      1.0000      1.0000      0.0044       0.3758\n    4000      3.366670      3.366670      1.0000      1.0000      0.0044       0.3774\n    4050      3.366661      3.366661      1.0000      1.0000      0.0044       0.3791\n    4100      3.366652      3.366652      1.0000      1.0000      0.0045       0.3808\n    4150      3.366644      3.366644      1.0000      1.0000      0.0045       0.3825\n    4200      3.366635      3.366635      1.0000      1.0000      0.0045       0.3842\n    4250      3.366626      3.366626      1.0000      1.0000      0.0045       0.3859\n    4300      3.366617      3.366617      1.0000      1.0000      0.0045       0.3877\n    4350      3.366607      3.366607      1.0000      1.0000      0.0045       0.3894\n    4400      3.366598      3.366598      1.0000      1.0000      0.0046       0.3912\n    4450      3.366589      3.366589      1.0000      1.0000      0.0046       0.3930\n    4500      3.366580      3.366580      1.0000      1.0000      0.0046       0.3948\n    4550      3.366571      3.366571      1.0000      1.0000      0.0046       0.3966\n    4600      3.366561      3.366561      1.0000      1.0000      0.0046       0.3985\n    4650      3.366552      3.366552      1.0000      1.0000      0.0047       0.4003\n    4700      3.366543      3.366543      1.0000      1.0000      0.0047       0.4022\n    4750      3.366534      3.366534      1.0000      1.0000      0.0047       0.4041\n    4800      3.366523      3.366523      1.0000      1.0000      0.0047       0.4059\n    4850      3.366514      3.366514      1.0000      1.0000      0.0047       0.4079\n    4900      3.366504      3.366504      1.0000      1.0000      0.0048       0.4098\n    4950      3.366495      3.366495      1.0000      1.0000      0.0048       0.4117\n    4999      3.366485      3.366485      1.0000      1.0000      0.0048       0.4136\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 3.366485\n  Test Loss   = 3.366485\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 0.4136\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.367311, "test_losses": 3.367311, "train_accs": 0.016647, "test_accs": 0.016647, "grad_norms": 0.003967, "param_norms": 0.307285}, {"epoch": 50, "train_losses": 3.367304, "test_losses": 3.367304, "train_accs": 0.019025, "test_accs": 0.019025, "grad_norms": 0.003965, "param_norms": 0.307251}, {"epoch": 100, "train_losses": 3.367296, "test_losses": 3.367296, "train_accs": 0.030916, "test_accs": 0.030916, "grad_norms": 0.003967, "param_norms": 0.307237}, {"epoch": 150, "train_losses": 3.367289, "test_losses": 3.367289, "train_accs": 0.047562, "test_accs": 0.047562, "grad_norms": 0.003966, "param_norms": 0.307249}, {"epoch": 200, "train_losses": 3.36728, "test_losses": 3.36728, "train_accs": 0.060642, "test_accs": 0.060642, "grad_norms": 0.003966, "param_norms": 0.307286}, {"epoch": 250, "train_losses": 3.367273, "test_losses": 3.367273, "train_accs": 0.078478, "test_accs": 0.078478, "grad_norms": 0.003968, "param_norms": 0.307349}, {"epoch": 300, "train_losses": 3.367265, "test_losses": 3.367265, "train_accs": 0.102259, "test_accs": 0.102259, "grad_norms": 0.003968, "param_norms": 0.307438}, {"epoch": 350, "train_losses": 3.367257, "test_losses": 3.367257, "train_accs": 0.13912, "test_accs": 0.13912, "grad_norms": 0.003966, "param_norms": 0.307552}, {"epoch": 400, "train_losses": 3.367249, "test_losses": 3.367249, "train_accs": 0.179548, "test_accs": 0.179548, "grad_norms": 0.003965, "param_norms": 0.307691}, {"epoch": 450, "train_losses": 3.367241, "test_losses": 3.367241, "train_accs": 0.208086, "test_accs": 0.208086, "grad_norms": 0.003964, "param_norms": 0.307856}, {"epoch": 500, "train_losses": 3.367233, "test_losses": 3.367233, "train_accs": 0.249703, "test_accs": 0.249703, "grad_norms": 0.003965, "param_norms": 0.308047}, {"epoch": 550, "train_losses": 3.367225, "test_losses": 3.367225, "train_accs": 0.297265, "test_accs": 0.297265, "grad_norms": 0.003966, "param_norms": 0.308263}, {"epoch": 600, "train_losses": 3.367218, "test_losses": 3.367218, "train_accs": 0.357907, "test_accs": 0.357907, "grad_norms": 0.003965, "param_norms": 0.308504}, {"epoch": 650, "train_losses": 3.36721, "test_losses": 3.36721, "train_accs": 0.407848, "test_accs": 0.407848, "grad_norms": 0.003966, "param_norms": 0.30877}, {"epoch": 700, "train_losses": 3.367202, "test_losses": 3.367202, "train_accs": 0.476813, "test_accs": 0.476813, "grad_norms": 0.003967, "param_norms": 0.309062}, {"epoch": 750, "train_losses": 3.367194, "test_losses": 3.367194, "train_accs": 0.516052, "test_accs": 0.516052, "grad_norms": 0.003968, "param_norms": 0.309378}, {"epoch": 800, "train_losses": 3.367186, "test_losses": 3.367186, "train_accs": 0.573127, "test_accs": 0.573127, "grad_norms": 0.003969, "param_norms": 0.30972}, {"epoch": 850, "train_losses": 3.367178, "test_losses": 3.367178, "train_accs": 0.638526, "test_accs": 0.638526, "grad_norms": 0.00397, "param_norms": 0.310087}, {"epoch": 900, "train_losses": 3.367171, "test_losses": 3.367171, "train_accs": 0.686088, "test_accs": 0.686088, "grad_norms": 0.003969, "param_norms": 0.310479}, {"epoch": 950, "train_losses": 3.367162, "test_losses": 3.367162, "train_accs": 0.730083, "test_accs": 0.730083, "grad_norms": 0.00397, "param_norms": 0.310895}, {"epoch": 1000, "train_losses": 3.367155, "test_losses": 3.367155, "train_accs": 0.764566, "test_accs": 0.764566, "grad_norms": 0.003971, "param_norms": 0.311336}, {"epoch": 1050, "train_losses": 3.367146, "test_losses": 3.367146, "train_accs": 0.806183, "test_accs": 0.806183, "grad_norms": 0.00397, "param_norms": 0.311802}, {"epoch": 1100, "train_losses": 3.367139, "test_losses": 3.367139, "train_accs": 0.828775, "test_accs": 0.828775, "grad_norms": 0.003971, "param_norms": 0.312292}, {"epoch": 1150, "train_losses": 3.367131, "test_losses": 3.367131, "train_accs": 0.86088, "test_accs": 0.86088, "grad_norms": 0.003972, "param_norms": 0.312807}, {"epoch": 1200, "train_losses": 3.367123, "test_losses": 3.367123, "train_accs": 0.895363, "test_accs": 0.895363, "grad_norms": 0.003972, "param_norms": 0.313345}, {"epoch": 1250, "train_losses": 3.367115, "test_losses": 3.367115, "train_accs": 0.917955, "test_accs": 0.917955, "grad_norms": 0.003974, "param_norms": 0.313908}, {"epoch": 1300, "train_losses": 3.367107, "test_losses": 3.367107, "train_accs": 0.939358, "test_accs": 0.939358, "grad_norms": 0.003977, "param_norms": 0.314495}, {"epoch": 1350, "train_losses": 3.3671, "test_losses": 3.3671, "train_accs": 0.954816, "test_accs": 0.954816, "grad_norms": 0.00398, "param_norms": 0.315105}, {"epoch": 1400, "train_losses": 3.367092, "test_losses": 3.367092, "train_accs": 0.967895, "test_accs": 0.967895, "grad_norms": 0.003982, "param_norms": 0.31574}, {"epoch": 1450, "train_losses": 3.367084, "test_losses": 3.367084, "train_accs": 0.977408, "test_accs": 0.977408, "grad_norms": 0.003983, "param_norms": 0.316398}, {"epoch": 1500, "train_losses": 3.367076, "test_losses": 3.367076, "train_accs": 0.982164, "test_accs": 0.982164, "grad_norms": 0.003985, "param_norms": 0.317079}, {"epoch": 1550, "train_losses": 3.367069, "test_losses": 3.367069, "train_accs": 0.98692, "test_accs": 0.98692, "grad_norms": 0.003988, "param_norms": 0.317783}, {"epoch": 1600, "train_losses": 3.36706, "test_losses": 3.36706, "train_accs": 0.996433, "test_accs": 0.996433, "grad_norms": 0.00399, "param_norms": 0.318511}, {"epoch": 1650, "train_losses": 3.367053, "test_losses": 3.367053, "train_accs": 0.998811, "test_accs": 0.998811, "grad_norms": 0.003991, "param_norms": 0.319262}, {"epoch": 1700, "train_losses": 3.367045, "test_losses": 3.367045, "train_accs": 0.998811, "test_accs": 0.998811, "grad_norms": 0.003993, "param_norms": 0.320035}, {"epoch": 1750, "train_losses": 3.367037, "test_losses": 3.367037, "train_accs": 0.998811, "test_accs": 0.998811, "grad_norms": 0.003997, "param_norms": 0.320831}, {"epoch": 1800, "train_losses": 3.367029, "test_losses": 3.367029, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004001, "param_norms": 0.32165}, {"epoch": 1850, "train_losses": 3.367021, "test_losses": 3.367021, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004005, "param_norms": 0.322491}, {"epoch": 1900, "train_losses": 3.367013, "test_losses": 3.367013, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004006, "param_norms": 0.323354}, {"epoch": 1950, "train_losses": 3.367005, "test_losses": 3.367005, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004009, "param_norms": 0.324239}, {"epoch": 2000, "train_losses": 3.366997, "test_losses": 3.366997, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004014, "param_norms": 0.325146}, {"epoch": 2050, "train_losses": 3.36699, "test_losses": 3.36699, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004018, "param_norms": 0.326074}, {"epoch": 2100, "train_losses": 3.366982, "test_losses": 3.366982, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004021, "param_norms": 0.327024}, {"epoch": 2150, "train_losses": 3.366974, "test_losses": 3.366974, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004027, "param_norms": 0.327995}, {"epoch": 2200, "train_losses": 3.366966, "test_losses": 3.366966, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004033, "param_norms": 0.328987}, {"epoch": 2250, "train_losses": 3.366958, "test_losses": 3.366958, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004039, "param_norms": 0.330001}, {"epoch": 2300, "train_losses": 3.36695, "test_losses": 3.36695, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004044, "param_norms": 0.331035}, {"epoch": 2350, "train_losses": 3.366942, "test_losses": 3.366942, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004051, "param_norms": 0.33209}, {"epoch": 2400, "train_losses": 3.366934, "test_losses": 3.366934, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00406, "param_norms": 0.333165}, {"epoch": 2450, "train_losses": 3.366926, "test_losses": 3.366926, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004069, "param_norms": 0.334261}, {"epoch": 2500, "train_losses": 3.366918, "test_losses": 3.366918, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004074, "param_norms": 0.335376}, {"epoch": 2550, "train_losses": 3.36691, "test_losses": 3.36691, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004082, "param_norms": 0.336512}, {"epoch": 2600, "train_losses": 3.366902, "test_losses": 3.366902, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004088, "param_norms": 0.337667}, {"epoch": 2650, "train_losses": 3.366894, "test_losses": 3.366894, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004096, "param_norms": 0.338843}, {"epoch": 2700, "train_losses": 3.366886, "test_losses": 3.366886, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004102, "param_norms": 0.340037}, {"epoch": 2750, "train_losses": 3.366878, "test_losses": 3.366878, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004111, "param_norms": 0.341251}, {"epoch": 2800, "train_losses": 3.36687, "test_losses": 3.36687, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004119, "param_norms": 0.342484}, {"epoch": 2850, "train_losses": 3.366862, "test_losses": 3.366862, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004128, "param_norms": 0.343737}, {"epoch": 2900, "train_losses": 3.366854, "test_losses": 3.366854, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004136, "param_norms": 0.345008}, {"epoch": 2950, "train_losses": 3.366846, "test_losses": 3.366846, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004145, "param_norms": 0.346297}, {"epoch": 3000, "train_losses": 3.366838, "test_losses": 3.366838, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004154, "param_norms": 0.347605}, {"epoch": 3050, "train_losses": 3.36683, "test_losses": 3.36683, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004161, "param_norms": 0.348932}, {"epoch": 3100, "train_losses": 3.366822, "test_losses": 3.366822, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004171, "param_norms": 0.350277}, {"epoch": 3150, "train_losses": 3.366814, "test_losses": 3.366814, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00418, "param_norms": 0.35164}, {"epoch": 3200, "train_losses": 3.366805, "test_losses": 3.366805, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004191, "param_norms": 0.35302}, {"epoch": 3250, "train_losses": 3.366797, "test_losses": 3.366797, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00421, "param_norms": 0.354419}, {"epoch": 3300, "train_losses": 3.366789, "test_losses": 3.366789, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004222, "param_norms": 0.355835}, {"epoch": 3350, "train_losses": 3.366781, "test_losses": 3.366781, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00423, "param_norms": 0.357269}, {"epoch": 3400, "train_losses": 3.366772, "test_losses": 3.366772, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004245, "param_norms": 0.35872}, {"epoch": 3450, "train_losses": 3.366764, "test_losses": 3.366764, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004261, "param_norms": 0.360188}, {"epoch": 3500, "train_losses": 3.366755, "test_losses": 3.366755, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004269, "param_norms": 0.361673}, {"epoch": 3550, "train_losses": 3.366747, "test_losses": 3.366747, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004282, "param_norms": 0.363175}, {"epoch": 3600, "train_losses": 3.366739, "test_losses": 3.366739, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004297, "param_norms": 0.364695}, {"epoch": 3650, "train_losses": 3.36673, "test_losses": 3.36673, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004312, "param_norms": 0.36623}, {"epoch": 3700, "train_losses": 3.366722, "test_losses": 3.366722, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004329, "param_norms": 0.367783}, {"epoch": 3750, "train_losses": 3.366713, "test_losses": 3.366713, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004339, "param_norms": 0.369352}, {"epoch": 3800, "train_losses": 3.366705, "test_losses": 3.366705, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004356, "param_norms": 0.370937}, {"epoch": 3850, "train_losses": 3.366696, "test_losses": 3.366696, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004371, "param_norms": 0.372538}, {"epoch": 3900, "train_losses": 3.366688, "test_losses": 3.366688, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004389, "param_norms": 0.374156}, {"epoch": 3950, "train_losses": 3.366679, "test_losses": 3.366679, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004405, "param_norms": 0.37579}, {"epoch": 4000, "train_losses": 3.36667, "test_losses": 3.36667, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004421, "param_norms": 0.37744}, {"epoch": 4050, "train_losses": 3.366661, "test_losses": 3.366661, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004438, "param_norms": 0.379106}, {"epoch": 4100, "train_losses": 3.366652, "test_losses": 3.366652, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004454, "param_norms": 0.380788}, {"epoch": 4150, "train_losses": 3.366644, "test_losses": 3.366644, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004465, "param_norms": 0.382485}, {"epoch": 4200, "train_losses": 3.366635, "test_losses": 3.366635, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004488, "param_norms": 0.384198}, {"epoch": 4250, "train_losses": 3.366626, "test_losses": 3.366626, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004502, "param_norms": 0.385927}, {"epoch": 4300, "train_losses": 3.366617, "test_losses": 3.366617, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004521, "param_norms": 0.387671}, {"epoch": 4350, "train_losses": 3.366607, "test_losses": 3.366607, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004538, "param_norms": 0.389431}, {"epoch": 4400, "train_losses": 3.366598, "test_losses": 3.366598, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004552, "param_norms": 0.391206}, {"epoch": 4450, "train_losses": 3.366589, "test_losses": 3.366589, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004575, "param_norms": 0.392996}, {"epoch": 4500, "train_losses": 3.36658, "test_losses": 3.36658, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004597, "param_norms": 0.394801}, {"epoch": 4550, "train_losses": 3.366571, "test_losses": 3.366571, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004623, "param_norms": 0.396621}, {"epoch": 4600, "train_losses": 3.366561, "test_losses": 3.366561, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004642, "param_norms": 0.398457}, {"epoch": 4650, "train_losses": 3.366552, "test_losses": 3.366552, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004662, "param_norms": 0.400307}, {"epoch": 4700, "train_losses": 3.366543, "test_losses": 3.366543, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004677, "param_norms": 0.402173}, {"epoch": 4750, "train_losses": 3.366534, "test_losses": 3.366534, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004704, "param_norms": 0.404053}, {"epoch": 4800, "train_losses": 3.366523, "test_losses": 3.366523, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004717, "param_norms": 0.405947}, {"epoch": 4850, "train_losses": 3.366514, "test_losses": 3.366514, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004743, "param_norms": 0.407857}, {"epoch": 4900, "train_losses": 3.366504, "test_losses": 3.366504, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004767, "param_norms": 0.409781}, {"epoch": 4950, "train_losses": 3.366495, "test_losses": 3.366495, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004782, "param_norms": 0.41172}, {"epoch": 4999, "train_losses": 3.366485, "test_losses": 3.366485, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004805, "param_norms": 0.413634}], "total_epochs": 5000}}