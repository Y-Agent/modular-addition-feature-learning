{"standard": {"config": {"prime": 15, "d_mlp": 512, "act_type": "ReLU", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 5e-05, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=15, run=standard\n======================================================================\n\nConfiguration:\n  prime (p)       = 15\n  d_mlp           = 512\n  activation      = ReLU\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 5e-05\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      2.708508      2.708508      0.0533      0.0533      0.0346       3.2100\n      50      2.701765      2.701765      0.1111      0.1111      0.0340       3.2262\n     100      2.694151      2.694151      0.3022      0.3022      0.0348       3.2977\n     150      2.686129      2.686129      0.5156      0.5156      0.0364       3.4137\n     200      2.677509      2.677509      0.6844      0.6844      0.0388       3.5632\n     250      2.668158      2.668158      0.8267      0.8267      0.0419       3.7387\n     300      2.657879      2.657879      0.8889      0.8889      0.0455       3.9345\n     350      2.646547      2.646547      0.9333      0.9333      0.0496       4.1460\n     400      2.634130      2.634130      0.9511      0.9511      0.0538       4.3693\n     450      2.620615      2.620615      0.9600      0.9600      0.0580       4.6013\n     500      2.605972      2.605972      0.9600      0.9600      0.0625       4.8398\n     550      2.590114      2.590114      0.9600      0.9600      0.0674       5.0836\n     600      2.573072      2.573072      0.9600      0.9600      0.0721       5.3320\n     650      2.554797      2.554797      0.9600      0.9600      0.0770       5.5838\n     700      2.535294      2.535294      0.9644      0.9644      0.0820       5.8388\n     750      2.514557      2.514557      0.9644      0.9644      0.0867       6.0964\n     800      2.492614      2.492614      0.9689      0.9689      0.0918       6.3561\n     850      2.469465      2.469465      0.9644      0.9644      0.0968       6.6175\n     900      2.445097      2.445097      0.9644      0.9644      0.1017       6.8802\n     950      2.419527      2.419527      0.9644      0.9644      0.1065       7.1443\n    1000      2.392782      2.392782      0.9644      0.9644      0.1113       7.4099\n    1050      2.364869      2.364869      0.9644      0.9644      0.1161       7.6768\n    1100      2.335790      2.335790      0.9644      0.9644      0.1212       7.9449\n    1150      2.305551      2.305551      0.9733      0.9733      0.1260       8.2140\n    1200      2.274176      2.274176      0.9689      0.9689      0.1305       8.4840\n    1250      2.241676      2.241676      0.9689      0.9689      0.1354       8.7547\n    1300      2.208121      2.208121      0.9778      0.9778      0.1398       9.0261\n    1350      2.173514      2.173514      0.9733      0.9733      0.1443       9.2980\n    1400      2.137855      2.137855      0.9733      0.9733      0.1489       9.5704\n    1450      2.101184      2.101184      0.9733      0.9733      0.1532       9.8429\n    1500      2.063508      2.063508      0.9733      0.9733      0.1573      10.1158\n    1550      2.024894      2.024894      0.9733      0.9733      0.1616      10.3891\n    1600      1.985370      1.985370      0.9733      0.9733      0.1653      10.6629\n    1650      1.944952      1.944952      0.9733      0.9733      0.1690      10.9369\n    1700      1.903671      1.903671      0.9733      0.9733      0.1732      11.2109\n    1750      1.861597      1.861597      0.9778      0.9778      0.1763      11.4848\n    1800      1.818776      1.818776      0.9778      0.9778      0.1801      11.7587\n    1850      1.775244      1.775244      0.9778      0.9778      0.1826      12.0323\n    1900      1.731057      1.731057      0.9778      0.9778      0.1858      12.3057\n    1950      1.686320      1.686320      0.9822      0.9822      0.1884      12.5791\n    2000      1.641035      1.641035      0.9822      0.9822      0.1909      12.8522\n    2050      1.595242      1.595242      0.9822      0.9822      0.1932      13.1251\n    2100      1.549028      1.549028      0.9911      0.9911      0.1955      13.3978\n    2150      1.502467      1.502467      0.9911      0.9911      0.1972      13.6703\n    2200      1.455593      1.455593      0.9911      0.9911      0.1985      13.9424\n    2250      1.408524      1.408524      0.9911      0.9911      0.1995      14.2141\n    2300      1.361254      1.361254      0.9911      0.9911      0.2006      14.4852\n    2350      1.313875      1.313875      0.9911      0.9911      0.2012      14.7558\n    2400      1.266527      1.266527      0.9911      0.9911      0.2013      15.0258\n    2450      1.219261      1.219261      0.9911      0.9911      0.2009      15.2951\n    2500      1.172157      1.172157      0.9956      0.9956      0.2007      15.5639\n    2550      1.125263      1.125263      1.0000      1.0000      0.1999      15.8320\n    2600      1.078703      1.078703      1.0000      1.0000      0.1988      16.0995\n    2650      1.032564      1.032564      1.0000      1.0000      0.1968      16.3664\n    2700      0.986893      0.986893      1.0000      1.0000      0.1951      16.6324\n    2750      0.941788      0.941788      1.0000      1.0000      0.1929      16.8977\n    2800      0.897309      0.897309      1.0000      1.0000      0.1902      17.1624\n    2850      0.853580      0.853580      1.0000      1.0000      0.1869      17.4266\n    2900      0.810676      0.810676      1.0000      1.0000      0.1837      17.6899\n    2950      0.768628      0.768628      1.0000      1.0000      0.1801      17.9525\n    3000      0.727545      0.727545      1.0000      1.0000      0.1763      18.2142\n    3050      0.687491      0.687491      1.0000      1.0000      0.1721      18.4750\n    3100      0.648535      0.648535      1.0000      1.0000      0.1675      18.7348\n    3150      0.610729      0.610729      1.0000      1.0000      0.1631      18.9937\n    3200      0.574078      0.574078      1.0000      1.0000      0.1581      19.2515\n    3250      0.538643      0.538643      1.0000      1.0000      0.1532      19.5083\n    3300      0.504470      0.504470      1.0000      1.0000      0.1476      19.7643\n    3350      0.471580      0.471580      1.0000      1.0000      0.1423      20.0197\n    3400      0.440040      0.440040      1.0000      1.0000      0.1364      20.2743\n    3450      0.409832      0.409832      1.0000      1.0000      0.1311      20.5284\n    3500      0.380988      0.380988      1.0000      1.0000      0.1252      20.7819\n    3550      0.353516      0.353516      1.0000      1.0000      0.1192      21.0348\n    3600      0.327428      0.327428      1.0000      1.0000      0.1137      21.2873\n    3650      0.302703      0.302703      1.0000      1.0000      0.1077      21.5394\n    3700      0.279346      0.279346      1.0000      1.0000      0.1022      21.7913\n    3750      0.257324      0.257324      1.0000      1.0000      0.0963      22.0428\n    3800      0.236634      0.236634      1.0000      1.0000      0.0909      22.2942\n    3850      0.217248      0.217248      1.0000      1.0000      0.0853      22.5455\n    3900      0.199119      0.199119      1.0000      1.0000      0.0797      22.7966\n    3950      0.182211      0.182211      1.0000      1.0000      0.0746      23.0474\n    4000      0.166479      0.166479      1.0000      1.0000      0.0696      23.2980\n    4050      0.151870      0.151870      1.0000      1.0000      0.0648      23.5484\n    4100      0.138337      0.138337      1.0000      1.0000      0.0600      23.7985\n    4150      0.125835      0.125835      1.0000      1.0000      0.0556      24.0482\n    4200      0.114298      0.114298      1.0000      1.0000      0.0515      24.2977\n    4250      0.103678      0.103678      1.0000      1.0000      0.0473      24.5468\n    4300      0.093922      0.093922      1.0000      1.0000      0.0437      24.7955\n    4350      0.084973      0.084973      1.0000      1.0000      0.0401      25.0438\n    4400      0.076784      0.076784      1.0000      1.0000      0.0368      25.2916\n    4450      0.069301      0.069301      1.0000      1.0000      0.0337      25.5390\n    4500      0.062470      0.062470      1.0000      1.0000      0.0308      25.7860\n    4550      0.056248      0.056248      1.0000      1.0000      0.0281      26.0323\n    4600      0.050594      0.050594      1.0000      1.0000      0.0256      26.2781\n    4650      0.045458      0.045458      1.0000      1.0000      0.0233      26.5234\n    4700      0.040801      0.040801      1.0000      1.0000      0.0212      26.7681\n    4750      0.036583      0.036583      1.0000      1.0000      0.0193      27.0124\n    4800      0.032766      0.032766      1.0000      1.0000      0.0174      27.2563\n    4850      0.029319      0.029319      1.0000      1.0000      0.0158      27.4998\n    4900      0.026207      0.026207      1.0000      1.0000      0.0143      27.7429\n    4950      0.023404      0.023404      1.0000      1.0000      0.0129      27.9856\n    4999      0.020929      0.020929      1.0000      1.0000      0.0117      28.2232\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.020929\n  Test Loss   = 0.020929\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 28.2232\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 2.708508, "test_losses": 2.708508, "train_accs": 0.053333, "test_accs": 0.053333, "grad_norms": 0.034605, "param_norms": 3.21002}, {"epoch": 50, "train_losses": 2.701765, "test_losses": 2.701765, "train_accs": 0.111111, "test_accs": 0.111111, "grad_norms": 0.033961, "param_norms": 3.226197}, {"epoch": 100, "train_losses": 2.694151, "test_losses": 2.694151, "train_accs": 0.302222, "test_accs": 0.302222, "grad_norms": 0.034834, "param_norms": 3.297734}, {"epoch": 150, "train_losses": 2.686129, "test_losses": 2.686129, "train_accs": 0.515556, "test_accs": 0.515556, "grad_norms": 0.036405, "param_norms": 3.413707}, {"epoch": 200, "train_losses": 2.677509, "test_losses": 2.677509, "train_accs": 0.684444, "test_accs": 0.684444, "grad_norms": 0.03883, "param_norms": 3.563211}, {"epoch": 250, "train_losses": 2.668158, "test_losses": 2.668158, "train_accs": 0.826667, "test_accs": 0.826667, "grad_norms": 0.041903, "param_norms": 3.73868}, {"epoch": 300, "train_losses": 2.657879, "test_losses": 2.657879, "train_accs": 0.888889, "test_accs": 0.888889, "grad_norms": 0.04547, "param_norms": 3.934533}, {"epoch": 350, "train_losses": 2.646547, "test_losses": 2.646547, "train_accs": 0.933333, "test_accs": 0.933333, "grad_norms": 0.049568, "param_norms": 4.146044}, {"epoch": 400, "train_losses": 2.63413, "test_losses": 2.63413, "train_accs": 0.951111, "test_accs": 0.951111, "grad_norms": 0.053777, "param_norms": 4.369293}, {"epoch": 450, "train_losses": 2.620615, "test_losses": 2.620615, "train_accs": 0.96, "test_accs": 0.96, "grad_norms": 0.058011, "param_norms": 4.601326}, {"epoch": 500, "train_losses": 2.605972, "test_losses": 2.605972, "train_accs": 0.96, "test_accs": 0.96, "grad_norms": 0.062547, "param_norms": 4.839811}, {"epoch": 550, "train_losses": 2.590114, "test_losses": 2.590114, "train_accs": 0.96, "test_accs": 0.96, "grad_norms": 0.067385, "param_norms": 5.083593}, {"epoch": 600, "train_losses": 2.573072, "test_losses": 2.573072, "train_accs": 0.96, "test_accs": 0.96, "grad_norms": 0.07205, "param_norms": 5.332009}, {"epoch": 650, "train_losses": 2.554797, "test_losses": 2.554797, "train_accs": 0.96, "test_accs": 0.96, "grad_norms": 0.076982, "param_norms": 5.58381}, {"epoch": 700, "train_losses": 2.535294, "test_losses": 2.535294, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.082043, "param_norms": 5.838843}, {"epoch": 750, "train_losses": 2.514557, "test_losses": 2.514557, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.086741, "param_norms": 6.096441}, {"epoch": 800, "train_losses": 2.492614, "test_losses": 2.492614, "train_accs": 0.968889, "test_accs": 0.968889, "grad_norms": 0.091847, "param_norms": 6.356139}, {"epoch": 850, "train_losses": 2.469465, "test_losses": 2.469465, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.096813, "param_norms": 6.617476}, {"epoch": 900, "train_losses": 2.445097, "test_losses": 2.445097, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.101699, "param_norms": 6.880217}, {"epoch": 950, "train_losses": 2.419527, "test_losses": 2.419527, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.106527, "param_norms": 7.14432}, {"epoch": 1000, "train_losses": 2.392782, "test_losses": 2.392782, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.111261, "param_norms": 7.409879}, {"epoch": 1050, "train_losses": 2.364869, "test_losses": 2.364869, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.116149, "param_norms": 7.676804}, {"epoch": 1100, "train_losses": 2.33579, "test_losses": 2.33579, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.121172, "param_norms": 7.944856}, {"epoch": 1150, "train_losses": 2.305551, "test_losses": 2.305551, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.125964, "param_norms": 8.213983}, {"epoch": 1200, "train_losses": 2.274176, "test_losses": 2.274176, "train_accs": 0.968889, "test_accs": 0.968889, "grad_norms": 0.130479, "param_norms": 8.48398}, {"epoch": 1250, "train_losses": 2.241676, "test_losses": 2.241676, "train_accs": 0.968889, "test_accs": 0.968889, "grad_norms": 0.135354, "param_norms": 8.754677}, {"epoch": 1300, "train_losses": 2.208121, "test_losses": 2.208121, "train_accs": 0.977778, "test_accs": 0.977778, "grad_norms": 0.139846, "param_norms": 9.026126}, {"epoch": 1350, "train_losses": 2.173514, "test_losses": 2.173514, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.144254, "param_norms": 9.298024}, {"epoch": 1400, "train_losses": 2.137855, "test_losses": 2.137855, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.14887, "param_norms": 9.570354}, {"epoch": 1450, "train_losses": 2.101184, "test_losses": 2.101184, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.153213, "param_norms": 9.842945}, {"epoch": 1500, "train_losses": 2.063508, "test_losses": 2.063508, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.157253, "param_norms": 10.115779}, {"epoch": 1550, "train_losses": 2.024894, "test_losses": 2.024894, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.161642, "param_norms": 10.389142}, {"epoch": 1600, "train_losses": 1.98537, "test_losses": 1.98537, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.165293, "param_norms": 10.662936}, {"epoch": 1650, "train_losses": 1.944952, "test_losses": 1.944952, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.169047, "param_norms": 10.936902}, {"epoch": 1700, "train_losses": 1.903671, "test_losses": 1.903671, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.17318, "param_norms": 11.210854}, {"epoch": 1750, "train_losses": 1.861597, "test_losses": 1.861597, "train_accs": 0.977778, "test_accs": 0.977778, "grad_norms": 0.176308, "param_norms": 11.484768}, {"epoch": 1800, "train_losses": 1.818776, "test_losses": 1.818776, "train_accs": 0.977778, "test_accs": 0.977778, "grad_norms": 0.180122, "param_norms": 11.758679}, {"epoch": 1850, "train_losses": 1.775244, "test_losses": 1.775244, "train_accs": 0.977778, "test_accs": 0.977778, "grad_norms": 0.182567, "param_norms": 12.032274}, {"epoch": 1900, "train_losses": 1.731057, "test_losses": 1.731057, "train_accs": 0.977778, "test_accs": 0.977778, "grad_norms": 0.18576, "param_norms": 12.305725}, {"epoch": 1950, "train_losses": 1.68632, "test_losses": 1.68632, "train_accs": 0.982222, "test_accs": 0.982222, "grad_norms": 0.188381, "param_norms": 12.579105}, {"epoch": 2000, "train_losses": 1.641035, "test_losses": 1.641035, "train_accs": 0.982222, "test_accs": 0.982222, "grad_norms": 0.190895, "param_norms": 12.852182}, {"epoch": 2050, "train_losses": 1.595242, "test_losses": 1.595242, "train_accs": 0.982222, "test_accs": 0.982222, "grad_norms": 0.193218, "param_norms": 13.125079}, {"epoch": 2100, "train_losses": 1.549028, "test_losses": 1.549028, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.195545, "param_norms": 13.397758}, {"epoch": 2150, "train_losses": 1.502467, "test_losses": 1.502467, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.197224, "param_norms": 13.670302}, {"epoch": 2200, "train_losses": 1.455593, "test_losses": 1.455593, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.198529, "param_norms": 13.942435}, {"epoch": 2250, "train_losses": 1.408524, "test_losses": 1.408524, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.199469, "param_norms": 14.214108}, {"epoch": 2300, "train_losses": 1.361254, "test_losses": 1.361254, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.200613, "param_norms": 14.485231}, {"epoch": 2350, "train_losses": 1.313875, "test_losses": 1.313875, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.201191, "param_norms": 14.75579}, {"epoch": 2400, "train_losses": 1.266527, "test_losses": 1.266527, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.201308, "param_norms": 15.025764}, {"epoch": 2450, "train_losses": 1.219261, "test_losses": 1.219261, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.200887, "param_norms": 15.295143}, {"epoch": 2500, "train_losses": 1.172157, "test_losses": 1.172157, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.200689, "param_norms": 15.563874}, {"epoch": 2550, "train_losses": 1.125263, "test_losses": 1.125263, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.199949, "param_norms": 15.831963}, {"epoch": 2600, "train_losses": 1.078703, "test_losses": 1.078703, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.198826, "param_norms": 16.099453}, {"epoch": 2650, "train_losses": 1.032564, "test_losses": 1.032564, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.19685, "param_norms": 16.366352}, {"epoch": 2700, "train_losses": 0.986893, "test_losses": 0.986893, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.19506, "param_norms": 16.63242}, {"epoch": 2750, "train_losses": 0.941788, "test_losses": 0.941788, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.192917, "param_norms": 16.897724}, {"epoch": 2800, "train_losses": 0.897309, "test_losses": 0.897309, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.190206, "param_norms": 17.162419}, {"epoch": 2850, "train_losses": 0.85358, "test_losses": 0.85358, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.186901, "param_norms": 17.42657}, {"epoch": 2900, "train_losses": 0.810676, "test_losses": 0.810676, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.183711, "param_norms": 17.689949}, {"epoch": 2950, "train_losses": 0.768628, "test_losses": 0.768628, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.180127, "param_norms": 17.952521}, {"epoch": 3000, "train_losses": 0.727545, "test_losses": 0.727545, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.176272, "param_norms": 18.214229}, {"epoch": 3050, "train_losses": 0.687491, "test_losses": 0.687491, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.172131, "param_norms": 18.474989}, {"epoch": 3100, "train_losses": 0.648535, "test_losses": 0.648535, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.16752, "param_norms": 18.734798}, {"epoch": 3150, "train_losses": 0.610729, "test_losses": 0.610729, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.163083, "param_norms": 18.993663}, {"epoch": 3200, "train_losses": 0.574078, "test_losses": 0.574078, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.158098, "param_norms": 19.251513}, {"epoch": 3250, "train_losses": 0.538643, "test_losses": 0.538643, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.15323, "param_norms": 19.50826}, {"epoch": 3300, "train_losses": 0.50447, "test_losses": 0.50447, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.147614, "param_norms": 19.764339}, {"epoch": 3350, "train_losses": 0.47158, "test_losses": 0.47158, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.142343, "param_norms": 20.019653}, {"epoch": 3400, "train_losses": 0.44004, "test_losses": 0.44004, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.136362, "param_norms": 20.274311}, {"epoch": 3450, "train_losses": 0.409832, "test_losses": 0.409832, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.131142, "param_norms": 20.528376}, {"epoch": 3500, "train_losses": 0.380988, "test_losses": 0.380988, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.125162, "param_norms": 20.781871}, {"epoch": 3550, "train_losses": 0.353516, "test_losses": 0.353516, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.119225, "param_norms": 21.034803}, {"epoch": 3600, "train_losses": 0.327428, "test_losses": 0.327428, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.113701, "param_norms": 21.287261}, {"epoch": 3650, "train_losses": 0.302703, "test_losses": 0.302703, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.107685, "param_norms": 21.539379}, {"epoch": 3700, "train_losses": 0.279346, "test_losses": 0.279346, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.102227, "param_norms": 21.791264}, {"epoch": 3750, "train_losses": 0.257324, "test_losses": 0.257324, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.096293, "param_norms": 22.042816}, {"epoch": 3800, "train_losses": 0.236634, "test_losses": 0.236634, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.090874, "param_norms": 22.294247}, {"epoch": 3850, "train_losses": 0.217248, "test_losses": 0.217248, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.08528, "param_norms": 22.545505}, {"epoch": 3900, "train_losses": 0.199119, "test_losses": 0.199119, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.079692, "param_norms": 22.796558}, {"epoch": 3950, "train_losses": 0.182211, "test_losses": 0.182211, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.074589, "param_norms": 23.047434}, {"epoch": 4000, "train_losses": 0.166479, "test_losses": 0.166479, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.069567, "param_norms": 23.298045}, {"epoch": 4050, "train_losses": 0.15187, "test_losses": 0.15187, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.064795, "param_norms": 23.548422}, {"epoch": 4100, "train_losses": 0.138337, "test_losses": 0.138337, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.059966, "param_norms": 23.798469}, {"epoch": 4150, "train_losses": 0.125835, "test_losses": 0.125835, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.05561, "param_norms": 24.048244}, {"epoch": 4200, "train_losses": 0.114298, "test_losses": 0.114298, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.051506, "param_norms": 24.297723}, {"epoch": 4250, "train_losses": 0.103678, "test_losses": 0.103678, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.047324, "param_norms": 24.546842}, {"epoch": 4300, "train_losses": 0.093922, "test_losses": 0.093922, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.043651, "param_norms": 24.795542}, {"epoch": 4350, "train_losses": 0.084973, "test_losses": 0.084973, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.040078, "param_norms": 25.043796}, {"epoch": 4400, "train_losses": 0.076784, "test_losses": 0.076784, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.036819, "param_norms": 25.291605}, {"epoch": 4450, "train_losses": 0.069301, "test_losses": 0.069301, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.033698, "param_norms": 25.539044}, {"epoch": 4500, "train_losses": 0.06247, "test_losses": 0.06247, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.030821, "param_norms": 25.785961}, {"epoch": 4550, "train_losses": 0.056248, "test_losses": 0.056248, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.02812, "param_norms": 26.032293}, {"epoch": 4600, "train_losses": 0.050594, "test_losses": 0.050594, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.02561, "param_norms": 26.278098}, {"epoch": 4650, "train_losses": 0.045458, "test_losses": 0.045458, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.023331, "param_norms": 26.523357}, {"epoch": 4700, "train_losses": 0.040801, "test_losses": 0.040801, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.021245, "param_norms": 26.768145}, {"epoch": 4750, "train_losses": 0.036583, "test_losses": 0.036583, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.019287, "param_norms": 27.012442}, {"epoch": 4800, "train_losses": 0.032766, "test_losses": 0.032766, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.017433, "param_norms": 27.256299}, {"epoch": 4850, "train_losses": 0.029319, "test_losses": 0.029319, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.015782, "param_norms": 27.499785}, {"epoch": 4900, "train_losses": 0.026207, "test_losses": 0.026207, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014273, "param_norms": 27.742866}, {"epoch": 4950, "train_losses": 0.023404, "test_losses": 0.023404, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012862, "param_norms": 27.985643}, {"epoch": 4999, "train_losses": 0.020929, "test_losses": 0.020929, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011655, "param_norms": 28.223215}], "total_epochs": 5000}, "quad_random": {"config": {"prime": 15, "d_mlp": 512, "act_type": "Quad", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 5e-05, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=15, run=quad_random\n======================================================================\n\nConfiguration:\n  prime (p)       = 15\n  d_mlp           = 512\n  activation      = Quad\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 5e-05\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      2.708178      2.708178      0.0311      0.0311      0.0048       3.2100\n      50      2.707150      2.707150      0.1200      0.1200      0.0049       3.2344\n     100      2.705877      2.705877      0.3689      0.3689      0.0054       3.3305\n     150      2.704360      2.704360      0.6044      0.6044      0.0063       3.4821\n     200      2.702511      2.702511      0.7600      0.7600      0.0075       3.6723\n     250      2.700259      2.700259      0.8533      0.8533      0.0089       3.8889\n     300      2.697540      2.697540      0.9200      0.9200      0.0106       4.1240\n     350      2.694298      2.694298      0.9467      0.9467      0.0126       4.3720\n     400      2.690480      2.690480      0.9867      0.9867      0.0147       4.6295\n     450      2.686031      2.686031      0.9956      0.9956      0.0171       4.8939\n     500      2.680901      2.680901      0.9956      0.9956      0.0196       5.1635\n     550      2.675037      2.675037      0.9956      0.9956      0.0224       5.4371\n     600      2.668388      2.668388      0.9956      0.9956      0.0254       5.7138\n     650      2.660902      2.660902      0.9956      0.9956      0.0285       5.9932\n     700      2.652529      2.652529      0.9956      0.9956      0.0319       6.2749\n     750      2.643218      2.643218      1.0000      1.0000      0.0354       6.5582\n     800      2.632921      2.632921      1.0000      1.0000      0.0392       6.8431\n     850      2.621588      2.621588      1.0000      1.0000      0.0431       7.1291\n     900      2.609170      2.609170      1.0000      1.0000      0.0472       7.4162\n     950      2.595621      2.595621      1.0000      1.0000      0.0515       7.7043\n    1000      2.580893      2.580893      1.0000      1.0000      0.0560       7.9936\n    1050      2.564942      2.564942      1.0000      1.0000      0.0607       8.2840\n    1100      2.547724      2.547724      1.0000      1.0000      0.0655       8.5755\n    1150      2.529197      2.529197      1.0000      1.0000      0.0705       8.8678\n    1200      2.509321      2.509321      1.0000      1.0000      0.0757       9.1608\n    1250      2.488054      2.488054      1.0000      1.0000      0.0810       9.4543\n    1300      2.465358      2.465358      1.0000      1.0000      0.0864       9.7483\n    1350      2.441196      2.441196      1.0000      1.0000      0.0921      10.0428\n    1400      2.415534      2.415534      1.0000      1.0000      0.0978      10.3380\n    1450      2.388340      2.388340      1.0000      1.0000      0.1037      10.6337\n    1500      2.359584      2.359584      1.0000      1.0000      0.1097      10.9300\n    1550      2.329242      2.329242      1.0000      1.0000      0.1158      11.2267\n    1600      2.297287      2.297287      1.0000      1.0000      0.1220      11.5237\n    1650      2.263699      2.263699      1.0000      1.0000      0.1283      11.8211\n    1700      2.228461      2.228461      1.0000      1.0000      0.1347      12.1185\n    1750      2.191558      2.191558      1.0000      1.0000      0.1412      12.4161\n    1800      2.152983      2.152983      1.0000      1.0000      0.1477      12.7139\n    1850      2.112732      2.112732      1.0000      1.0000      0.1542      13.0118\n    1900      2.070807      2.070807      1.0000      1.0000      0.1607      13.3097\n    1950      2.027215      2.027215      1.0000      1.0000      0.1673      13.6077\n    2000      1.981971      1.981971      1.0000      1.0000      0.1738      13.9055\n    2050      1.935097      1.935097      1.0000      1.0000      0.1802      14.2032\n    2100      1.886621      1.886621      1.0000      1.0000      0.1865      14.5005\n    2150      1.836583      1.836583      1.0000      1.0000      0.1928      14.7975\n    2200      1.785030      1.785030      1.0000      1.0000      0.1988      15.0941\n    2250      1.732021      1.732021      1.0000      1.0000      0.2047      15.3903\n    2300      1.677627      1.677627      1.0000      1.0000      0.2103      15.6861\n    2350      1.621930      1.621930      1.0000      1.0000      0.2157      15.9813\n    2400      1.565025      1.565025      1.0000      1.0000      0.2207      16.2758\n    2450      1.507023      1.507023      1.0000      1.0000      0.2253      16.5697\n    2500      1.448047      1.448047      1.0000      1.0000      0.2296      16.8628\n    2550      1.388237      1.388237      1.0000      1.0000      0.2333      17.1551\n    2600      1.327748      1.327748      1.0000      1.0000      0.2364      17.4466\n    2650      1.266753      1.266753      1.0000      1.0000      0.2390      17.7372\n    2700      1.205436      1.205436      1.0000      1.0000      0.2409      18.0268\n    2750      1.143998      1.143998      1.0000      1.0000      0.2421      18.3151\n    2800      1.082651      1.082651      1.0000      1.0000      0.2425      18.6021\n    2850      1.021620      1.021620      1.0000      1.0000      0.2421      18.8877\n    2900      0.961135      0.961135      1.0000      1.0000      0.2408      19.1719\n    2950      0.901435      0.901435      1.0000      1.0000      0.2387      19.4546\n    3000      0.842757      0.842757      1.0000      1.0000      0.2357      19.7355\n    3050      0.785334      0.785334      1.0000      1.0000      0.2319      20.0145\n    3100      0.729387      0.729387      1.0000      1.0000      0.2271      20.2910\n    3150      0.675124      0.675124      1.0000      1.0000      0.2216      20.5650\n    3200      0.622735      0.622735      1.0000      1.0000      0.2152      20.8361\n    3250      0.572388      0.572388      1.0000      1.0000      0.2081      21.1045\n    3300      0.524228      0.524228      1.0000      1.0000      0.2003      21.3699\n    3350      0.478375      0.478375      1.0000      1.0000      0.1920      21.6324\n    3400      0.434923      0.434923      1.0000      1.0000      0.1831      21.8921\n    3450      0.393942      0.393942      1.0000      1.0000      0.1738      22.1489\n    3500      0.355477      0.355477      1.0000      1.0000      0.1641      22.4033\n    3550      0.319550      0.319550      1.0000      1.0000      0.1542      22.6555\n    3600      0.286163      0.286163      1.0000      1.0000      0.1441      22.9060\n    3650      0.255295      0.255295      1.0000      1.0000      0.1340      23.1551\n    3700      0.226904      0.226904      1.0000      1.0000      0.1239      23.4033\n    3750      0.200925      0.200925      1.0000      1.0000      0.1140      23.6508\n    3800      0.177277      0.177277      1.0000      1.0000      0.1042      23.8980\n    3850      0.155858      0.155858      1.0000      1.0000      0.0949      24.1451\n    3900      0.136555      0.136555      1.0000      1.0000      0.0859      24.3921\n    3950      0.119240      0.119240      1.0000      1.0000      0.0774      24.6391\n    4000      0.103780      0.103780      1.0000      1.0000      0.0694      24.8859\n    4050      0.090036      0.090036      1.0000      1.0000      0.0620      25.1325\n    4100      0.077871      0.077871      1.0000      1.0000      0.0551      25.3789\n    4150      0.067146      0.067146      1.0000      1.0000      0.0488      25.6250\n    4200      0.057727      0.057727      1.0000      1.0000      0.0430      25.8707\n    4250      0.049488      0.049488      1.0000      1.0000      0.0377      26.1160\n    4300      0.042306      0.042306      1.0000      1.0000      0.0330      26.3609\n    4350      0.036067      0.036067      1.0000      1.0000      0.0288      26.6053\n    4400      0.030666      0.030666      1.0000      1.0000      0.0250      26.8492\n    4450      0.026006      0.026006      1.0000      1.0000      0.0217      27.0925\n    4500      0.021996      0.021996      1.0000      1.0000      0.0187      27.3354\n    4550      0.018558      0.018558      1.0000      1.0000      0.0161      27.5776\n    4600      0.015618      0.015618      1.0000      1.0000      0.0138      27.8193\n    4650      0.013112      0.013112      1.0000      1.0000      0.0118      28.0604\n    4700      0.010981      0.010981      1.0000      1.0000      0.0101      28.3010\n    4750      0.009175      0.009175      1.0000      1.0000      0.0086      28.5409\n    4800      0.007647      0.007647      1.0000      1.0000      0.0073      28.7801\n    4850      0.006359      0.006359      1.0000      1.0000      0.0061      29.0186\n    4900      0.005276      0.005276      1.0000      1.0000      0.0052      29.2566\n    4950      0.004367      0.004367      1.0000      1.0000      0.0044      29.4939\n    4999      0.003620      0.003620      1.0000      1.0000      0.0037      29.7259\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.003620\n  Test Loss   = 0.003620\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 29.7259\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 2.708178, "test_losses": 2.708178, "train_accs": 0.031111, "test_accs": 0.031111, "grad_norms": 0.004795, "param_norms": 3.21002}, {"epoch": 50, "train_losses": 2.70715, "test_losses": 2.70715, "train_accs": 0.12, "test_accs": 0.12, "grad_norms": 0.004915, "param_norms": 3.23436}, {"epoch": 100, "train_losses": 2.705877, "test_losses": 2.705877, "train_accs": 0.368889, "test_accs": 0.368889, "grad_norms": 0.005434, "param_norms": 3.330455}, {"epoch": 150, "train_losses": 2.70436, "test_losses": 2.70436, "train_accs": 0.604444, "test_accs": 0.604444, "grad_norms": 0.006306, "param_norms": 3.482077}, {"epoch": 200, "train_losses": 2.702511, "test_losses": 2.702511, "train_accs": 0.76, "test_accs": 0.76, "grad_norms": 0.007481, "param_norms": 3.672288}, {"epoch": 250, "train_losses": 2.700259, "test_losses": 2.700259, "train_accs": 0.853333, "test_accs": 0.853333, "grad_norms": 0.008928, "param_norms": 3.888915}, {"epoch": 300, "train_losses": 2.69754, "test_losses": 2.69754, "train_accs": 0.92, "test_accs": 0.92, "grad_norms": 0.010624, "param_norms": 4.123989}, {"epoch": 350, "train_losses": 2.694298, "test_losses": 2.694298, "train_accs": 0.946667, "test_accs": 0.946667, "grad_norms": 0.012551, "param_norms": 4.372023}, {"epoch": 400, "train_losses": 2.69048, "test_losses": 2.69048, "train_accs": 0.986667, "test_accs": 0.986667, "grad_norms": 0.014697, "param_norms": 4.629505}, {"epoch": 450, "train_losses": 2.686031, "test_losses": 2.686031, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.017057, "param_norms": 4.893934}, {"epoch": 500, "train_losses": 2.680901, "test_losses": 2.680901, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.019623, "param_norms": 5.163547}, {"epoch": 550, "train_losses": 2.675037, "test_losses": 2.675037, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.02239, "param_norms": 5.437083}, {"epoch": 600, "train_losses": 2.668388, "test_losses": 2.668388, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.025358, "param_norms": 5.713787}, {"epoch": 650, "train_losses": 2.660902, "test_losses": 2.660902, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.028524, "param_norms": 5.993239}, {"epoch": 700, "train_losses": 2.652529, "test_losses": 2.652529, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.031885, "param_norms": 6.274878}, {"epoch": 750, "train_losses": 2.643218, "test_losses": 2.643218, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.035439, "param_norms": 6.55825}, {"epoch": 800, "train_losses": 2.632921, "test_losses": 2.632921, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.039184, "param_norms": 6.843055}, {"epoch": 850, "train_losses": 2.621588, "test_losses": 2.621588, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.043118, "param_norms": 7.129062}, {"epoch": 900, "train_losses": 2.60917, "test_losses": 2.60917, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.047238, "param_norms": 7.416171}, {"epoch": 950, "train_losses": 2.595621, "test_losses": 2.595621, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.051539, "param_norms": 7.704348}, {"epoch": 1000, "train_losses": 2.580893, "test_losses": 2.580893, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.056021, "param_norms": 7.99362}, {"epoch": 1050, "train_losses": 2.564942, "test_losses": 2.564942, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.060677, "param_norms": 8.284031}, {"epoch": 1100, "train_losses": 2.547724, "test_losses": 2.547724, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.065505, "param_norms": 8.57547}, {"epoch": 1150, "train_losses": 2.529197, "test_losses": 2.529197, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0705, "param_norms": 8.867778}, {"epoch": 1200, "train_losses": 2.509321, "test_losses": 2.509321, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.075658, "param_norms": 9.16081}, {"epoch": 1250, "train_losses": 2.488054, "test_losses": 2.488054, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.080973, "param_norms": 9.454339}, {"epoch": 1300, "train_losses": 2.465358, "test_losses": 2.465358, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.086439, "param_norms": 9.748324}, {"epoch": 1350, "train_losses": 2.441196, "test_losses": 2.441196, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.092052, "param_norms": 10.04283}, {"epoch": 1400, "train_losses": 2.415534, "test_losses": 2.415534, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.097803, "param_norms": 10.337965}, {"epoch": 1450, "train_losses": 2.38834, "test_losses": 2.38834, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.103685, "param_norms": 10.633719}, {"epoch": 1500, "train_losses": 2.359584, "test_losses": 2.359584, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.109688, "param_norms": 10.930015}, {"epoch": 1550, "train_losses": 2.329242, "test_losses": 2.329242, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.115804, "param_norms": 11.22671}, {"epoch": 1600, "train_losses": 2.297287, "test_losses": 2.297287, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.122023, "param_norms": 11.523747}, {"epoch": 1650, "train_losses": 2.263699, "test_losses": 2.263699, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.128332, "param_norms": 11.821057}, {"epoch": 1700, "train_losses": 2.228461, "test_losses": 2.228461, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.134719, "param_norms": 12.118534}, {"epoch": 1750, "train_losses": 2.191558, "test_losses": 2.191558, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.14117, "param_norms": 12.41614}, {"epoch": 1800, "train_losses": 2.152983, "test_losses": 2.152983, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.147669, "param_norms": 12.713887}, {"epoch": 1850, "train_losses": 2.112732, "test_losses": 2.112732, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.154198, "param_norms": 13.011777}, {"epoch": 1900, "train_losses": 2.070807, "test_losses": 2.070807, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.160738, "param_norms": 13.309738}, {"epoch": 1950, "train_losses": 2.027215, "test_losses": 2.027215, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.167268, "param_norms": 13.607689}, {"epoch": 2000, "train_losses": 1.981971, "test_losses": 1.981971, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.173763, "param_norms": 13.905512}, {"epoch": 2050, "train_losses": 1.935097, "test_losses": 1.935097, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.180197, "param_norms": 14.203162}, {"epoch": 2100, "train_losses": 1.886621, "test_losses": 1.886621, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.186541, "param_norms": 14.500509}, {"epoch": 2150, "train_losses": 1.836583, "test_losses": 1.836583, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.192764, "param_norms": 14.797478}, {"epoch": 2200, "train_losses": 1.78503, "test_losses": 1.78503, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.19883, "param_norms": 15.09409}, {"epoch": 2250, "train_losses": 1.732021, "test_losses": 1.732021, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.2047, "param_norms": 15.390315}, {"epoch": 2300, "train_losses": 1.677627, "test_losses": 1.677627, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.210334, "param_norms": 15.686098}, {"epoch": 2350, "train_losses": 1.62193, "test_losses": 1.62193, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.215685, "param_norms": 15.981305}, {"epoch": 2400, "train_losses": 1.565025, "test_losses": 1.565025, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.220706, "param_norms": 16.275824}, {"epoch": 2450, "train_losses": 1.507023, "test_losses": 1.507023, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.225347, "param_norms": 16.569656}, {"epoch": 2500, "train_losses": 1.448047, "test_losses": 1.448047, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.229553, "param_norms": 16.86276}, {"epoch": 2550, "train_losses": 1.388237, "test_losses": 1.388237, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.233267, "param_norms": 17.155053}, {"epoch": 2600, "train_losses": 1.327748, "test_losses": 1.327748, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.236433, "param_norms": 17.446576}, {"epoch": 2650, "train_losses": 1.266753, "test_losses": 1.266753, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.238993, "param_norms": 17.737243}, {"epoch": 2700, "train_losses": 1.205436, "test_losses": 1.205436, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.24089, "param_norms": 18.026775}, {"epoch": 2750, "train_losses": 1.143998, "test_losses": 1.143998, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.242071, "param_norms": 18.315074}, {"epoch": 2800, "train_losses": 1.082651, "test_losses": 1.082651, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.242486, "param_norms": 18.602116}, {"epoch": 2850, "train_losses": 1.02162, "test_losses": 1.02162, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.24209, "param_norms": 18.887743}, {"epoch": 2900, "train_losses": 0.961135, "test_losses": 0.961135, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.240848, "param_norms": 19.171906}, {"epoch": 2950, "train_losses": 0.901435, "test_losses": 0.901435, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.238737, "param_norms": 19.454568}, {"epoch": 3000, "train_losses": 0.842757, "test_losses": 0.842757, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.235746, "param_norms": 19.735529}, {"epoch": 3050, "train_losses": 0.785334, "test_losses": 0.785334, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.231874, "param_norms": 20.014457}, {"epoch": 3100, "train_losses": 0.729387, "test_losses": 0.729387, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.227138, "param_norms": 20.291004}, {"epoch": 3150, "train_losses": 0.675124, "test_losses": 0.675124, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.221565, "param_norms": 20.564953}, {"epoch": 3200, "train_losses": 0.622735, "test_losses": 0.622735, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.215201, "param_norms": 20.83614}, {"epoch": 3250, "train_losses": 0.572388, "test_losses": 0.572388, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.208102, "param_norms": 21.104493}, {"epoch": 3300, "train_losses": 0.524228, "test_losses": 0.524228, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.200337, "param_norms": 21.369934}, {"epoch": 3350, "train_losses": 0.478375, "test_losses": 0.478375, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.191977, "param_norms": 21.632446}, {"epoch": 3400, "train_losses": 0.434923, "test_losses": 0.434923, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.183102, "param_norms": 21.892057}, {"epoch": 3450, "train_losses": 0.393942, "test_losses": 0.393942, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.173794, "param_norms": 22.148937}, {"epoch": 3500, "train_losses": 0.355477, "test_losses": 0.355477, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.164138, "param_norms": 22.403287}, {"epoch": 3550, "train_losses": 0.31955, "test_losses": 0.31955, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.154225, "param_norms": 22.655502}, {"epoch": 3600, "train_losses": 0.286163, "test_losses": 0.286163, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.144149, "param_norms": 22.906003}, {"epoch": 3650, "train_losses": 0.255295, "test_losses": 0.255295, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.134009, "param_norms": 23.155135}, {"epoch": 3700, "train_losses": 0.226904, "test_losses": 0.226904, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.123908, "param_norms": 23.403272}, {"epoch": 3750, "train_losses": 0.200925, "test_losses": 0.200925, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.113951, "param_norms": 23.650789}, {"epoch": 3800, "train_losses": 0.177277, "test_losses": 0.177277, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.104238, "param_norms": 23.898007}, {"epoch": 3850, "train_losses": 0.155858, "test_losses": 0.155858, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.094859, "param_norms": 24.145105}, {"epoch": 3900, "train_losses": 0.136555, "test_losses": 0.136555, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.085891, "param_norms": 24.392142}, {"epoch": 3950, "train_losses": 0.11924, "test_losses": 0.11924, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.077392, "param_norms": 24.639084}, {"epoch": 4000, "train_losses": 0.10378, "test_losses": 0.10378, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.069407, "param_norms": 24.885889}, {"epoch": 4050, "train_losses": 0.090036, "test_losses": 0.090036, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.061965, "param_norms": 25.132527}, {"epoch": 4100, "train_losses": 0.077871, "test_losses": 0.077871, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.05508, "param_norms": 25.378917}, {"epoch": 4150, "train_losses": 0.067146, "test_losses": 0.067146, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.048756, "param_norms": 25.624985}, {"epoch": 4200, "train_losses": 0.057727, "test_losses": 0.057727, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.042983, "param_norms": 25.870686}, {"epoch": 4250, "train_losses": 0.049488, "test_losses": 0.049488, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.037748, "param_norms": 26.115975}, {"epoch": 4300, "train_losses": 0.042306, "test_losses": 0.042306, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.033026, "param_norms": 26.360854}, {"epoch": 4350, "train_losses": 0.036067, "test_losses": 0.036067, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.028792, "param_norms": 26.605266}, {"epoch": 4400, "train_losses": 0.030666, "test_losses": 0.030666, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.025013, "param_norms": 26.849162}, {"epoch": 4450, "train_losses": 0.026006, "test_losses": 0.026006, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.021658, "param_norms": 27.092535}, {"epoch": 4500, "train_losses": 0.021996, "test_losses": 0.021996, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.018692, "param_norms": 27.335357}, {"epoch": 4550, "train_losses": 0.018558, "test_losses": 0.018558, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016081, "param_norms": 27.577617}, {"epoch": 4600, "train_losses": 0.015618, "test_losses": 0.015618, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013793, "param_norms": 27.819312}, {"epoch": 4650, "train_losses": 0.013112, "test_losses": 0.013112, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011796, "param_norms": 28.060449}, {"epoch": 4700, "train_losses": 0.010981, "test_losses": 0.010981, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010059, "param_norms": 28.300996}, {"epoch": 4750, "train_losses": 0.009175, "test_losses": 0.009175, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008553, "param_norms": 28.540879}, {"epoch": 4800, "train_losses": 0.007647, "test_losses": 0.007647, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007253, "param_norms": 28.78009}, {"epoch": 4850, "train_losses": 0.006359, "test_losses": 0.006359, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006134, "param_norms": 29.018647}, {"epoch": 4900, "train_losses": 0.005276, "test_losses": 0.005276, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005173, "param_norms": 29.256596}, {"epoch": 4950, "train_losses": 0.004367, "test_losses": 0.004367, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004352, "param_norms": 29.493906}, {"epoch": 4999, "train_losses": 0.00362, "test_losses": 0.00362, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003665, "param_norms": 29.725864}], "total_epochs": 5000}, "quad_single_freq": {"config": {"prime": 15, "d_mlp": 512, "act_type": "Quad", "init_type": "single-freq", "init_scale": 0.02, "optimizer": "SGD", "lr": 0.1, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 10000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=15, run=quad_single_freq\n======================================================================\n\nConfiguration:\n  prime (p)       = 15\n  d_mlp           = 512\n  activation      = Quad\n  init_type       = single-freq\n  init_scale      = 0.02\n  optimizer       = SGD\n  learning_rate   = 0.1\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 10000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      2.708081      2.708081      0.0267      0.0267      0.0037       1.7527\n     100      2.707951      2.707951      0.1733      0.1733      0.0037       1.7533\n     200      2.707815      2.707815      0.4267      0.4267      0.0037       1.7561\n     300      2.707675      2.707675      0.7333      0.7333      0.0037       1.7613\n     400      2.707533      2.707533      0.9467      0.9467      0.0038       1.7689\n     500      2.707387      2.707387      0.9911      0.9911      0.0039       1.7788\n     600      2.707236      2.707236      1.0000      1.0000      0.0039       1.7912\n     700      2.707078      2.707078      1.0000      1.0000      0.0040       1.8061\n     800      2.706913      2.706913      1.0000      1.0000      0.0041       1.8235\n     900      2.706738      2.706738      1.0000      1.0000      0.0042       1.8435\n    1000      2.706552      2.706552      1.0000      1.0000      0.0044       1.8662\n    1100      2.706353      2.706353      1.0000      1.0000      0.0045       1.8917\n    1200      2.706138      2.706138      1.0000      1.0000      0.0047       1.9201\n    1300      2.705906      2.705906      1.0000      1.0000      0.0049       1.9515\n    1400      2.705653      2.705653      1.0000      1.0000      0.0051       1.9860\n    1500      2.705376      2.705376      1.0000      1.0000      0.0054       2.0239\n    1600      2.705070      2.705070      1.0000      1.0000      0.0057       2.0653\n    1700      2.704731      2.704731      1.0000      1.0000      0.0060       2.1105\n    1800      2.704353      2.704353      1.0000      1.0000      0.0063       2.1597\n    1900      2.703930      2.703930      1.0000      1.0000      0.0067       2.2133\n    2000      2.703452      2.703452      1.0000      1.0000      0.0071       2.2715\n    2100      2.702910      2.702910      1.0000      1.0000      0.0076       2.3348\n    2200      2.702291      2.702291      1.0000      1.0000      0.0081       2.4037\n    2300      2.701579      2.701579      1.0000      1.0000      0.0087       2.4787\n    2400      2.700757      2.700757      1.0000      1.0000      0.0094       2.5604\n    2500      2.699798      2.699798      1.0000      1.0000      0.0102       2.6497\n    2600      2.698672      2.698672      1.0000      1.0000      0.0111       2.7474\n    2700      2.697340      2.697340      1.0000      1.0000      0.0121       2.8547\n    2800      2.695748      2.695748      1.0000      1.0000      0.0132       2.9728\n    2900      2.693829      2.693829      1.0000      1.0000      0.0145       3.1033\n    3000      2.691490      2.691490      1.0000      1.0000      0.0161       3.2481\n    3100      2.688606      2.688606      1.0000      1.0000      0.0179       3.4096\n    3200      2.685004      2.685004      1.0000      1.0000      0.0201       3.5907\n    3300      2.680436      2.680436      1.0000      1.0000      0.0227       3.7953\n    3400      2.674549      2.674549      1.0000      1.0000      0.0259       4.0281\n    3500      2.666817      2.666817      1.0000      1.0000      0.0298       4.2953\n    3600      2.656435      2.656435      1.0000      1.0000      0.0348       4.6053\n    3700      2.642137      2.642137      1.0000      1.0000      0.0411       4.9694\n    3800      2.621845      2.621845      1.0000      1.0000      0.0493       5.4032\n    3900      2.592003      2.592003      1.0000      1.0000      0.0604       5.9292\n    4000      2.546203      2.546203      1.0000      1.0000      0.0756       6.5804\n    4100      2.472230      2.472230      1.0000      1.0000      0.0974       7.4070\n    4200      2.345479      2.345479      1.0000      1.0000      0.1293       8.4872\n    4300      2.115065      2.115065      1.0000      1.0000      0.1762       9.9403\n    4400      1.689702      1.689702      1.0000      1.0000      0.2357      11.9094\n    4500      1.033969      1.033969      1.0000      1.0000      0.2643      14.3430\n    4600      0.455987      0.455987      1.0000      1.0000      0.2011      16.5894\n    4700      0.200983      0.200983      1.0000      1.0000      0.1201      18.0494\n    4800      0.108569      0.108569      1.0000      1.0000      0.0753      18.9230\n    4900      0.068956      0.068956      1.0000      1.0000      0.0521      19.4946\n    5000      0.048617      0.048617      1.0000      1.0000      0.0388      19.9041\n    5100      0.036716      0.036716      1.0000      1.0000      0.0305      20.2174\n    5200      0.029079      0.029079      1.0000      1.0000      0.0249      20.4683\n    5300      0.023839      0.023839      1.0000      1.0000      0.0210      20.6762\n    5400      0.020057      0.020057      1.0000      1.0000      0.0180      20.8527\n    5500      0.017219      0.017219      1.0000      1.0000      0.0157      21.0056\n    5600      0.015023      0.015023      1.0000      1.0000      0.0139      21.1401\n    5700      0.013281      0.013281      1.0000      1.0000      0.0125      21.2599\n    5800      0.011869      0.011869      1.0000      1.0000      0.0113      21.3677\n    5900      0.010705      0.010705      1.0000      1.0000      0.0103      21.4656\n    6000      0.009731      0.009731      1.0000      1.0000      0.0095      21.5551\n    6100      0.008905      0.008905      1.0000      1.0000      0.0087      21.6376\n    6200      0.008197      0.008197      1.0000      1.0000      0.0081      21.7139\n    6300      0.007585      0.007585      1.0000      1.0000      0.0076      21.7848\n    6400      0.007051      0.007051      1.0000      1.0000      0.0071      21.8512\n    6500      0.006581      0.006581      1.0000      1.0000      0.0066      21.9133\n    6600      0.006165      0.006165      1.0000      1.0000      0.0063      21.9719\n    6700      0.005794      0.005794      1.0000      1.0000      0.0059      22.0271\n    6800      0.005462      0.005462      1.0000      1.0000      0.0056      22.0794\n    6900      0.005163      0.005163      1.0000      1.0000      0.0053      22.1290\n    7000      0.004892      0.004892      1.0000      1.0000      0.0051      22.1762\n    7100      0.004646      0.004646      1.0000      1.0000      0.0048      22.2212\n    7200      0.004421      0.004421      1.0000      1.0000      0.0046      22.2641\n    7300      0.004216      0.004216      1.0000      1.0000      0.0044      22.3053\n    7400      0.004027      0.004027      1.0000      1.0000      0.0043      22.3446\n    7500      0.003854      0.003854      1.0000      1.0000      0.0041      22.3825\n    7600      0.003693      0.003693      1.0000      1.0000      0.0039      22.4188\n    7700      0.003544      0.003544      1.0000      1.0000      0.0038      22.4538\n    7800      0.003406      0.003406      1.0000      1.0000      0.0037      22.4875\n    7900      0.003277      0.003277      1.0000      1.0000      0.0035      22.5200\n    8000      0.003157      0.003157      1.0000      1.0000      0.0034      22.5515\n    8100      0.003045      0.003045      1.0000      1.0000      0.0033      22.5819\n    8200      0.002940      0.002940      1.0000      1.0000      0.0032      22.6113\n    8300      0.002841      0.002841      1.0000      1.0000      0.0031      22.6398\n    8400      0.002748      0.002748      1.0000      1.0000      0.0030      22.6674\n    8500      0.002661      0.002661      1.0000      1.0000      0.0029      22.6942\n    8600      0.002578      0.002578      1.0000      1.0000      0.0028      22.7203\n    8700      0.002501      0.002501      1.0000      1.0000      0.0028      22.7456\n    8800      0.002427      0.002427      1.0000      1.0000      0.0027      22.7702\n    8900      0.002357      0.002357      1.0000      1.0000      0.0026      22.7942\n    9000      0.002291      0.002291      1.0000      1.0000      0.0025      22.8175\n    9100      0.002228      0.002228      1.0000      1.0000      0.0025      22.8402\n    9200      0.002168      0.002168      1.0000      1.0000      0.0024      22.8624\n    9300      0.002111      0.002111      1.0000      1.0000      0.0024      22.8840\n    9400      0.002057      0.002057      1.0000      1.0000      0.0023      22.9052\n    9500      0.002005      0.002005      1.0000      1.0000      0.0022      22.9258\n    9600      0.001956      0.001956      1.0000      1.0000      0.0022      22.9459\n    9700      0.001909      0.001909      1.0000      1.0000      0.0021      22.9656\n    9800      0.001864      0.001864      1.0000      1.0000      0.0021      22.9849\n    9900      0.001821      0.001821      1.0000      1.0000      0.0021      23.0038\n    9999      0.001780      0.001780      1.0000      1.0000      0.0020      23.0220\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.001780\n  Test Loss   = 0.001780\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 23.0220\n\nTotal epochs trained: 10000\n", "table": [{"epoch": 0, "train_losses": 2.708081, "test_losses": 2.708081, "train_accs": 0.026667, "test_accs": 0.026667, "grad_norms": 0.003695, "param_norms": 1.752712}, {"epoch": 100, "train_losses": 2.707951, "test_losses": 2.707951, "train_accs": 0.173333, "test_accs": 0.173333, "grad_norms": 0.003699, "param_norms": 1.753253}, {"epoch": 200, "train_losses": 2.707815, "test_losses": 2.707815, "train_accs": 0.426667, "test_accs": 0.426667, "grad_norms": 0.003716, "param_norms": 1.756101}, {"epoch": 300, "train_losses": 2.707675, "test_losses": 2.707675, "train_accs": 0.733333, "test_accs": 0.733333, "grad_norms": 0.003748, "param_norms": 1.7613}, {"epoch": 400, "train_losses": 2.707533, "test_losses": 2.707533, "train_accs": 0.946667, "test_accs": 0.946667, "grad_norms": 0.003793, "param_norms": 1.768868}, {"epoch": 500, "train_losses": 2.707387, "test_losses": 2.707387, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.003853, "param_norms": 1.778832}, {"epoch": 600, "train_losses": 2.707236, "test_losses": 2.707236, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003927, "param_norms": 1.79123}, {"epoch": 700, "train_losses": 2.707078, "test_losses": 2.707078, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004017, "param_norms": 1.806106}, {"epoch": 800, "train_losses": 2.706913, "test_losses": 2.706913, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004122, "param_norms": 1.823519}, {"epoch": 900, "train_losses": 2.706738, "test_losses": 2.706738, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004245, "param_norms": 1.843535}, {"epoch": 1000, "train_losses": 2.706552, "test_losses": 2.706552, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004384, "param_norms": 1.866235}, {"epoch": 1100, "train_losses": 2.706353, "test_losses": 2.706353, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004543, "param_norms": 1.891715}, {"epoch": 1200, "train_losses": 2.706138, "test_losses": 2.706138, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004721, "param_norms": 1.920084}, {"epoch": 1300, "train_losses": 2.705906, "test_losses": 2.705906, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004921, "param_norms": 1.95147}, {"epoch": 1400, "train_losses": 2.705653, "test_losses": 2.705653, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005144, "param_norms": 1.986022}, {"epoch": 1500, "train_losses": 2.705376, "test_losses": 2.705376, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005393, "param_norms": 2.023911}, {"epoch": 1600, "train_losses": 2.70507, "test_losses": 2.70507, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00567, "param_norms": 2.065333}, {"epoch": 1700, "train_losses": 2.704731, "test_losses": 2.704731, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005978, "param_norms": 2.110519}, {"epoch": 1800, "train_losses": 2.704353, "test_losses": 2.704353, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006321, "param_norms": 2.15973}, {"epoch": 1900, "train_losses": 2.70393, "test_losses": 2.70393, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006703, "param_norms": 2.213269}, {"epoch": 2000, "train_losses": 2.703452, "test_losses": 2.703452, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007129, "param_norms": 2.27149}, {"epoch": 2100, "train_losses": 2.70291, "test_losses": 2.70291, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007606, "param_norms": 2.334798}, {"epoch": 2200, "train_losses": 2.702291, "test_losses": 2.702291, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008139, "param_norms": 2.403668}, {"epoch": 2300, "train_losses": 2.701579, "test_losses": 2.701579, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008739, "param_norms": 2.478657}, {"epoch": 2400, "train_losses": 2.700757, "test_losses": 2.700757, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009416, "param_norms": 2.560414}, {"epoch": 2500, "train_losses": 2.699798, "test_losses": 2.699798, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010182, "param_norms": 2.649703}, {"epoch": 2600, "train_losses": 2.698672, "test_losses": 2.698672, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011055, "param_norms": 2.747435}, {"epoch": 2700, "train_losses": 2.69734, "test_losses": 2.69734, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012053, "param_norms": 2.854692}, {"epoch": 2800, "train_losses": 2.695748, "test_losses": 2.695748, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013202, "param_norms": 2.972774}, {"epoch": 2900, "train_losses": 2.693829, "test_losses": 2.693829, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014534, "param_norms": 3.103256}, {"epoch": 3000, "train_losses": 2.69149, "test_losses": 2.69149, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016089, "param_norms": 3.248062}, {"epoch": 3100, "train_losses": 2.688606, "test_losses": 2.688606, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01792, "param_norms": 3.409565}, {"epoch": 3200, "train_losses": 2.685004, "test_losses": 2.685004, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.020097, "param_norms": 3.590727}, {"epoch": 3300, "train_losses": 2.680436, "test_losses": 2.680436, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.022714, "param_norms": 3.795291}, {"epoch": 3400, "train_losses": 2.674549, "test_losses": 2.674549, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0259, "param_norms": 4.028061}, {"epoch": 3500, "train_losses": 2.666817, "test_losses": 2.666817, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.029832, "param_norms": 4.295294}, {"epoch": 3600, "train_losses": 2.656435, "test_losses": 2.656435, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.034765, "param_norms": 4.605308}, {"epoch": 3700, "train_losses": 2.642137, "test_losses": 2.642137, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.041074, "param_norms": 4.969381}, {"epoch": 3800, "train_losses": 2.621845, "test_losses": 2.621845, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.049318, "param_norms": 5.403206}, {"epoch": 3900, "train_losses": 2.592003, "test_losses": 2.592003, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.060371, "param_norms": 5.929201}, {"epoch": 4000, "train_losses": 2.546203, "test_losses": 2.546203, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.075629, "param_norms": 6.580399}, {"epoch": 4100, "train_losses": 2.47223, "test_losses": 2.47223, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.097376, "param_norms": 7.407022}, {"epoch": 4200, "train_losses": 2.345479, "test_losses": 2.345479, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.129287, "param_norms": 8.487206}, {"epoch": 4300, "train_losses": 2.115065, "test_losses": 2.115065, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.17615, "param_norms": 9.94026}, {"epoch": 4400, "train_losses": 1.689702, "test_losses": 1.689702, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.235651, "param_norms": 11.909365}, {"epoch": 4500, "train_losses": 1.033969, "test_losses": 1.033969, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.264259, "param_norms": 14.343005}, {"epoch": 4600, "train_losses": 0.455987, "test_losses": 0.455987, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.201054, "param_norms": 16.589395}, {"epoch": 4700, "train_losses": 0.200983, "test_losses": 0.200983, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.120068, "param_norms": 18.049367}, {"epoch": 4800, "train_losses": 0.108569, "test_losses": 0.108569, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.075309, "param_norms": 18.922972}, {"epoch": 4900, "train_losses": 0.068956, "test_losses": 0.068956, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.052087, "param_norms": 19.494588}, {"epoch": 5000, "train_losses": 0.048617, "test_losses": 0.048617, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.038837, "param_norms": 19.904144}, {"epoch": 5100, "train_losses": 0.036716, "test_losses": 0.036716, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.030538, "param_norms": 20.217401}, {"epoch": 5200, "train_losses": 0.029079, "test_losses": 0.029079, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.024948, "param_norms": 20.468329}, {"epoch": 5300, "train_losses": 0.023839, "test_losses": 0.023839, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.020968, "param_norms": 20.676155}, {"epoch": 5400, "train_losses": 0.020057, "test_losses": 0.020057, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01801, "param_norms": 20.852677}, {"epoch": 5500, "train_losses": 0.017219, "test_losses": 0.017219, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.015737, "param_norms": 21.005571}, {"epoch": 5600, "train_losses": 0.015023, "test_losses": 0.015023, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013941, "param_norms": 21.140063}, {"epoch": 5700, "train_losses": 0.013281, "test_losses": 0.013281, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01249, "param_norms": 21.259856}, {"epoch": 5800, "train_losses": 0.011869, "test_losses": 0.011869, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011296, "param_norms": 21.367679}, {"epoch": 5900, "train_losses": 0.010705, "test_losses": 0.010705, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010298, "param_norms": 21.465574}, {"epoch": 6000, "train_losses": 0.009731, "test_losses": 0.009731, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009453, "param_norms": 21.555119}, {"epoch": 6100, "train_losses": 0.008905, "test_losses": 0.008905, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008728, "param_norms": 21.637556}, {"epoch": 6200, "train_losses": 0.008197, "test_losses": 0.008197, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008101, "param_norms": 21.713863}, {"epoch": 6300, "train_losses": 0.007585, "test_losses": 0.007585, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007553, "param_norms": 21.784848}, {"epoch": 6400, "train_losses": 0.007051, "test_losses": 0.007051, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007071, "param_norms": 21.851159}, {"epoch": 6500, "train_losses": 0.006581, "test_losses": 0.006581, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006644, "param_norms": 21.913344}, {"epoch": 6600, "train_losses": 0.006165, "test_losses": 0.006165, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006262, "param_norms": 21.971866}, {"epoch": 6700, "train_losses": 0.005794, "test_losses": 0.005794, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00592, "param_norms": 22.027099}, {"epoch": 6800, "train_losses": 0.005462, "test_losses": 0.005462, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005611, "param_norms": 22.079387}, {"epoch": 6900, "train_losses": 0.005163, "test_losses": 0.005163, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005331, "param_norms": 22.129006}, {"epoch": 7000, "train_losses": 0.004892, "test_losses": 0.004892, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005077, "param_norms": 22.176198}, {"epoch": 7100, "train_losses": 0.004646, "test_losses": 0.004646, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004844, "param_norms": 22.221178}, {"epoch": 7200, "train_losses": 0.004421, "test_losses": 0.004421, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004631, "param_norms": 22.264145}, {"epoch": 7300, "train_losses": 0.004216, "test_losses": 0.004216, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004434, "param_norms": 22.305256}, {"epoch": 7400, "train_losses": 0.004027, "test_losses": 0.004027, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004253, "param_norms": 22.344645}, {"epoch": 7500, "train_losses": 0.003854, "test_losses": 0.003854, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004086, "param_norms": 22.382456}, {"epoch": 7600, "train_losses": 0.003693, "test_losses": 0.003693, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00393, "param_norms": 22.418806}, {"epoch": 7700, "train_losses": 0.003544, "test_losses": 0.003544, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003785, "param_norms": 22.453799}, {"epoch": 7800, "train_losses": 0.003406, "test_losses": 0.003406, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00365, "param_norms": 22.487513}, {"epoch": 7900, "train_losses": 0.003277, "test_losses": 0.003277, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003524, "param_norms": 22.520048}, {"epoch": 8000, "train_losses": 0.003157, "test_losses": 0.003157, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003406, "param_norms": 22.551476}, {"epoch": 8100, "train_losses": 0.003045, "test_losses": 0.003045, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003295, "param_norms": 22.581864}, {"epoch": 8200, "train_losses": 0.00294, "test_losses": 0.00294, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003191, "param_norms": 22.61128}, {"epoch": 8300, "train_losses": 0.002841, "test_losses": 0.002841, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003093, "param_norms": 22.639775}, {"epoch": 8400, "train_losses": 0.002748, "test_losses": 0.002748, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003, "param_norms": 22.667399}, {"epoch": 8500, "train_losses": 0.002661, "test_losses": 0.002661, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002913, "param_norms": 22.694218}, {"epoch": 8600, "train_losses": 0.002578, "test_losses": 0.002578, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00283, "param_norms": 22.720259}, {"epoch": 8700, "train_losses": 0.002501, "test_losses": 0.002501, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002752, "param_norms": 22.745571}, {"epoch": 8800, "train_losses": 0.002427, "test_losses": 0.002427, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002677, "param_norms": 22.770194}, {"epoch": 8900, "train_losses": 0.002357, "test_losses": 0.002357, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002607, "param_norms": 22.794154}, {"epoch": 9000, "train_losses": 0.002291, "test_losses": 0.002291, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002539, "param_norms": 22.817489}, {"epoch": 9100, "train_losses": 0.002228, "test_losses": 0.002228, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002475, "param_norms": 22.840235}, {"epoch": 9200, "train_losses": 0.002168, "test_losses": 0.002168, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002414, "param_norms": 22.862405}, {"epoch": 9300, "train_losses": 0.002111, "test_losses": 0.002111, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002356, "param_norms": 22.884038}, {"epoch": 9400, "train_losses": 0.002057, "test_losses": 0.002057, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002301, "param_norms": 22.90516}, {"epoch": 9500, "train_losses": 0.002005, "test_losses": 0.002005, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002247, "param_norms": 22.925781}, {"epoch": 9600, "train_losses": 0.001956, "test_losses": 0.001956, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002197, "param_norms": 22.945941}, {"epoch": 9700, "train_losses": 0.001909, "test_losses": 0.001909, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002148, "param_norms": 22.965638}, {"epoch": 9800, "train_losses": 0.001864, "test_losses": 0.001864, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002101, "param_norms": 22.984914}, {"epoch": 9900, "train_losses": 0.001821, "test_losses": 0.001821, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002056, "param_norms": 23.003762}, {"epoch": 9999, "train_losses": 0.00178, "test_losses": 0.00178, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002014, "param_norms": 23.02204}], "total_epochs": 10000}, "relu_single_freq": {"config": {"prime": 15, "d_mlp": 512, "act_type": "ReLU", "init_type": "single-freq", "init_scale": 0.002, "optimizer": "SGD", "lr": 0.01, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 10000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=15, run=relu_single_freq\n======================================================================\n\nConfiguration:\n  prime (p)       = 15\n  d_mlp           = 512\n  activation      = ReLU\n  init_type       = single-freq\n  init_scale      = 0.002\n  optimizer       = SGD\n  learning_rate   = 0.01\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 10000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      2.708055      2.708055      0.0489      0.0489      0.0045       0.1753\n     100      2.708035      2.708035      0.0933      0.0933      0.0045       0.1753\n     200      2.708015      2.708015      0.2444      0.2444      0.0045       0.1756\n     300      2.707995      2.707995      0.3556      0.3556      0.0045       0.1761\n     400      2.707975      2.707975      0.5644      0.5644      0.0045       0.1769\n     500      2.707954      2.707954      0.6889      0.6889      0.0045       0.1778\n     600      2.707935      2.707935      0.8178      0.8178      0.0045       0.1790\n     700      2.707915      2.707915      0.8800      0.8800      0.0045       0.1804\n     800      2.707894      2.707894      0.9422      0.9422      0.0046       0.1820\n     900      2.707875      2.707875      0.9600      0.9600      0.0046       0.1838\n    1000      2.707855      2.707855      0.9778      0.9778      0.0046       0.1858\n    1100      2.707834      2.707834      0.9956      0.9956      0.0046       0.1880\n    1200      2.707814      2.707814      1.0000      1.0000      0.0047       0.1904\n    1300      2.707793      2.707793      1.0000      1.0000      0.0047       0.1930\n    1400      2.707773      2.707773      1.0000      1.0000      0.0047       0.1957\n    1500      2.707753      2.707753      1.0000      1.0000      0.0048       0.1987\n    1600      2.707732      2.707732      1.0000      1.0000      0.0049       0.2017\n    1700      2.707710      2.707710      1.0000      1.0000      0.0049       0.2050\n    1800      2.707689      2.707689      1.0000      1.0000      0.0050       0.2084\n    1900      2.707667      2.707667      1.0000      1.0000      0.0051       0.2119\n    2000      2.707644      2.707644      1.0000      1.0000      0.0052       0.2156\n    2100      2.707621      2.707621      1.0000      1.0000      0.0053       0.2194\n    2200      2.707597      2.707597      1.0000      1.0000      0.0054       0.2234\n    2300      2.707573      2.707573      1.0000      1.0000      0.0055       0.2276\n    2400      2.707548      2.707548      1.0000      1.0000      0.0056       0.2318\n    2500      2.707522      2.707522      1.0000      1.0000      0.0057       0.2362\n    2600      2.707496      2.707496      1.0000      1.0000      0.0058       0.2408\n    2700      2.707468      2.707468      1.0000      1.0000      0.0059       0.2454\n    2800      2.707440      2.707440      1.0000      1.0000      0.0060       0.2502\n    2900      2.707411      2.707411      1.0000      1.0000      0.0062       0.2552\n    3000      2.707380      2.707380      1.0000      1.0000      0.0063       0.2603\n    3100      2.707349      2.707349      1.0000      1.0000      0.0064       0.2655\n    3200      2.707317      2.707317      1.0000      1.0000      0.0066       0.2708\n    3300      2.707283      2.707283      1.0000      1.0000      0.0067       0.2763\n    3400      2.707247      2.707247      1.0000      1.0000      0.0069       0.2819\n    3500      2.707211      2.707211      1.0000      1.0000      0.0070       0.2877\n    3600      2.707173      2.707173      1.0000      1.0000      0.0072       0.2936\n    3700      2.707133      2.707133      1.0000      1.0000      0.0073       0.2997\n    3800      2.707092      2.707092      1.0000      1.0000      0.0075       0.3058\n    3900      2.707049      2.707049      1.0000      1.0000      0.0077       0.3122\n    4000      2.707005      2.707005      1.0000      1.0000      0.0079       0.3187\n    4100      2.706959      2.706959      1.0000      1.0000      0.0081       0.3253\n    4200      2.706910      2.706910      1.0000      1.0000      0.0083       0.3321\n    4300      2.706860      2.706860      1.0000      1.0000      0.0084       0.3390\n    4400      2.706807      2.706807      1.0000      1.0000      0.0086       0.3461\n    4500      2.706752      2.706752      1.0000      1.0000      0.0088       0.3534\n    4600      2.706695      2.706695      1.0000      1.0000      0.0090       0.3608\n    4700      2.706635      2.706635      1.0000      1.0000      0.0092       0.3684\n    4800      2.706573      2.706573      1.0000      1.0000      0.0094       0.3762\n    4900      2.706508      2.706508      1.0000      1.0000      0.0096       0.3841\n    5000      2.706440      2.706440      1.0000      1.0000      0.0098       0.3923\n    5100      2.706369      2.706369      1.0000      1.0000      0.0101       0.4006\n    5200      2.706295      2.706295      1.0000      1.0000      0.0103       0.4090\n    5300      2.706218      2.706218      1.0000      1.0000      0.0105       0.4177\n    5400      2.706137      2.706137      1.0000      1.0000      0.0108       0.4266\n    5500      2.706053      2.706053      1.0000      1.0000      0.0110       0.4357\n    5600      2.705965      2.705965      1.0000      1.0000      0.0113       0.4449\n    5700      2.705873      2.705873      1.0000      1.0000      0.0115       0.4544\n    5800      2.705777      2.705777      1.0000      1.0000      0.0118       0.4641\n    5900      2.705677      2.705677      1.0000      1.0000      0.0120       0.4740\n    6000      2.705573      2.705573      1.0000      1.0000      0.0123       0.4841\n    6100      2.705464      2.705464      1.0000      1.0000      0.0126       0.4944\n    6200      2.705349      2.705349      1.0000      1.0000      0.0130       0.5050\n    6300      2.705230      2.705230      1.0000      1.0000      0.0132       0.5158\n    6400      2.705106      2.705106      1.0000      1.0000      0.0135       0.5269\n    6500      2.704976      2.704976      1.0000      1.0000      0.0137       0.5382\n    6600      2.704840      2.704840      1.0000      1.0000      0.0141       0.5497\n    6700      2.704699      2.704699      1.0000      1.0000      0.0144       0.5615\n    6800      2.704551      2.704551      1.0000      1.0000      0.0147       0.5736\n    6900      2.704396      2.704396      1.0000      1.0000      0.0151       0.5859\n    7000      2.704234      2.704234      1.0000      1.0000      0.0154       0.5985\n    7100      2.704065      2.704065      1.0000      1.0000      0.0158       0.6114\n    7200      2.703889      2.703889      1.0000      1.0000      0.0161       0.6246\n    7300      2.703704      2.703704      1.0000      1.0000      0.0165       0.6381\n    7400      2.703512      2.703512      1.0000      1.0000      0.0169       0.6518\n    7500      2.703311      2.703311      1.0000      1.0000      0.0172       0.6659\n    7600      2.703101      2.703101      1.0000      1.0000      0.0177       0.6803\n    7700      2.702881      2.702881      1.0000      1.0000      0.0180       0.6950\n    7800      2.702651      2.702651      1.0000      1.0000      0.0184       0.7100\n    7900      2.702412      2.702412      1.0000      1.0000      0.0188       0.7254\n    8000      2.702162      2.702162      1.0000      1.0000      0.0193       0.7411\n    8100      2.701900      2.701900      1.0000      1.0000      0.0197       0.7572\n    8200      2.701626      2.701626      1.0000      1.0000      0.0201       0.7736\n    8300      2.701340      2.701340      1.0000      1.0000      0.0206       0.7904\n    8400      2.701041      2.701041      1.0000      1.0000      0.0211       0.8076\n    8500      2.700729      2.700729      1.0000      1.0000      0.0216       0.8251\n    8600      2.700402      2.700402      1.0000      1.0000      0.0221       0.8430\n    8700      2.700062      2.700062      1.0000      1.0000      0.0225       0.8614\n    8800      2.699705      2.699705      1.0000      1.0000      0.0230       0.8801\n    8900      2.699333      2.699333      1.0000      1.0000      0.0236       0.8993\n    9000      2.698943      2.698943      1.0000      1.0000      0.0241       0.9189\n    9100      2.698536      2.698536      1.0000      1.0000      0.0247       0.9389\n    9200      2.698112      2.698112      1.0000      1.0000      0.0253       0.9594\n    9300      2.697667      2.697667      1.0000      1.0000      0.0259       0.9803\n    9400      2.697203      2.697203      1.0000      1.0000      0.0264       1.0017\n    9500      2.696718      2.696718      1.0000      1.0000      0.0271       1.0236\n    9600      2.696210      2.696210      1.0000      1.0000      0.0276       1.0460\n    9700      2.695680      2.695680      1.0000      1.0000      0.0282       1.0689\n    9800      2.695125      2.695125      1.0000      1.0000      0.0289       1.0923\n    9900      2.694546      2.694546      1.0000      1.0000      0.0296       1.1162\n    9999      2.693947      2.693947      1.0000      1.0000      0.0302       1.1404\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 2.693947\n  Test Loss   = 2.693947\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 1.1404\n\nTotal epochs trained: 10000\n", "table": [{"epoch": 0, "train_losses": 2.708055, "test_losses": 2.708055, "train_accs": 0.048889, "test_accs": 0.048889, "grad_norms": 0.004524, "param_norms": 0.175271}, {"epoch": 100, "train_losses": 2.708035, "test_losses": 2.708035, "train_accs": 0.093333, "test_accs": 0.093333, "grad_norms": 0.004521, "param_norms": 0.175328}, {"epoch": 200, "train_losses": 2.708015, "test_losses": 2.708015, "train_accs": 0.244444, "test_accs": 0.244444, "grad_norms": 0.004512, "param_norms": 0.175614}, {"epoch": 300, "train_losses": 2.707995, "test_losses": 2.707995, "train_accs": 0.355556, "test_accs": 0.355556, "grad_norms": 0.004511, "param_norms": 0.17613}, {"epoch": 400, "train_losses": 2.707975, "test_losses": 2.707975, "train_accs": 0.564444, "test_accs": 0.564444, "grad_norms": 0.004516, "param_norms": 0.176872}, {"epoch": 500, "train_losses": 2.707954, "test_losses": 2.707954, "train_accs": 0.688889, "test_accs": 0.688889, "grad_norms": 0.004526, "param_norms": 0.177838}, {"epoch": 600, "train_losses": 2.707935, "test_losses": 2.707935, "train_accs": 0.817778, "test_accs": 0.817778, "grad_norms": 0.004514, "param_norms": 0.179023}, {"epoch": 700, "train_losses": 2.707915, "test_losses": 2.707915, "train_accs": 0.88, "test_accs": 0.88, "grad_norms": 0.004521, "param_norms": 0.180422}, {"epoch": 800, "train_losses": 2.707894, "test_losses": 2.707894, "train_accs": 0.942222, "test_accs": 0.942222, "grad_norms": 0.004551, "param_norms": 0.182029}, {"epoch": 900, "train_losses": 2.707875, "test_losses": 2.707875, "train_accs": 0.96, "test_accs": 0.96, "grad_norms": 0.004576, "param_norms": 0.183839}, {"epoch": 1000, "train_losses": 2.707855, "test_losses": 2.707855, "train_accs": 0.977778, "test_accs": 0.977778, "grad_norms": 0.004599, "param_norms": 0.185848}, {"epoch": 1100, "train_losses": 2.707834, "test_losses": 2.707834, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.00463, "param_norms": 0.18805}, {"epoch": 1200, "train_losses": 2.707814, "test_losses": 2.707814, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00467, "param_norms": 0.190438}, {"epoch": 1300, "train_losses": 2.707793, "test_losses": 2.707793, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004709, "param_norms": 0.193006}, {"epoch": 1400, "train_losses": 2.707773, "test_losses": 2.707773, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004745, "param_norms": 0.195749}, {"epoch": 1500, "train_losses": 2.707753, "test_losses": 2.707753, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0048, "param_norms": 0.198662}, {"epoch": 1600, "train_losses": 2.707732, "test_losses": 2.707732, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004863, "param_norms": 0.201738}, {"epoch": 1700, "train_losses": 2.70771, "test_losses": 2.70771, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004927, "param_norms": 0.204974}, {"epoch": 1800, "train_losses": 2.707689, "test_losses": 2.707689, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005007, "param_norms": 0.208366}, {"epoch": 1900, "train_losses": 2.707667, "test_losses": 2.707667, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005109, "param_norms": 0.211911}, {"epoch": 2000, "train_losses": 2.707644, "test_losses": 2.707644, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0052, "param_norms": 0.215604}, {"epoch": 2100, "train_losses": 2.707621, "test_losses": 2.707621, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005277, "param_norms": 0.219444}, {"epoch": 2200, "train_losses": 2.707597, "test_losses": 2.707597, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005383, "param_norms": 0.223428}, {"epoch": 2300, "train_losses": 2.707573, "test_losses": 2.707573, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005477, "param_norms": 0.227553}, {"epoch": 2400, "train_losses": 2.707548, "test_losses": 2.707548, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005589, "param_norms": 0.231817}, {"epoch": 2500, "train_losses": 2.707522, "test_losses": 2.707522, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005699, "param_norms": 0.23622}, {"epoch": 2600, "train_losses": 2.707496, "test_losses": 2.707496, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005798, "param_norms": 0.240759}, {"epoch": 2700, "train_losses": 2.707468, "test_losses": 2.707468, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005914, "param_norms": 0.245433}, {"epoch": 2800, "train_losses": 2.70744, "test_losses": 2.70744, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006048, "param_norms": 0.250243}, {"epoch": 2900, "train_losses": 2.707411, "test_losses": 2.707411, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006177, "param_norms": 0.255188}, {"epoch": 3000, "train_losses": 2.70738, "test_losses": 2.70738, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006307, "param_norms": 0.260267}, {"epoch": 3100, "train_losses": 2.707349, "test_losses": 2.707349, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00644, "param_norms": 0.265482}, {"epoch": 3200, "train_losses": 2.707317, "test_losses": 2.707317, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006562, "param_norms": 0.270832}, {"epoch": 3300, "train_losses": 2.707283, "test_losses": 2.707283, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006727, "param_norms": 0.276319}, {"epoch": 3400, "train_losses": 2.707247, "test_losses": 2.707247, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006852, "param_norms": 0.281943}, {"epoch": 3500, "train_losses": 2.707211, "test_losses": 2.707211, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007005, "param_norms": 0.287707}, {"epoch": 3600, "train_losses": 2.707173, "test_losses": 2.707173, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007194, "param_norms": 0.293611}, {"epoch": 3700, "train_losses": 2.707133, "test_losses": 2.707133, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007323, "param_norms": 0.299658}, {"epoch": 3800, "train_losses": 2.707092, "test_losses": 2.707092, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00751, "param_norms": 0.305848}, {"epoch": 3900, "train_losses": 2.707049, "test_losses": 2.707049, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007714, "param_norms": 0.312185}, {"epoch": 4000, "train_losses": 2.707005, "test_losses": 2.707005, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007879, "param_norms": 0.31867}, {"epoch": 4100, "train_losses": 2.706959, "test_losses": 2.706959, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008085, "param_norms": 0.325305}, {"epoch": 4200, "train_losses": 2.70691, "test_losses": 2.70691, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008255, "param_norms": 0.332094}, {"epoch": 4300, "train_losses": 2.70686, "test_losses": 2.70686, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00842, "param_norms": 0.339038}, {"epoch": 4400, "train_losses": 2.706807, "test_losses": 2.706807, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008604, "param_norms": 0.34614}, {"epoch": 4500, "train_losses": 2.706752, "test_losses": 2.706752, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008814, "param_norms": 0.353404}, {"epoch": 4600, "train_losses": 2.706695, "test_losses": 2.706695, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009006, "param_norms": 0.360832}, {"epoch": 4700, "train_losses": 2.706635, "test_losses": 2.706635, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009206, "param_norms": 0.368428}, {"epoch": 4800, "train_losses": 2.706573, "test_losses": 2.706573, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009409, "param_norms": 0.376194}, {"epoch": 4900, "train_losses": 2.706508, "test_losses": 2.706508, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009644, "param_norms": 0.384136}, {"epoch": 5000, "train_losses": 2.70644, "test_losses": 2.70644, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009817, "param_norms": 0.392255}, {"epoch": 5100, "train_losses": 2.706369, "test_losses": 2.706369, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01006, "param_norms": 0.400555}, {"epoch": 5200, "train_losses": 2.706295, "test_losses": 2.706295, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010296, "param_norms": 0.409042}, {"epoch": 5300, "train_losses": 2.706218, "test_losses": 2.706218, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010538, "param_norms": 0.417717}, {"epoch": 5400, "train_losses": 2.706137, "test_losses": 2.706137, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010761, "param_norms": 0.426586}, {"epoch": 5500, "train_losses": 2.706053, "test_losses": 2.706053, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01098, "param_norms": 0.435653}, {"epoch": 5600, "train_losses": 2.705965, "test_losses": 2.705965, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011275, "param_norms": 0.444921}, {"epoch": 5700, "train_losses": 2.705873, "test_losses": 2.705873, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011523, "param_norms": 0.454396}, {"epoch": 5800, "train_losses": 2.705777, "test_losses": 2.705777, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011797, "param_norms": 0.464081}, {"epoch": 5900, "train_losses": 2.705677, "test_losses": 2.705677, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012029, "param_norms": 0.473982}, {"epoch": 6000, "train_losses": 2.705573, "test_losses": 2.705573, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012342, "param_norms": 0.484103}, {"epoch": 6100, "train_losses": 2.705464, "test_losses": 2.705464, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012649, "param_norms": 0.494448}, {"epoch": 6200, "train_losses": 2.705349, "test_losses": 2.705349, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012952, "param_norms": 0.505024}, {"epoch": 6300, "train_losses": 2.70523, "test_losses": 2.70523, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013225, "param_norms": 0.515834}, {"epoch": 6400, "train_losses": 2.705106, "test_losses": 2.705106, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013458, "param_norms": 0.526884}, {"epoch": 6500, "train_losses": 2.704976, "test_losses": 2.704976, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013746, "param_norms": 0.53818}, {"epoch": 6600, "train_losses": 2.70484, "test_losses": 2.70484, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014084, "param_norms": 0.549727}, {"epoch": 6700, "train_losses": 2.704699, "test_losses": 2.704699, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014401, "param_norms": 0.56153}, {"epoch": 6800, "train_losses": 2.704551, "test_losses": 2.704551, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014723, "param_norms": 0.573596}, {"epoch": 6900, "train_losses": 2.704396, "test_losses": 2.704396, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.015097, "param_norms": 0.58593}, {"epoch": 7000, "train_losses": 2.704234, "test_losses": 2.704234, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.015391, "param_norms": 0.598538}, {"epoch": 7100, "train_losses": 2.704065, "test_losses": 2.704065, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.015774, "param_norms": 0.611427}, {"epoch": 7200, "train_losses": 2.703889, "test_losses": 2.703889, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016119, "param_norms": 0.624602}, {"epoch": 7300, "train_losses": 2.703704, "test_losses": 2.703704, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016459, "param_norms": 0.638071}, {"epoch": 7400, "train_losses": 2.703512, "test_losses": 2.703512, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016852, "param_norms": 0.651838}, {"epoch": 7500, "train_losses": 2.703311, "test_losses": 2.703311, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01717, "param_norms": 0.665913}, {"epoch": 7600, "train_losses": 2.703101, "test_losses": 2.703101, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.017677, "param_norms": 0.6803}, {"epoch": 7700, "train_losses": 2.702881, "test_losses": 2.702881, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.018043, "param_norms": 0.695008}, {"epoch": 7800, "train_losses": 2.702651, "test_losses": 2.702651, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.018403, "param_norms": 0.710043}, {"epoch": 7900, "train_losses": 2.702412, "test_losses": 2.702412, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.018806, "param_norms": 0.725413}, {"epoch": 8000, "train_losses": 2.702162, "test_losses": 2.702162, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.019288, "param_norms": 0.741125}, {"epoch": 8100, "train_losses": 2.7019, "test_losses": 2.7019, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.019741, "param_norms": 0.757187}, {"epoch": 8200, "train_losses": 2.701626, "test_losses": 2.701626, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.020126, "param_norms": 0.773607}, {"epoch": 8300, "train_losses": 2.70134, "test_losses": 2.70134, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.020628, "param_norms": 0.790394}, {"epoch": 8400, "train_losses": 2.701041, "test_losses": 2.701041, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.021093, "param_norms": 0.807555}, {"epoch": 8500, "train_losses": 2.700729, "test_losses": 2.700729, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.021578, "param_norms": 0.825099}, {"epoch": 8600, "train_losses": 2.700402, "test_losses": 2.700402, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.022071, "param_norms": 0.843035}, {"epoch": 8700, "train_losses": 2.700062, "test_losses": 2.700062, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.022521, "param_norms": 0.861371}, {"epoch": 8800, "train_losses": 2.699705, "test_losses": 2.699705, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.023044, "param_norms": 0.880118}, {"epoch": 8900, "train_losses": 2.699333, "test_losses": 2.699333, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.023561, "param_norms": 0.899283}, {"epoch": 9000, "train_losses": 2.698943, "test_losses": 2.698943, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.024131, "param_norms": 0.918876}, {"epoch": 9100, "train_losses": 2.698536, "test_losses": 2.698536, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.024717, "param_norms": 0.938908}, {"epoch": 9200, "train_losses": 2.698112, "test_losses": 2.698112, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.025283, "param_norms": 0.959387}, {"epoch": 9300, "train_losses": 2.697667, "test_losses": 2.697667, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.025878, "param_norms": 0.980324}, {"epoch": 9400, "train_losses": 2.697203, "test_losses": 2.697203, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.026438, "param_norms": 1.00173}, {"epoch": 9500, "train_losses": 2.696718, "test_losses": 2.696718, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.027114, "param_norms": 1.023615}, {"epoch": 9600, "train_losses": 2.69621, "test_losses": 2.69621, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.02764, "param_norms": 1.04599}, {"epoch": 9700, "train_losses": 2.69568, "test_losses": 2.69568, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.028233, "param_norms": 1.068866}, {"epoch": 9800, "train_losses": 2.695125, "test_losses": 2.695125, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.028868, "param_norms": 1.092254}, {"epoch": 9900, "train_losses": 2.694546, "test_losses": 2.694546, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.02964, "param_norms": 1.116166}, {"epoch": 9999, "train_losses": 2.693947, "test_losses": 2.693947, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.030195, "param_norms": 1.140367}], "total_epochs": 10000}}