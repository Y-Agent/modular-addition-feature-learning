{"standard": {"config": {"prime": 15, "d_mlp": 512, "act_type": "ReLU", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 5e-05, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=15, run=standard\n======================================================================\n\nConfiguration:\n  prime (p)       = 15\n  d_mlp           = 512\n  activation      = ReLU\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 5e-05\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      2.708508      2.708508      0.0533      0.0533      0.0346       3.2100\n      50      2.701765      2.701765      0.1111      0.1111      0.0340       3.2262\n     100      2.694151      2.694151      0.3022      0.3022      0.0348       3.2977\n     150      2.686129      2.686129      0.5156      0.5156      0.0364       3.4137\n     200      2.677509      2.677509      0.6844      0.6844      0.0388       3.5632\n     250      2.668158      2.668158      0.8267      0.8267      0.0419       3.7387\n     300      2.657879      2.657879      0.8889      0.8889      0.0455       3.9345\n     350      2.646547      2.646547      0.9333      0.9333      0.0496       4.1460\n     400      2.634130      2.634130      0.9511      0.9511      0.0538       4.3693\n     450      2.620615      2.620615      0.9600      0.9600      0.0580       4.6013\n     500      2.605972      2.605972      0.9600      0.9600      0.0625       4.8398\n     550      2.590114      2.590114      0.9600      0.9600      0.0674       5.0836\n     600      2.573072      2.573072      0.9600      0.9600      0.0721       5.3320\n     650      2.554797      2.554797      0.9600      0.9600      0.0770       5.5838\n     700      2.535294      2.535294      0.9644      0.9644      0.0820       5.8388\n     750      2.514557      2.514557      0.9644      0.9644      0.0867       6.0964\n     800      2.492614      2.492614      0.9689      0.9689      0.0918       6.3561\n     850      2.469465      2.469465      0.9644      0.9644      0.0968       6.6175\n     900      2.445097      2.445097      0.9644      0.9644      0.1017       6.8802\n     950      2.419527      2.419527      0.9644      0.9644      0.1065       7.1443\n    1000      2.392782      2.392782      0.9644      0.9644      0.1113       7.4099\n    1050      2.364869      2.364869      0.9644      0.9644      0.1161       7.6768\n    1100      2.335790      2.335790      0.9644      0.9644      0.1212       7.9449\n    1150      2.305551      2.305551      0.9733      0.9733      0.1260       8.2140\n    1200      2.274176      2.274176      0.9689      0.9689      0.1305       8.4840\n    1250      2.241676      2.241676      0.9689      0.9689      0.1354       8.7547\n    1300      2.208121      2.208121      0.9778      0.9778      0.1398       9.0261\n    1350      2.173514      2.173514      0.9733      0.9733      0.1443       9.2980\n    1400      2.137855      2.137855      0.9733      0.9733      0.1489       9.5704\n    1450      2.101184      2.101184      0.9733      0.9733      0.1532       9.8429\n    1500      2.063508      2.063508      0.9733      0.9733      0.1573      10.1158\n    1550      2.024894      2.024894      0.9733      0.9733      0.1616      10.3891\n    1600      1.985370      1.985370      0.9733      0.9733      0.1653      10.6629\n    1650      1.944952      1.944952      0.9733      0.9733      0.1690      10.9369\n    1700      1.903671      1.903671      0.9733      0.9733      0.1732      11.2109\n    1750      1.861597      1.861597      0.9778      0.9778      0.1763      11.4848\n    1800      1.818776      1.818776      0.9778      0.9778      0.1801      11.7587\n    1850      1.775244      1.775244      0.9778      0.9778      0.1826      12.0323\n    1900      1.731057      1.731057      0.9778      0.9778      0.1858      12.3057\n    1950      1.686320      1.686320      0.9822      0.9822      0.1884      12.5791\n    2000      1.641035      1.641035      0.9822      0.9822      0.1909      12.8522\n    2050      1.595242      1.595242      0.9822      0.9822      0.1932      13.1251\n    2100      1.549028      1.549028      0.9911      0.9911      0.1955      13.3978\n    2150      1.502467      1.502467      0.9911      0.9911      0.1972      13.6703\n    2200      1.455593      1.455593      0.9911      0.9911      0.1985      13.9424\n    2250      1.408524      1.408524      0.9911      0.9911      0.1995      14.2141\n    2300      1.361254      1.361254      0.9911      0.9911      0.2006      14.4852\n    2350      1.313875      1.313875      0.9911      0.9911      0.2012      14.7558\n    2400      1.266527      1.266527      0.9911      0.9911      0.2013      15.0258\n    2450      1.219261      1.219261      0.9911      0.9911      0.2009      15.2951\n    2500      1.172157      1.172157      0.9956      0.9956      0.2007      15.5639\n    2550      1.125263      1.125263      1.0000      1.0000      0.1999      15.8320\n    2600      1.078703      1.078703      1.0000      1.0000      0.1988      16.0995\n    2650      1.032564      1.032564      1.0000      1.0000      0.1968      16.3664\n    2700      0.986893      0.986893      1.0000      1.0000      0.1951      16.6324\n    2750      0.941788      0.941788      1.0000      1.0000      0.1929      16.8977\n    2800      0.897309      0.897309      1.0000      1.0000      0.1902      17.1624\n    2850      0.853580      0.853580      1.0000      1.0000      0.1869      17.4266\n    2900      0.810676      0.810676      1.0000      1.0000      0.1837      17.6899\n    2950      0.768628      0.768628      1.0000      1.0000      0.1801      17.9525\n    3000      0.727545      0.727545      1.0000      1.0000      0.1763      18.2142\n    3050      0.687491      0.687491      1.0000      1.0000      0.1721      18.4750\n    3100      0.648535      0.648535      1.0000      1.0000      0.1675      18.7348\n    3150      0.610729      0.610729      1.0000      1.0000      0.1631      18.9937\n    3200      0.574078      0.574078      1.0000      1.0000      0.1581      19.2515\n    3250      0.538643      0.538643      1.0000      1.0000      0.1532      19.5083\n    3300      0.504470      0.504470      1.0000      1.0000      0.1476      19.7643\n    3350      0.471580      0.471580      1.0000      1.0000      0.1423      20.0197\n    3400      0.440040      0.440040      1.0000      1.0000      0.1364      20.2743\n    3450      0.409832      0.409832      1.0000      1.0000      0.1311      20.5284\n    3500      0.380988      0.380988      1.0000      1.0000      0.1252      20.7819\n    3550      0.353516      0.353516      1.0000      1.0000      0.1192      21.0348\n    3600      0.327428      0.327428      1.0000      1.0000      0.1137      21.2873\n    3650      0.302703      0.302703      1.0000      1.0000      0.1077      21.5394\n    3700      0.279346      0.279346      1.0000      1.0000      0.1022      21.7913\n    3750      0.257324      0.257324      1.0000      1.0000      0.0963      22.0428\n    3800      0.236634      0.236634      1.0000      1.0000      0.0909      22.2942\n    3850      0.217248      0.217248      1.0000      1.0000      0.0853      22.5455\n    3900      0.199119      0.199119      1.0000      1.0000      0.0797      22.7966\n    3950      0.182211      0.182211      1.0000      1.0000      0.0746      23.0474\n    4000      0.166479      0.166479      1.0000      1.0000      0.0696      23.2980\n    4050      0.151870      0.151870      1.0000      1.0000      0.0648      23.5484\n    4100      0.138337      0.138337      1.0000      1.0000      0.0600      23.7985\n    4150      0.125835      0.125835      1.0000      1.0000      0.0556      24.0482\n    4200      0.114298      0.114298      1.0000      1.0000      0.0515      24.2977\n    4250      0.103678      0.103678      1.0000      1.0000      0.0473      24.5468\n    4300      0.093922      0.093922      1.0000      1.0000      0.0437      24.7955\n    4350      0.084973      0.084973      1.0000      1.0000      0.0401      25.0438\n    4400      0.076784      0.076784      1.0000      1.0000      0.0368      25.2916\n    4450      0.069301      0.069301      1.0000      1.0000      0.0337      25.5390\n    4500      0.062470      0.062470      1.0000      1.0000      0.0308      25.7860\n    4550      0.056248      0.056248      1.0000      1.0000      0.0281      26.0323\n    4600      0.050594      0.050594      1.0000      1.0000      0.0256      26.2781\n    4650      0.045458      0.045458      1.0000      1.0000      0.0233      26.5234\n    4700      0.040801      0.040801      1.0000      1.0000      0.0212      26.7681\n    4750      0.036583      0.036583      1.0000      1.0000      0.0193      27.0124\n    4800      0.032766      0.032766      1.0000      1.0000      0.0174      27.2563\n    4850      0.029319      0.029319      1.0000      1.0000      0.0158      27.4998\n    4900      0.026207      0.026207      1.0000      1.0000      0.0143      27.7429\n    4950      0.023404      0.023404      1.0000      1.0000      0.0129      27.9856\n    4999      0.020929      0.020929      1.0000      1.0000      0.0117      28.2232\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.020929\n  Test Loss   = 0.020929\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 28.2232\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 2.708508, "test_losses": 2.708508, "train_accs": 0.053333, "test_accs": 0.053333, "grad_norms": 0.034605, "param_norms": 3.21002}, {"epoch": 50, "train_losses": 2.701765, "test_losses": 2.701765, "train_accs": 0.111111, "test_accs": 0.111111, "grad_norms": 0.033961, "param_norms": 3.226197}, {"epoch": 100, "train_losses": 2.694151, "test_losses": 2.694151, "train_accs": 0.302222, "test_accs": 0.302222, "grad_norms": 0.034834, "param_norms": 3.297734}, {"epoch": 150, "train_losses": 2.686129, "test_losses": 2.686129, "train_accs": 0.515556, "test_accs": 0.515556, "grad_norms": 0.036405, "param_norms": 3.413707}, {"epoch": 200, "train_losses": 2.677509, "test_losses": 2.677509, "train_accs": 0.684444, "test_accs": 0.684444, "grad_norms": 0.03883, "param_norms": 3.563211}, {"epoch": 250, "train_losses": 2.668158, "test_losses": 2.668158, "train_accs": 0.826667, "test_accs": 0.826667, "grad_norms": 0.041903, "param_norms": 3.73868}, {"epoch": 300, "train_losses": 2.657879, "test_losses": 2.657879, "train_accs": 0.888889, "test_accs": 0.888889, "grad_norms": 0.04547, "param_norms": 3.934533}, {"epoch": 350, "train_losses": 2.646547, "test_losses": 2.646547, "train_accs": 0.933333, "test_accs": 0.933333, "grad_norms": 0.049568, "param_norms": 4.146044}, {"epoch": 400, "train_losses": 2.63413, "test_losses": 2.63413, "train_accs": 0.951111, "test_accs": 0.951111, "grad_norms": 0.053777, "param_norms": 4.369293}, {"epoch": 450, "train_losses": 2.620615, "test_losses": 2.620615, "train_accs": 0.96, "test_accs": 0.96, "grad_norms": 0.058011, "param_norms": 4.601326}, {"epoch": 500, "train_losses": 2.605972, "test_losses": 2.605972, "train_accs": 0.96, "test_accs": 0.96, "grad_norms": 0.062547, "param_norms": 4.839811}, {"epoch": 550, "train_losses": 2.590114, "test_losses": 2.590114, "train_accs": 0.96, "test_accs": 0.96, "grad_norms": 0.067385, "param_norms": 5.083593}, {"epoch": 600, "train_losses": 2.573072, "test_losses": 2.573072, "train_accs": 0.96, "test_accs": 0.96, "grad_norms": 0.07205, "param_norms": 5.332009}, {"epoch": 650, "train_losses": 2.554797, "test_losses": 2.554797, "train_accs": 0.96, "test_accs": 0.96, "grad_norms": 0.076982, "param_norms": 5.58381}, {"epoch": 700, "train_losses": 2.535294, "test_losses": 2.535294, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.082043, "param_norms": 5.838843}, {"epoch": 750, "train_losses": 2.514557, "test_losses": 2.514557, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.086741, "param_norms": 6.096441}, {"epoch": 800, "train_losses": 2.492614, "test_losses": 2.492614, "train_accs": 0.968889, "test_accs": 0.968889, "grad_norms": 0.091847, "param_norms": 6.356139}, {"epoch": 850, "train_losses": 2.469465, "test_losses": 2.469465, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.096813, "param_norms": 6.617476}, {"epoch": 900, "train_losses": 2.445097, "test_losses": 2.445097, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.101699, "param_norms": 6.880217}, {"epoch": 950, "train_losses": 2.419527, "test_losses": 2.419527, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.106527, "param_norms": 7.14432}, {"epoch": 1000, "train_losses": 2.392782, "test_losses": 2.392782, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.111261, "param_norms": 7.409879}, {"epoch": 1050, "train_losses": 2.364869, "test_losses": 2.364869, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.116149, "param_norms": 7.676804}, {"epoch": 1100, "train_losses": 2.33579, "test_losses": 2.33579, "train_accs": 0.964444, "test_accs": 0.964444, "grad_norms": 0.121172, "param_norms": 7.944856}, {"epoch": 1150, "train_losses": 2.305551, "test_losses": 2.305551, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.125964, "param_norms": 8.213983}, {"epoch": 1200, "train_losses": 2.274176, "test_losses": 2.274176, "train_accs": 0.968889, "test_accs": 0.968889, "grad_norms": 0.130479, "param_norms": 8.48398}, {"epoch": 1250, "train_losses": 2.241676, "test_losses": 2.241676, "train_accs": 0.968889, "test_accs": 0.968889, "grad_norms": 0.135354, "param_norms": 8.754677}, {"epoch": 1300, "train_losses": 2.208121, "test_losses": 2.208121, "train_accs": 0.977778, "test_accs": 0.977778, "grad_norms": 0.139846, "param_norms": 9.026126}, {"epoch": 1350, "train_losses": 2.173514, "test_losses": 2.173514, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.144254, "param_norms": 9.298024}, {"epoch": 1400, "train_losses": 2.137855, "test_losses": 2.137855, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.14887, "param_norms": 9.570354}, {"epoch": 1450, "train_losses": 2.101184, "test_losses": 2.101184, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.153213, "param_norms": 9.842945}, {"epoch": 1500, "train_losses": 2.063508, "test_losses": 2.063508, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.157253, "param_norms": 10.115779}, {"epoch": 1550, "train_losses": 2.024894, "test_losses": 2.024894, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.161642, "param_norms": 10.389142}, {"epoch": 1600, "train_losses": 1.98537, "test_losses": 1.98537, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.165293, "param_norms": 10.662936}, {"epoch": 1650, "train_losses": 1.944952, "test_losses": 1.944952, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.169047, "param_norms": 10.936902}, {"epoch": 1700, "train_losses": 1.903671, "test_losses": 1.903671, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.17318, "param_norms": 11.210854}, {"epoch": 1750, "train_losses": 1.861597, "test_losses": 1.861597, "train_accs": 0.977778, "test_accs": 0.977778, "grad_norms": 0.176308, "param_norms": 11.484768}, {"epoch": 1800, "train_losses": 1.818776, "test_losses": 1.818776, "train_accs": 0.977778, "test_accs": 0.977778, "grad_norms": 0.180122, "param_norms": 11.758679}, {"epoch": 1850, "train_losses": 1.775244, "test_losses": 1.775244, "train_accs": 0.977778, "test_accs": 0.977778, "grad_norms": 0.182567, "param_norms": 12.032274}, {"epoch": 1900, "train_losses": 1.731057, "test_losses": 1.731057, "train_accs": 0.977778, "test_accs": 0.977778, "grad_norms": 0.18576, "param_norms": 12.305725}, {"epoch": 1950, "train_losses": 1.68632, "test_losses": 1.68632, "train_accs": 0.982222, "test_accs": 0.982222, "grad_norms": 0.188381, "param_norms": 12.579105}, {"epoch": 2000, "train_losses": 1.641035, "test_losses": 1.641035, "train_accs": 0.982222, "test_accs": 0.982222, "grad_norms": 0.190895, "param_norms": 12.852182}, {"epoch": 2050, "train_losses": 1.595242, "test_losses": 1.595242, "train_accs": 0.982222, "test_accs": 0.982222, "grad_norms": 0.193218, "param_norms": 13.125079}, {"epoch": 2100, "train_losses": 1.549028, "test_losses": 1.549028, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.195545, "param_norms": 13.397758}, {"epoch": 2150, "train_losses": 1.502467, "test_losses": 1.502467, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.197224, "param_norms": 13.670302}, {"epoch": 2200, "train_losses": 1.455593, "test_losses": 1.455593, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.198529, "param_norms": 13.942435}, {"epoch": 2250, "train_losses": 1.408524, "test_losses": 1.408524, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.199469, "param_norms": 14.214108}, {"epoch": 2300, "train_losses": 1.361254, "test_losses": 1.361254, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.200613, "param_norms": 14.485231}, {"epoch": 2350, "train_losses": 1.313875, "test_losses": 1.313875, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.201191, "param_norms": 14.75579}, {"epoch": 2400, "train_losses": 1.266527, "test_losses": 1.266527, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.201308, "param_norms": 15.025764}, {"epoch": 2450, "train_losses": 1.219261, "test_losses": 1.219261, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.200887, "param_norms": 15.295143}, {"epoch": 2500, "train_losses": 1.172157, "test_losses": 1.172157, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.200689, "param_norms": 15.563874}, {"epoch": 2550, "train_losses": 1.125263, "test_losses": 1.125263, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.199949, "param_norms": 15.831963}, {"epoch": 2600, "train_losses": 1.078703, "test_losses": 1.078703, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.198826, "param_norms": 16.099453}, {"epoch": 2650, "train_losses": 1.032564, "test_losses": 1.032564, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.19685, "param_norms": 16.366352}, {"epoch": 2700, "train_losses": 0.986893, "test_losses": 0.986893, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.19506, "param_norms": 16.63242}, {"epoch": 2750, "train_losses": 0.941788, "test_losses": 0.941788, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.192917, "param_norms": 16.897724}, {"epoch": 2800, "train_losses": 0.897309, "test_losses": 0.897309, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.190206, "param_norms": 17.162419}, {"epoch": 2850, "train_losses": 0.85358, "test_losses": 0.85358, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.186901, "param_norms": 17.42657}, {"epoch": 2900, "train_losses": 0.810676, "test_losses": 0.810676, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.183711, "param_norms": 17.689949}, {"epoch": 2950, "train_losses": 0.768628, "test_losses": 0.768628, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.180127, "param_norms": 17.952521}, {"epoch": 3000, "train_losses": 0.727545, "test_losses": 0.727545, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.176272, "param_norms": 18.214229}, {"epoch": 3050, "train_losses": 0.687491, "test_losses": 0.687491, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.172131, "param_norms": 18.474989}, {"epoch": 3100, "train_losses": 0.648535, "test_losses": 0.648535, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.16752, "param_norms": 18.734798}, {"epoch": 3150, "train_losses": 0.610729, "test_losses": 0.610729, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.163083, "param_norms": 18.993663}, {"epoch": 3200, "train_losses": 0.574078, "test_losses": 0.574078, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.158098, "param_norms": 19.251513}, {"epoch": 3250, "train_losses": 0.538643, "test_losses": 0.538643, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.15323, "param_norms": 19.50826}, {"epoch": 3300, "train_losses": 0.50447, "test_losses": 0.50447, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.147614, "param_norms": 19.764339}, {"epoch": 3350, "train_losses": 0.47158, "test_losses": 0.47158, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.142343, "param_norms": 20.019653}, {"epoch": 3400, "train_losses": 0.44004, "test_losses": 0.44004, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.136362, "param_norms": 20.274311}, {"epoch": 3450, "train_losses": 0.409832, "test_losses": 0.409832, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.131142, "param_norms": 20.528376}, {"epoch": 3500, "train_losses": 0.380988, "test_losses": 0.380988, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.125162, "param_norms": 20.781871}, {"epoch": 3550, "train_losses": 0.353516, "test_losses": 0.353516, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.119225, "param_norms": 21.034803}, {"epoch": 3600, "train_losses": 0.327428, "test_losses": 0.327428, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.113701, "param_norms": 21.287261}, {"epoch": 3650, "train_losses": 0.302703, "test_losses": 0.302703, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.107685, "param_norms": 21.539379}, {"epoch": 3700, "train_losses": 0.279346, "test_losses": 0.279346, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.102227, "param_norms": 21.791264}, {"epoch": 3750, "train_losses": 0.257324, "test_losses": 0.257324, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.096293, "param_norms": 22.042816}, {"epoch": 3800, "train_losses": 0.236634, "test_losses": 0.236634, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.090874, "param_norms": 22.294247}, {"epoch": 3850, "train_losses": 0.217248, "test_losses": 0.217248, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.08528, "param_norms": 22.545505}, {"epoch": 3900, "train_losses": 0.199119, "test_losses": 0.199119, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.079692, "param_norms": 22.796558}, {"epoch": 3950, "train_losses": 0.182211, "test_losses": 0.182211, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.074589, "param_norms": 23.047434}, {"epoch": 4000, "train_losses": 0.166479, "test_losses": 0.166479, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.069567, "param_norms": 23.298045}, {"epoch": 4050, "train_losses": 0.15187, "test_losses": 0.15187, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.064795, "param_norms": 23.548422}, {"epoch": 4100, "train_losses": 0.138337, "test_losses": 0.138337, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.059966, "param_norms": 23.798469}, {"epoch": 4150, "train_losses": 0.125835, "test_losses": 0.125835, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.05561, "param_norms": 24.048244}, {"epoch": 4200, "train_losses": 0.114298, "test_losses": 0.114298, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.051506, "param_norms": 24.297723}, {"epoch": 4250, "train_losses": 0.103678, "test_losses": 0.103678, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.047324, "param_norms": 24.546842}, {"epoch": 4300, "train_losses": 0.093922, "test_losses": 0.093922, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.043651, "param_norms": 24.795542}, {"epoch": 4350, "train_losses": 0.084973, "test_losses": 0.084973, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.040078, "param_norms": 25.043796}, {"epoch": 4400, "train_losses": 0.076784, "test_losses": 0.076784, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.036819, "param_norms": 25.291605}, {"epoch": 4450, "train_losses": 0.069301, "test_losses": 0.069301, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.033698, "param_norms": 25.539044}, {"epoch": 4500, "train_losses": 0.06247, "test_losses": 0.06247, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.030821, "param_norms": 25.785961}, {"epoch": 4550, "train_losses": 0.056248, "test_losses": 0.056248, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.02812, "param_norms": 26.032293}, {"epoch": 4600, "train_losses": 0.050594, "test_losses": 0.050594, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.02561, "param_norms": 26.278098}, {"epoch": 4650, "train_losses": 0.045458, "test_losses": 0.045458, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.023331, "param_norms": 26.523357}, {"epoch": 4700, "train_losses": 0.040801, "test_losses": 0.040801, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.021245, "param_norms": 26.768145}, {"epoch": 4750, "train_losses": 0.036583, "test_losses": 0.036583, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.019287, "param_norms": 27.012442}, {"epoch": 4800, "train_losses": 0.032766, "test_losses": 0.032766, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.017433, "param_norms": 27.256299}, {"epoch": 4850, "train_losses": 0.029319, "test_losses": 0.029319, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.015782, "param_norms": 27.499785}, {"epoch": 4900, "train_losses": 0.026207, "test_losses": 0.026207, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014273, "param_norms": 27.742866}, {"epoch": 4950, "train_losses": 0.023404, "test_losses": 0.023404, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012862, "param_norms": 27.985643}, {"epoch": 4999, "train_losses": 0.020929, "test_losses": 0.020929, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011655, "param_norms": 28.223215}], "total_epochs": 5000}, "quad_random": {"config": {"prime": 15, "d_mlp": 512, "act_type": "Quad", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 5e-05, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=15, run=quad_random\n======================================================================\n\nConfiguration:\n  prime (p)       = 15\n  d_mlp           = 512\n  activation      = Quad\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 5e-05\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      2.708178      2.708178      0.0311      0.0311      0.0048       3.2100\n      50      2.707150      2.707150      0.1200      0.1200      0.0049       3.2344\n     100      2.705877      2.705877      0.3689      0.3689      0.0054       3.3305\n     150      2.704360      2.704360      0.6044      0.6044      0.0063       3.4821\n     200      2.702511      2.702511      0.7600      0.7600      0.0075       3.6723\n     250      2.700259      2.700259      0.8533      0.8533      0.0089       3.8889\n     300      2.697540      2.697540      0.9200      0.9200      0.0106       4.1240\n     350      2.694298      2.694298      0.9467      0.9467      0.0126       4.3720\n     400      2.690480      2.690480      0.9867      0.9867      0.0147       4.6295\n     450      2.686031      2.686031      0.9956      0.9956      0.0171       4.8939\n     500      2.680901      2.680901      0.9956      0.9956      0.0196       5.1635\n     550      2.675037      2.675037      0.9956      0.9956      0.0224       5.4371\n     600      2.668388      2.668388      0.9956      0.9956      0.0254       5.7138\n     650      2.660902      2.660902      0.9956      0.9956      0.0285       5.9932\n     700      2.652529      2.652529      0.9956      0.9956      0.0319       6.2749\n     750      2.643218      2.643218      1.0000      1.0000      0.0354       6.5582\n     800      2.632921      2.632921      1.0000      1.0000      0.0392       6.8431\n     850      2.621588      2.621588      1.0000      1.0000      0.0431       7.1291\n     900      2.609170      2.609170      1.0000      1.0000      0.0472       7.4162\n     950      2.595621      2.595621      1.0000      1.0000      0.0515       7.7043\n    1000      2.580893      2.580893      1.0000      1.0000      0.0560       7.9936\n    1050      2.564942      2.564942      1.0000      1.0000      0.0607       8.2840\n    1100      2.547724      2.547724      1.0000      1.0000      0.0655       8.5755\n    1150      2.529197      2.529197      1.0000      1.0000      0.0705       8.8678\n    1200      2.509321      2.509321      1.0000      1.0000      0.0757       9.1608\n    1250      2.488054      2.488054      1.0000      1.0000      0.0810       9.4543\n    1300      2.465358      2.465358      1.0000      1.0000      0.0864       9.7483\n    1350      2.441196      2.441196      1.0000      1.0000      0.0921      10.0428\n    1400      2.415534      2.415534      1.0000      1.0000      0.0978      10.3380\n    1450      2.388340      2.388340      1.0000      1.0000      0.1037      10.6337\n    1500      2.359584      2.359584      1.0000      1.0000      0.1097      10.9300\n    1550      2.329242      2.329242      1.0000      1.0000      0.1158      11.2267\n    1600      2.297287      2.297287      1.0000      1.0000      0.1220      11.5237\n    1650      2.263699      2.263699      1.0000      1.0000      0.1283      11.8211\n    1700      2.228461      2.228461      1.0000      1.0000      0.1347      12.1185\n    1750      2.191558      2.191558      1.0000      1.0000      0.1412      12.4161\n    1800      2.152983      2.152983      1.0000      1.0000      0.1477      12.7139\n    1850      2.112732      2.112732      1.0000      1.0000      0.1542      13.0118\n    1900      2.070807      2.070807      1.0000      1.0000      0.1607      13.3097\n    1950      2.027215      2.027215      1.0000      1.0000      0.1673      13.6077\n    2000      1.981971      1.981971      1.0000      1.0000      0.1738      13.9055\n    2050      1.935097      1.935097      1.0000      1.0000      0.1802      14.2032\n    2100      1.886621      1.886621      1.0000      1.0000      0.1865      14.5005\n    2150      1.836583      1.836583      1.0000      1.0000      0.1928      14.7975\n    2200      1.785030      1.785030      1.0000      1.0000      0.1988      15.0941\n    2250      1.732021      1.732021      1.0000      1.0000      0.2047      15.3903\n    2300      1.677627      1.677627      1.0000      1.0000      0.2103      15.6861\n    2350      1.621930      1.621930      1.0000      1.0000      0.2157      15.9813\n    2400      1.565025      1.565025      1.0000      1.0000      0.2207      16.2758\n    2450      1.507023      1.507023      1.0000      1.0000      0.2253      16.5697\n    2500      1.448047      1.448047      1.0000      1.0000      0.2296      16.8628\n    2550      1.388237      1.388237      1.0000      1.0000      0.2333      17.1551\n    2600      1.327748      1.327748      1.0000      1.0000      0.2364      17.4466\n    2650      1.266753      1.266753      1.0000      1.0000      0.2390      17.7372\n    2700      1.205436      1.205436      1.0000      1.0000      0.2409      18.0268\n    2750      1.143998      1.143998      1.0000      1.0000      0.2421      18.3151\n    2800      1.082651      1.082651      1.0000      1.0000      0.2425      18.6021\n    2850      1.021620      1.021620      1.0000      1.0000      0.2421      18.8877\n    2900      0.961135      0.961135      1.0000      1.0000      0.2408      19.1719\n    2950      0.901435      0.901435      1.0000      1.0000      0.2387      19.4546\n    3000      0.842757      0.842757      1.0000      1.0000      0.2357      19.7355\n    3050      0.785334      0.785334      1.0000      1.0000      0.2319      20.0145\n    3100      0.729387      0.729387      1.0000      1.0000      0.2271      20.2910\n    3150      0.675124      0.675124      1.0000      1.0000      0.2216      20.5650\n    3200      0.622735      0.622735      1.0000      1.0000      0.2152      20.8361\n    3250      0.572388      0.572388      1.0000      1.0000      0.2081      21.1045\n    3300      0.524228      0.524228      1.0000      1.0000      0.2003      21.3699\n    3350      0.478375      0.478375      1.0000      1.0000      0.1920      21.6324\n    3400      0.434923      0.434923      1.0000      1.0000      0.1831      21.8921\n    3450      0.393942      0.393942      1.0000      1.0000      0.1738      22.1489\n    3500      0.355477      0.355477      1.0000      1.0000      0.1641      22.4033\n    3550      0.319550      0.319550      1.0000      1.0000      0.1542      22.6555\n    3600      0.286163      0.286163      1.0000      1.0000      0.1441      22.9060\n    3650      0.255295      0.255295      1.0000      1.0000      0.1340      23.1551\n    3700      0.226904      0.226904      1.0000      1.0000      0.1239      23.4033\n    3750      0.200925      0.200925      1.0000      1.0000      0.1140      23.6508\n    3800      0.177277      0.177277      1.0000      1.0000      0.1042      23.8980\n    3850      0.155858      0.155858      1.0000      1.0000      0.0949      24.1451\n    3900      0.136555      0.136555      1.0000      1.0000      0.0859      24.3921\n    3950      0.119240      0.119240      1.0000      1.0000      0.0774      24.6391\n    4000      0.103780      0.103780      1.0000      1.0000      0.0694      24.8859\n    4050      0.090036      0.090036      1.0000      1.0000      0.0620      25.1325\n    4100      0.077871      0.077871      1.0000      1.0000      0.0551      25.3789\n    4150      0.067146      0.067146      1.0000      1.0000      0.0488      25.6250\n    4200      0.057727      0.057727      1.0000      1.0000      0.0430      25.8707\n    4250      0.049488      0.049488      1.0000      1.0000      0.0377      26.1160\n    4300      0.042306      0.042306      1.0000      1.0000      0.0330      26.3609\n    4350      0.036067      0.036067      1.0000      1.0000      0.0288      26.6053\n    4400      0.030666      0.030666      1.0000      1.0000      0.0250      26.8492\n    4450      0.026006      0.026006      1.0000      1.0000      0.0217      27.0925\n    4500      0.021996      0.021996      1.0000      1.0000      0.0187      27.3354\n    4550      0.018558      0.018558      1.0000      1.0000      0.0161      27.5776\n    4600      0.015618      0.015618      1.0000      1.0000      0.0138      27.8193\n    4650      0.013112      0.013112      1.0000      1.0000      0.0118      28.0604\n    4700      0.010981      0.010981      1.0000      1.0000      0.0101      28.3010\n    4750      0.009175      0.009175      1.0000      1.0000      0.0086      28.5409\n    4800      0.007647      0.007647      1.0000      1.0000      0.0073      28.7801\n    4850      0.006359      0.006359      1.0000      1.0000      0.0061      29.0186\n    4900      0.005276      0.005276      1.0000      1.0000      0.0052      29.2566\n    4950      0.004367      0.004367      1.0000      1.0000      0.0044      29.4939\n    4999      0.003620      0.003620      1.0000      1.0000      0.0037      29.7259\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.003620\n  Test Loss   = 0.003620\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 29.7259\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 2.708178, "test_losses": 2.708178, "train_accs": 0.031111, "test_accs": 0.031111, "grad_norms": 0.004795, "param_norms": 3.21002}, {"epoch": 50, "train_losses": 2.70715, "test_losses": 2.70715, "train_accs": 0.12, "test_accs": 0.12, "grad_norms": 0.004915, "param_norms": 3.23436}, {"epoch": 100, "train_losses": 2.705877, "test_losses": 2.705877, "train_accs": 0.368889, "test_accs": 0.368889, "grad_norms": 0.005434, "param_norms": 3.330455}, {"epoch": 150, "train_losses": 2.70436, "test_losses": 2.70436, "train_accs": 0.604444, "test_accs": 0.604444, "grad_norms": 0.006306, "param_norms": 3.482077}, {"epoch": 200, "train_losses": 2.702511, "test_losses": 2.702511, "train_accs": 0.76, "test_accs": 0.76, "grad_norms": 0.007481, "param_norms": 3.672288}, {"epoch": 250, "train_losses": 2.700259, "test_losses": 2.700259, "train_accs": 0.853333, "test_accs": 0.853333, "grad_norms": 0.008928, "param_norms": 3.888915}, {"epoch": 300, "train_losses": 2.69754, "test_losses": 2.69754, "train_accs": 0.92, "test_accs": 0.92, "grad_norms": 0.010624, "param_norms": 4.123989}, {"epoch": 350, "train_losses": 2.694298, "test_losses": 2.694298, "train_accs": 0.946667, "test_accs": 0.946667, "grad_norms": 0.012551, "param_norms": 4.372023}, {"epoch": 400, "train_losses": 2.69048, "test_losses": 2.69048, "train_accs": 0.986667, "test_accs": 0.986667, "grad_norms": 0.014697, "param_norms": 4.629505}, {"epoch": 450, "train_losses": 2.686031, "test_losses": 2.686031, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.017057, "param_norms": 4.893934}, {"epoch": 500, "train_losses": 2.680901, "test_losses": 2.680901, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.019623, "param_norms": 5.163547}, {"epoch": 550, "train_losses": 2.675037, "test_losses": 2.675037, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.02239, "param_norms": 5.437083}, {"epoch": 600, "train_losses": 2.668388, "test_losses": 2.668388, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.025358, "param_norms": 5.713787}, {"epoch": 650, "train_losses": 2.660902, "test_losses": 2.660902, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.028524, "param_norms": 5.993239}, {"epoch": 700, "train_losses": 2.652529, "test_losses": 2.652529, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.031885, "param_norms": 6.274878}, {"epoch": 750, "train_losses": 2.643218, "test_losses": 2.643218, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.035439, "param_norms": 6.55825}, {"epoch": 800, "train_losses": 2.632921, "test_losses": 2.632921, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.039184, "param_norms": 6.843055}, {"epoch": 850, "train_losses": 2.621588, "test_losses": 2.621588, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.043118, "param_norms": 7.129062}, {"epoch": 900, "train_losses": 2.60917, "test_losses": 2.60917, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.047238, "param_norms": 7.416171}, {"epoch": 950, "train_losses": 2.595621, "test_losses": 2.595621, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.051539, "param_norms": 7.704348}, {"epoch": 1000, "train_losses": 2.580893, "test_losses": 2.580893, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.056021, "param_norms": 7.99362}, {"epoch": 1050, "train_losses": 2.564942, "test_losses": 2.564942, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.060677, "param_norms": 8.284031}, {"epoch": 1100, "train_losses": 2.547724, "test_losses": 2.547724, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.065505, "param_norms": 8.57547}, {"epoch": 1150, "train_losses": 2.529197, "test_losses": 2.529197, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0705, "param_norms": 8.867778}, {"epoch": 1200, "train_losses": 2.509321, "test_losses": 2.509321, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.075658, "param_norms": 9.16081}, {"epoch": 1250, "train_losses": 2.488054, "test_losses": 2.488054, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.080973, "param_norms": 9.454339}, {"epoch": 1300, "train_losses": 2.465358, "test_losses": 2.465358, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.086439, "param_norms": 9.748324}, {"epoch": 1350, "train_losses": 2.441196, "test_losses": 2.441196, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.092052, "param_norms": 10.04283}, {"epoch": 1400, "train_losses": 2.415534, "test_losses": 2.415534, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.097803, "param_norms": 10.337965}, {"epoch": 1450, "train_losses": 2.38834, "test_losses": 2.38834, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.103685, "param_norms": 10.633719}, {"epoch": 1500, "train_losses": 2.359584, "test_losses": 2.359584, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.109688, "param_norms": 10.930015}, {"epoch": 1550, "train_losses": 2.329242, "test_losses": 2.329242, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.115804, "param_norms": 11.22671}, {"epoch": 1600, "train_losses": 2.297287, "test_losses": 2.297287, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.122023, "param_norms": 11.523747}, {"epoch": 1650, "train_losses": 2.263699, "test_losses": 2.263699, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.128332, "param_norms": 11.821057}, {"epoch": 1700, "train_losses": 2.228461, "test_losses": 2.228461, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.134719, "param_norms": 12.118534}, {"epoch": 1750, "train_losses": 2.191558, "test_losses": 2.191558, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.14117, "param_norms": 12.41614}, {"epoch": 1800, "train_losses": 2.152983, "test_losses": 2.152983, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.147669, "param_norms": 12.713887}, {"epoch": 1850, "train_losses": 2.112732, "test_losses": 2.112732, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.154198, "param_norms": 13.011777}, {"epoch": 1900, "train_losses": 2.070807, "test_losses": 2.070807, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.160738, "param_norms": 13.309738}, {"epoch": 1950, "train_losses": 2.027215, "test_losses": 2.027215, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.167268, "param_norms": 13.607689}, {"epoch": 2000, "train_losses": 1.981971, "test_losses": 1.981971, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.173763, "param_norms": 13.905512}, {"epoch": 2050, "train_losses": 1.935097, "test_losses": 1.935097, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.180197, "param_norms": 14.203162}, {"epoch": 2100, "train_losses": 1.886621, "test_losses": 1.886621, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.186541, "param_norms": 14.500509}, {"epoch": 2150, "train_losses": 1.836583, "test_losses": 1.836583, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.192764, "param_norms": 14.797478}, {"epoch": 2200, "train_losses": 1.78503, "test_losses": 1.78503, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.19883, "param_norms": 15.09409}, {"epoch": 2250, "train_losses": 1.732021, "test_losses": 1.732021, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.2047, "param_norms": 15.390315}, {"epoch": 2300, "train_losses": 1.677627, "test_losses": 1.677627, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.210334, "param_norms": 15.686098}, {"epoch": 2350, "train_losses": 1.62193, "test_losses": 1.62193, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.215685, "param_norms": 15.981305}, {"epoch": 2400, "train_losses": 1.565025, "test_losses": 1.565025, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.220706, "param_norms": 16.275824}, {"epoch": 2450, "train_losses": 1.507023, "test_losses": 1.507023, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.225347, "param_norms": 16.569656}, {"epoch": 2500, "train_losses": 1.448047, "test_losses": 1.448047, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.229553, "param_norms": 16.86276}, {"epoch": 2550, "train_losses": 1.388237, "test_losses": 1.388237, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.233267, "param_norms": 17.155053}, {"epoch": 2600, "train_losses": 1.327748, "test_losses": 1.327748, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.236433, "param_norms": 17.446576}, {"epoch": 2650, "train_losses": 1.266753, "test_losses": 1.266753, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.238993, "param_norms": 17.737243}, {"epoch": 2700, "train_losses": 1.205436, "test_losses": 1.205436, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.24089, "param_norms": 18.026775}, {"epoch": 2750, "train_losses": 1.143998, "test_losses": 1.143998, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.242071, "param_norms": 18.315074}, {"epoch": 2800, "train_losses": 1.082651, "test_losses": 1.082651, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.242486, "param_norms": 18.602116}, {"epoch": 2850, "train_losses": 1.02162, "test_losses": 1.02162, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.24209, "param_norms": 18.887743}, {"epoch": 2900, "train_losses": 0.961135, "test_losses": 0.961135, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.240848, "param_norms": 19.171906}, {"epoch": 2950, "train_losses": 0.901435, "test_losses": 0.901435, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.238737, "param_norms": 19.454568}, {"epoch": 3000, "train_losses": 0.842757, "test_losses": 0.842757, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.235746, "param_norms": 19.735529}, {"epoch": 3050, "train_losses": 0.785334, "test_losses": 0.785334, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.231874, "param_norms": 20.014457}, {"epoch": 3100, "train_losses": 0.729387, "test_losses": 0.729387, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.227138, "param_norms": 20.291004}, {"epoch": 3150, "train_losses": 0.675124, "test_losses": 0.675124, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.221565, "param_norms": 20.564953}, {"epoch": 3200, "train_losses": 0.622735, "test_losses": 0.622735, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.215201, "param_norms": 20.83614}, {"epoch": 3250, "train_losses": 0.572388, "test_losses": 0.572388, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.208102, "param_norms": 21.104493}, {"epoch": 3300, "train_losses": 0.524228, "test_losses": 0.524228, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.200337, "param_norms": 21.369934}, {"epoch": 3350, "train_losses": 0.478375, "test_losses": 0.478375, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.191977, "param_norms": 21.632446}, {"epoch": 3400, "train_losses": 0.434923, "test_losses": 0.434923, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.183102, "param_norms": 21.892057}, {"epoch": 3450, "train_losses": 0.393942, "test_losses": 0.393942, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.173794, "param_norms": 22.148937}, {"epoch": 3500, "train_losses": 0.355477, "test_losses": 0.355477, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.164138, "param_norms": 22.403287}, {"epoch": 3550, "train_losses": 0.31955, "test_losses": 0.31955, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.154225, "param_norms": 22.655502}, {"epoch": 3600, "train_losses": 0.286163, "test_losses": 0.286163, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.144149, "param_norms": 22.906003}, {"epoch": 3650, "train_losses": 0.255295, "test_losses": 0.255295, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.134009, "param_norms": 23.155135}, {"epoch": 3700, "train_losses": 0.226904, "test_losses": 0.226904, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.123908, "param_norms": 23.403272}, {"epoch": 3750, "train_losses": 0.200925, "test_losses": 0.200925, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.113951, "param_norms": 23.650789}, {"epoch": 3800, "train_losses": 0.177277, "test_losses": 0.177277, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.104238, "param_norms": 23.898007}, {"epoch": 3850, "train_losses": 0.155858, "test_losses": 0.155858, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.094859, "param_norms": 24.145105}, {"epoch": 3900, "train_losses": 0.136555, "test_losses": 0.136555, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.085891, "param_norms": 24.392142}, {"epoch": 3950, "train_losses": 0.11924, "test_losses": 0.11924, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.077392, "param_norms": 24.639084}, {"epoch": 4000, "train_losses": 0.10378, "test_losses": 0.10378, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.069407, "param_norms": 24.885889}, {"epoch": 4050, "train_losses": 0.090036, "test_losses": 0.090036, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.061965, "param_norms": 25.132527}, {"epoch": 4100, "train_losses": 0.077871, "test_losses": 0.077871, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.05508, "param_norms": 25.378917}, {"epoch": 4150, "train_losses": 0.067146, "test_losses": 0.067146, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.048756, "param_norms": 25.624985}, {"epoch": 4200, "train_losses": 0.057727, "test_losses": 0.057727, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.042983, "param_norms": 25.870686}, {"epoch": 4250, "train_losses": 0.049488, "test_losses": 0.049488, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.037748, "param_norms": 26.115975}, {"epoch": 4300, "train_losses": 0.042306, "test_losses": 0.042306, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.033026, "param_norms": 26.360854}, {"epoch": 4350, "train_losses": 0.036067, "test_losses": 0.036067, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.028792, "param_norms": 26.605266}, {"epoch": 4400, "train_losses": 0.030666, "test_losses": 0.030666, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.025013, "param_norms": 26.849162}, {"epoch": 4450, "train_losses": 0.026006, "test_losses": 0.026006, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.021658, "param_norms": 27.092535}, {"epoch": 4500, "train_losses": 0.021996, "test_losses": 0.021996, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.018692, "param_norms": 27.335357}, {"epoch": 4550, "train_losses": 0.018558, "test_losses": 0.018558, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016081, "param_norms": 27.577617}, {"epoch": 4600, "train_losses": 0.015618, "test_losses": 0.015618, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013793, "param_norms": 27.819312}, {"epoch": 4650, "train_losses": 0.013112, "test_losses": 0.013112, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011796, "param_norms": 28.060449}, {"epoch": 4700, "train_losses": 0.010981, "test_losses": 0.010981, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010059, "param_norms": 28.300996}, {"epoch": 4750, "train_losses": 0.009175, "test_losses": 0.009175, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008553, "param_norms": 28.540879}, {"epoch": 4800, "train_losses": 0.007647, "test_losses": 0.007647, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007253, "param_norms": 28.78009}, {"epoch": 4850, "train_losses": 0.006359, "test_losses": 0.006359, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006134, "param_norms": 29.018647}, {"epoch": 4900, "train_losses": 0.005276, "test_losses": 0.005276, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005173, "param_norms": 29.256596}, {"epoch": 4950, "train_losses": 0.004367, "test_losses": 0.004367, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004352, "param_norms": 29.493906}, {"epoch": 4999, "train_losses": 0.00362, "test_losses": 0.00362, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003665, "param_norms": 29.725864}], "total_epochs": 5000}, "quad_single_freq": {"config": {"prime": 15, "d_mlp": 512, "act_type": "Quad", "init_type": "single-freq", "init_scale": 0.02, "optimizer": "SGD", "lr": 0.1, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=15, run=quad_single_freq\n======================================================================\n\nConfiguration:\n  prime (p)       = 15\n  d_mlp           = 512\n  activation      = Quad\n  init_type       = single-freq\n  init_scale      = 0.02\n  optimizer       = SGD\n  learning_rate   = 0.1\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      2.708081      2.708081      0.0267      0.0267      0.0037       1.7527\n      50      2.708020      2.708020      0.0933      0.0933      0.0037       1.7527\n     100      2.707951      2.707951      0.1733      0.1733      0.0037       1.7533\n     150      2.707883      2.707883      0.2800      0.2800      0.0037       1.7544\n     200      2.707815      2.707815      0.4267      0.4267      0.0037       1.7561\n     250      2.707745      2.707745      0.5867      0.5867      0.0037       1.7584\n     300      2.707675      2.707675      0.7333      0.7333      0.0037       1.7613\n     350      2.707605      2.707605      0.8444      0.8444      0.0038       1.7648\n     400      2.707533      2.707533      0.9467      0.9467      0.0038       1.7689\n     450      2.707460      2.707460      0.9733      0.9733      0.0038       1.7735\n     500      2.707387      2.707387      0.9911      0.9911      0.0039       1.7788\n     550      2.707312      2.707312      1.0000      1.0000      0.0039       1.7847\n     600      2.707236      2.707236      1.0000      1.0000      0.0039       1.7912\n     650      2.707158      2.707158      1.0000      1.0000      0.0040       1.7984\n     700      2.707078      2.707078      1.0000      1.0000      0.0040       1.8061\n     750      2.706996      2.706996      1.0000      1.0000      0.0041       1.8145\n     800      2.706913      2.706913      1.0000      1.0000      0.0041       1.8235\n     850      2.706827      2.706827      1.0000      1.0000      0.0042       1.8332\n     900      2.706738      2.706738      1.0000      1.0000      0.0042       1.8435\n     950      2.706646      2.706646      1.0000      1.0000      0.0043       1.8545\n    1000      2.706552      2.706552      1.0000      1.0000      0.0044       1.8662\n    1050      2.706454      2.706454      1.0000      1.0000      0.0045       1.8786\n    1100      2.706353      2.706353      1.0000      1.0000      0.0045       1.8917\n    1150      2.706248      2.706248      1.0000      1.0000      0.0046       1.9055\n    1200      2.706138      2.706138      1.0000      1.0000      0.0047       1.9201\n    1250      2.706025      2.706025      1.0000      1.0000      0.0048       1.9354\n    1300      2.705906      2.705906      1.0000      1.0000      0.0049       1.9515\n    1350      2.705782      2.705782      1.0000      1.0000      0.0050       1.9683\n    1400      2.705653      2.705653      1.0000      1.0000      0.0051       1.9860\n    1450      2.705518      2.705518      1.0000      1.0000      0.0053       2.0045\n    1500      2.705376      2.705376      1.0000      1.0000      0.0054       2.0239\n    1550      2.705227      2.705227      1.0000      1.0000      0.0055       2.0442\n    1600      2.705070      2.705070      1.0000      1.0000      0.0057       2.0653\n    1650      2.704905      2.704905      1.0000      1.0000      0.0058       2.0874\n    1700      2.704731      2.704731      1.0000      1.0000      0.0060       2.1105\n    1750      2.704548      2.704548      1.0000      1.0000      0.0061       2.1346\n    1800      2.704353      2.704353      1.0000      1.0000      0.0063       2.1597\n    1850      2.704148      2.704148      1.0000      1.0000      0.0065       2.1859\n    1900      2.703930      2.703930      1.0000      1.0000      0.0067       2.2133\n    1950      2.703698      2.703698      1.0000      1.0000      0.0069       2.2418\n    2000      2.703452      2.703452      1.0000      1.0000      0.0071       2.2715\n    2050      2.703190      2.703190      1.0000      1.0000      0.0074       2.3025\n    2100      2.702910      2.702910      1.0000      1.0000      0.0076       2.3348\n    2150      2.702611      2.702611      1.0000      1.0000      0.0079       2.3685\n    2200      2.702291      2.702291      1.0000      1.0000      0.0081       2.4037\n    2250      2.701948      2.701948      1.0000      1.0000      0.0084       2.4404\n    2300      2.701579      2.701579      1.0000      1.0000      0.0087       2.4787\n    2350      2.701184      2.701184      1.0000      1.0000      0.0091       2.5186\n    2400      2.700757      2.700757      1.0000      1.0000      0.0094       2.5604\n    2450      2.700296      2.700296      1.0000      1.0000      0.0098       2.6041\n    2500      2.699798      2.699798      1.0000      1.0000      0.0102       2.6497\n    2550      2.699258      2.699258      1.0000      1.0000      0.0106       2.6975\n    2600      2.698672      2.698672      1.0000      1.0000      0.0111       2.7474\n    2650      2.698034      2.698034      1.0000      1.0000      0.0115       2.7998\n    2700      2.697340      2.697340      1.0000      1.0000      0.0121       2.8547\n    2750      2.696580      2.696580      1.0000      1.0000      0.0126       2.9123\n    2800      2.695748      2.695748      1.0000      1.0000      0.0132       2.9728\n    2850      2.694835      2.694835      1.0000      1.0000      0.0138       3.0364\n    2900      2.693829      2.693829      1.0000      1.0000      0.0145       3.1033\n    2950      2.692719      2.692719      1.0000      1.0000      0.0153       3.1737\n    3000      2.691490      2.691490      1.0000      1.0000      0.0161       3.2481\n    3050      2.690126      2.690126      1.0000      1.0000      0.0170       3.3266\n    3100      2.688606      2.688606      1.0000      1.0000      0.0179       3.4096\n    3150      2.686908      2.686908      1.0000      1.0000      0.0190       3.4975\n    3200      2.685004      2.685004      1.0000      1.0000      0.0201       3.5907\n    3250      2.682859      2.682859      1.0000      1.0000      0.0213       3.6898\n    3300      2.680436      2.680436      1.0000      1.0000      0.0227       3.7953\n    3350      2.677685      2.677685      1.0000      1.0000      0.0242       3.9078\n    3400      2.674549      2.674549      1.0000      1.0000      0.0259       4.0281\n    3450      2.670956      2.670956      1.0000      1.0000      0.0278       4.1569\n    3500      2.666817      2.666817      1.0000      1.0000      0.0298       4.2953\n    3550      2.662022      2.662022      1.0000      1.0000      0.0322       4.4443\n    3600      2.656435      2.656435      1.0000      1.0000      0.0348       4.6053\n    3650      2.649881      2.649881      1.0000      1.0000      0.0377       4.7797\n    3700      2.642137      2.642137      1.0000      1.0000      0.0411       4.9694\n    3750      2.632916      2.632916      1.0000      1.0000      0.0449       5.1764\n    3800      2.621845      2.621845      1.0000      1.0000      0.0493       5.4032\n    3850      2.608428      2.608428      1.0000      1.0000      0.0544       5.6529\n    3900      2.592003      2.592003      1.0000      1.0000      0.0604       5.9292\n    3950      2.571673      2.571673      1.0000      1.0000      0.0674       6.2365\n    4000      2.546203      2.546203      1.0000      1.0000      0.0756       6.5804\n    4050      2.513869      2.513869      1.0000      1.0000      0.0855       6.9677\n    4100      2.472230      2.472230      1.0000      1.0000      0.0974       7.4070\n    4150      2.417788      2.417788      1.0000      1.0000      0.1118       7.9091\n    4200      2.345479      2.345479      1.0000      1.0000      0.1293       8.4872\n    4250      2.247998      2.247998      1.0000      1.0000      0.1506       9.1579\n    4300      2.115065      2.115065      1.0000      1.0000      0.1762       9.9403\n    4350      1.933388      1.933388      1.0000      1.0000      0.2055      10.8536\n    4400      1.689702      1.689702      1.0000      1.0000      0.2357      11.9094\n    4450      1.381379      1.381379      1.0000      1.0000      0.2590      13.0932\n    4500      1.033969      1.033969      1.0000      1.0000      0.2643      14.3430\n    4550      0.706031      0.706031      1.0000      1.0000      0.2431      15.5475\n    4600      0.455987      0.455987      1.0000      1.0000      0.2011      16.5894\n    4650      0.295980      0.295980      1.0000      1.0000      0.1562      17.4160\n    4700      0.200983      0.200983      1.0000      1.0000      0.1201      18.0494\n    4750      0.144133      0.144133      1.0000      1.0000      0.0939      18.5376\n    4800      0.108569      0.108569      1.0000      1.0000      0.0753      18.9230\n    4850      0.085142      0.085142      1.0000      1.0000      0.0619      19.2353\n    4900      0.068956      0.068956      1.0000      1.0000      0.0521      19.4946\n    4950      0.057302      0.057302      1.0000      1.0000      0.0446      19.7144\n    4999      0.048769      0.048769      1.0000      1.0000      0.0389      19.9006\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.048769\n  Test Loss   = 0.048769\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 19.9006\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 2.708081, "test_losses": 2.708081, "train_accs": 0.026667, "test_accs": 0.026667, "grad_norms": 0.003695, "param_norms": 1.752712}, {"epoch": 50, "train_losses": 2.70802, "test_losses": 2.70802, "train_accs": 0.093333, "test_accs": 0.093333, "grad_norms": 0.003695, "param_norms": 1.752706}, {"epoch": 100, "train_losses": 2.707951, "test_losses": 2.707951, "train_accs": 0.173333, "test_accs": 0.173333, "grad_norms": 0.003699, "param_norms": 1.753253}, {"epoch": 150, "train_losses": 2.707883, "test_losses": 2.707883, "train_accs": 0.28, "test_accs": 0.28, "grad_norms": 0.003706, "param_norms": 1.754384}, {"epoch": 200, "train_losses": 2.707815, "test_losses": 2.707815, "train_accs": 0.426667, "test_accs": 0.426667, "grad_norms": 0.003716, "param_norms": 1.756101}, {"epoch": 250, "train_losses": 2.707745, "test_losses": 2.707745, "train_accs": 0.586667, "test_accs": 0.586667, "grad_norms": 0.00373, "param_norms": 1.758406}, {"epoch": 300, "train_losses": 2.707675, "test_losses": 2.707675, "train_accs": 0.733333, "test_accs": 0.733333, "grad_norms": 0.003748, "param_norms": 1.7613}, {"epoch": 350, "train_losses": 2.707605, "test_losses": 2.707605, "train_accs": 0.844444, "test_accs": 0.844444, "grad_norms": 0.003768, "param_norms": 1.764786}, {"epoch": 400, "train_losses": 2.707533, "test_losses": 2.707533, "train_accs": 0.946667, "test_accs": 0.946667, "grad_norms": 0.003793, "param_norms": 1.768868}, {"epoch": 450, "train_losses": 2.70746, "test_losses": 2.70746, "train_accs": 0.973333, "test_accs": 0.973333, "grad_norms": 0.003821, "param_norms": 1.773548}, {"epoch": 500, "train_losses": 2.707387, "test_losses": 2.707387, "train_accs": 0.991111, "test_accs": 0.991111, "grad_norms": 0.003853, "param_norms": 1.778832}, {"epoch": 550, "train_losses": 2.707312, "test_losses": 2.707312, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003888, "param_norms": 1.784724}, {"epoch": 600, "train_losses": 2.707236, "test_losses": 2.707236, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003927, "param_norms": 1.79123}, {"epoch": 650, "train_losses": 2.707158, "test_losses": 2.707158, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00397, "param_norms": 1.798355}, {"epoch": 700, "train_losses": 2.707078, "test_losses": 2.707078, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004017, "param_norms": 1.806106}, {"epoch": 750, "train_losses": 2.706996, "test_losses": 2.706996, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004067, "param_norms": 1.814492}, {"epoch": 800, "train_losses": 2.706913, "test_losses": 2.706913, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004122, "param_norms": 1.823519}, {"epoch": 850, "train_losses": 2.706827, "test_losses": 2.706827, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004181, "param_norms": 1.833197}, {"epoch": 900, "train_losses": 2.706738, "test_losses": 2.706738, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004245, "param_norms": 1.843535}, {"epoch": 950, "train_losses": 2.706646, "test_losses": 2.706646, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004312, "param_norms": 1.854545}, {"epoch": 1000, "train_losses": 2.706552, "test_losses": 2.706552, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004384, "param_norms": 1.866235}, {"epoch": 1050, "train_losses": 2.706454, "test_losses": 2.706454, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004461, "param_norms": 1.878621}, {"epoch": 1100, "train_losses": 2.706353, "test_losses": 2.706353, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004543, "param_norms": 1.891715}, {"epoch": 1150, "train_losses": 2.706248, "test_losses": 2.706248, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004629, "param_norms": 1.905531}, {"epoch": 1200, "train_losses": 2.706138, "test_losses": 2.706138, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004721, "param_norms": 1.920084}, {"epoch": 1250, "train_losses": 2.706025, "test_losses": 2.706025, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004818, "param_norms": 1.93539}, {"epoch": 1300, "train_losses": 2.705906, "test_losses": 2.705906, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004921, "param_norms": 1.95147}, {"epoch": 1350, "train_losses": 2.705782, "test_losses": 2.705782, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00503, "param_norms": 1.96834}, {"epoch": 1400, "train_losses": 2.705653, "test_losses": 2.705653, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005144, "param_norms": 1.986022}, {"epoch": 1450, "train_losses": 2.705518, "test_losses": 2.705518, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005265, "param_norms": 2.004538}, {"epoch": 1500, "train_losses": 2.705376, "test_losses": 2.705376, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005393, "param_norms": 2.023911}, {"epoch": 1550, "train_losses": 2.705227, "test_losses": 2.705227, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005528, "param_norms": 2.044167}, {"epoch": 1600, "train_losses": 2.70507, "test_losses": 2.70507, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00567, "param_norms": 2.065333}, {"epoch": 1650, "train_losses": 2.704905, "test_losses": 2.704905, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00582, "param_norms": 2.087441}, {"epoch": 1700, "train_losses": 2.704731, "test_losses": 2.704731, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005978, "param_norms": 2.110519}, {"epoch": 1750, "train_losses": 2.704548, "test_losses": 2.704548, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006145, "param_norms": 2.134604}, {"epoch": 1800, "train_losses": 2.704353, "test_losses": 2.704353, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006321, "param_norms": 2.15973}, {"epoch": 1850, "train_losses": 2.704148, "test_losses": 2.704148, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006507, "param_norms": 2.185938}, {"epoch": 1900, "train_losses": 2.70393, "test_losses": 2.70393, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006703, "param_norms": 2.213269}, {"epoch": 1950, "train_losses": 2.703698, "test_losses": 2.703698, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006911, "param_norms": 2.24177}, {"epoch": 2000, "train_losses": 2.703452, "test_losses": 2.703452, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007129, "param_norms": 2.27149}, {"epoch": 2050, "train_losses": 2.70319, "test_losses": 2.70319, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007361, "param_norms": 2.30248}, {"epoch": 2100, "train_losses": 2.70291, "test_losses": 2.70291, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007606, "param_norms": 2.334798}, {"epoch": 2150, "train_losses": 2.702611, "test_losses": 2.702611, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007865, "param_norms": 2.368505}, {"epoch": 2200, "train_losses": 2.702291, "test_losses": 2.702291, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008139, "param_norms": 2.403668}, {"epoch": 2250, "train_losses": 2.701948, "test_losses": 2.701948, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00843, "param_norms": 2.440361}, {"epoch": 2300, "train_losses": 2.701579, "test_losses": 2.701579, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008739, "param_norms": 2.478657}, {"epoch": 2350, "train_losses": 2.701184, "test_losses": 2.701184, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009067, "param_norms": 2.518645}, {"epoch": 2400, "train_losses": 2.700757, "test_losses": 2.700757, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009416, "param_norms": 2.560414}, {"epoch": 2450, "train_losses": 2.700296, "test_losses": 2.700296, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009787, "param_norms": 2.604063}, {"epoch": 2500, "train_losses": 2.699798, "test_losses": 2.699798, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010182, "param_norms": 2.649703}, {"epoch": 2550, "train_losses": 2.699258, "test_losses": 2.699258, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010604, "param_norms": 2.697451}, {"epoch": 2600, "train_losses": 2.698672, "test_losses": 2.698672, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011055, "param_norms": 2.747435}, {"epoch": 2650, "train_losses": 2.698034, "test_losses": 2.698034, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011537, "param_norms": 2.799797}, {"epoch": 2700, "train_losses": 2.69734, "test_losses": 2.69734, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012053, "param_norms": 2.854692}, {"epoch": 2750, "train_losses": 2.69658, "test_losses": 2.69658, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012607, "param_norms": 2.912288}, {"epoch": 2800, "train_losses": 2.695748, "test_losses": 2.695748, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013202, "param_norms": 2.972774}, {"epoch": 2850, "train_losses": 2.694835, "test_losses": 2.694835, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013843, "param_norms": 3.036354}, {"epoch": 2900, "train_losses": 2.693829, "test_losses": 2.693829, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014534, "param_norms": 3.103256}, {"epoch": 2950, "train_losses": 2.692719, "test_losses": 2.692719, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01528, "param_norms": 3.173733}, {"epoch": 3000, "train_losses": 2.69149, "test_losses": 2.69149, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016089, "param_norms": 3.248062}, {"epoch": 3050, "train_losses": 2.690126, "test_losses": 2.690126, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016966, "param_norms": 3.326557}, {"epoch": 3100, "train_losses": 2.688606, "test_losses": 2.688606, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01792, "param_norms": 3.409565}, {"epoch": 3150, "train_losses": 2.686908, "test_losses": 2.686908, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01896, "param_norms": 3.497476}, {"epoch": 3200, "train_losses": 2.685004, "test_losses": 2.685004, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.020097, "param_norms": 3.590727}, {"epoch": 3250, "train_losses": 2.682859, "test_losses": 2.682859, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.021344, "param_norms": 3.689812}, {"epoch": 3300, "train_losses": 2.680436, "test_losses": 2.680436, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.022714, "param_norms": 3.795291}, {"epoch": 3350, "train_losses": 2.677685, "test_losses": 2.677685, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.024226, "param_norms": 3.9078}, {"epoch": 3400, "train_losses": 2.674549, "test_losses": 2.674549, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0259, "param_norms": 4.028061}, {"epoch": 3450, "train_losses": 2.670956, "test_losses": 2.670956, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.027759, "param_norms": 4.156907}, {"epoch": 3500, "train_losses": 2.666817, "test_losses": 2.666817, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.029832, "param_norms": 4.295294}, {"epoch": 3550, "train_losses": 2.662022, "test_losses": 2.662022, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.032154, "param_norms": 4.444331}, {"epoch": 3600, "train_losses": 2.656435, "test_losses": 2.656435, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.034765, "param_norms": 4.605308}, {"epoch": 3650, "train_losses": 2.649881, "test_losses": 2.649881, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.037718, "param_norms": 4.779732}, {"epoch": 3700, "train_losses": 2.642137, "test_losses": 2.642137, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.041074, "param_norms": 4.969381}, {"epoch": 3750, "train_losses": 2.632916, "test_losses": 2.632916, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.044908, "param_norms": 5.176368}, {"epoch": 3800, "train_losses": 2.621845, "test_losses": 2.621845, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.049318, "param_norms": 5.403206}, {"epoch": 3850, "train_losses": 2.608428, "test_losses": 2.608428, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.054422, "param_norms": 5.652927}, {"epoch": 3900, "train_losses": 2.592003, "test_losses": 2.592003, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.060371, "param_norms": 5.929201}, {"epoch": 3950, "train_losses": 2.571673, "test_losses": 2.571673, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.067358, "param_norms": 6.236515}, {"epoch": 4000, "train_losses": 2.546203, "test_losses": 2.546203, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.075629, "param_norms": 6.580399}, {"epoch": 4050, "train_losses": 2.513869, "test_losses": 2.513869, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0855, "param_norms": 6.967712}, {"epoch": 4100, "train_losses": 2.47223, "test_losses": 2.47223, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.097376, "param_norms": 7.407022}, {"epoch": 4150, "train_losses": 2.417788, "test_losses": 2.417788, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.111767, "param_norms": 7.909054}, {"epoch": 4200, "train_losses": 2.345479, "test_losses": 2.345479, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.129287, "param_norms": 8.487206}, {"epoch": 4250, "train_losses": 2.247998, "test_losses": 2.247998, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.150595, "param_norms": 9.157899}, {"epoch": 4300, "train_losses": 2.115065, "test_losses": 2.115065, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.17615, "param_norms": 9.94026}, {"epoch": 4350, "train_losses": 1.933388, "test_losses": 1.933388, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.205498, "param_norms": 10.853614}, {"epoch": 4400, "train_losses": 1.689702, "test_losses": 1.689702, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.235651, "param_norms": 11.909365}, {"epoch": 4450, "train_losses": 1.381379, "test_losses": 1.381379, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.259013, "param_norms": 13.093186}, {"epoch": 4500, "train_losses": 1.033969, "test_losses": 1.033969, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.264259, "param_norms": 14.343005}, {"epoch": 4550, "train_losses": 0.706031, "test_losses": 0.706031, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.243059, "param_norms": 15.547494}, {"epoch": 4600, "train_losses": 0.455987, "test_losses": 0.455987, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.201054, "param_norms": 16.589395}, {"epoch": 4650, "train_losses": 0.29598, "test_losses": 0.29598, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.156214, "param_norms": 17.416021}, {"epoch": 4700, "train_losses": 0.200983, "test_losses": 0.200983, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.120068, "param_norms": 18.049367}, {"epoch": 4750, "train_losses": 0.144133, "test_losses": 0.144133, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.093886, "param_norms": 18.537632}, {"epoch": 4800, "train_losses": 0.108569, "test_losses": 0.108569, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.075309, "param_norms": 18.922972}, {"epoch": 4850, "train_losses": 0.085142, "test_losses": 0.085142, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.061943, "param_norms": 19.235264}, {"epoch": 4900, "train_losses": 0.068956, "test_losses": 0.068956, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.052087, "param_norms": 19.494588}, {"epoch": 4950, "train_losses": 0.057302, "test_losses": 0.057302, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.044626, "param_norms": 19.714449}, {"epoch": 4999, "train_losses": 0.048769, "test_losses": 0.048769, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.03894, "param_norms": 19.900596}], "total_epochs": 5000}, "relu_single_freq": {"config": {"prime": 15, "d_mlp": 512, "act_type": "ReLU", "init_type": "single-freq", "init_scale": 0.002, "optimizer": "SGD", "lr": 0.01, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=15, run=relu_single_freq\n======================================================================\n\nConfiguration:\n  prime (p)       = 15\n  d_mlp           = 512\n  activation      = ReLU\n  init_type       = single-freq\n  init_scale      = 0.002\n  optimizer       = SGD\n  learning_rate   = 0.01\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      2.708055      2.708055      0.0489      0.0489      0.0045       0.1753\n      50      2.708045      2.708045      0.0667      0.0667      0.0045       0.1753\n     100      2.708035      2.708035      0.0933      0.0933      0.0045       0.1753\n     150      2.708025      2.708025      0.1778      0.1778      0.0045       0.1754\n     200      2.708015      2.708015      0.2444      0.2444      0.0045       0.1756\n     250      2.708004      2.708004      0.3067      0.3067      0.0045       0.1758\n     300      2.707995      2.707995      0.3556      0.3556      0.0045       0.1761\n     350      2.707985      2.707985      0.4667      0.4667      0.0045       0.1765\n     400      2.707975      2.707975      0.5644      0.5644      0.0045       0.1769\n     450      2.707964      2.707964      0.6267      0.6267      0.0045       0.1773\n     500      2.707954      2.707954      0.6889      0.6889      0.0045       0.1778\n     550      2.707944      2.707944      0.7644      0.7644      0.0045       0.1784\n     600      2.707935      2.707935      0.8178      0.8178      0.0045       0.1790\n     650      2.707924      2.707924      0.8622      0.8622      0.0045       0.1797\n     700      2.707915      2.707915      0.8800      0.8800      0.0045       0.1804\n     750      2.707905      2.707905      0.9156      0.9156      0.0045       0.1812\n     800      2.707894      2.707894      0.9422      0.9422      0.0046       0.1820\n     850      2.707885      2.707885      0.9511      0.9511      0.0046       0.1829\n     900      2.707875      2.707875      0.9600      0.9600      0.0046       0.1838\n     950      2.707864      2.707864      0.9689      0.9689      0.0046       0.1848\n    1000      2.707855      2.707855      0.9778      0.9778      0.0046       0.1858\n    1050      2.707844      2.707844      0.9867      0.9867      0.0046       0.1869\n    1100      2.707834      2.707834      0.9956      0.9956      0.0046       0.1880\n    1150      2.707824      2.707824      1.0000      1.0000      0.0047       0.1892\n    1200      2.707814      2.707814      1.0000      1.0000      0.0047       0.1904\n    1250      2.707804      2.707804      1.0000      1.0000      0.0047       0.1917\n    1300      2.707793      2.707793      1.0000      1.0000      0.0047       0.1930\n    1350      2.707784      2.707784      1.0000      1.0000      0.0047       0.1944\n    1400      2.707773      2.707773      1.0000      1.0000      0.0047       0.1957\n    1450      2.707763      2.707763      1.0000      1.0000      0.0048       0.1972\n    1500      2.707753      2.707753      1.0000      1.0000      0.0048       0.1987\n    1550      2.707742      2.707742      1.0000      1.0000      0.0048       0.2002\n    1600      2.707732      2.707732      1.0000      1.0000      0.0049       0.2017\n    1650      2.707721      2.707721      1.0000      1.0000      0.0049       0.2033\n    1700      2.707710      2.707710      1.0000      1.0000      0.0049       0.2050\n    1750      2.707700      2.707700      1.0000      1.0000      0.0050       0.2067\n    1800      2.707689      2.707689      1.0000      1.0000      0.0050       0.2084\n    1850      2.707678      2.707678      1.0000      1.0000      0.0051       0.2101\n    1900      2.707667      2.707667      1.0000      1.0000      0.0051       0.2119\n    1950      2.707655      2.707655      1.0000      1.0000      0.0052       0.2137\n    2000      2.707644      2.707644      1.0000      1.0000      0.0052       0.2156\n    2050      2.707633      2.707633      1.0000      1.0000      0.0052       0.2175\n    2100      2.707621      2.707621      1.0000      1.0000      0.0053       0.2194\n    2150      2.707609      2.707609      1.0000      1.0000      0.0053       0.2214\n    2200      2.707597      2.707597      1.0000      1.0000      0.0054       0.2234\n    2250      2.707585      2.707585      1.0000      1.0000      0.0054       0.2255\n    2300      2.707573      2.707573      1.0000      1.0000      0.0055       0.2276\n    2350      2.707561      2.707561      1.0000      1.0000      0.0055       0.2297\n    2400      2.707548      2.707548      1.0000      1.0000      0.0056       0.2318\n    2450      2.707535      2.707535      1.0000      1.0000      0.0056       0.2340\n    2500      2.707522      2.707522      1.0000      1.0000      0.0057       0.2362\n    2550      2.707509      2.707509      1.0000      1.0000      0.0058       0.2385\n    2600      2.707496      2.707496      1.0000      1.0000      0.0058       0.2408\n    2650      2.707482      2.707482      1.0000      1.0000      0.0059       0.2431\n    2700      2.707468      2.707468      1.0000      1.0000      0.0059       0.2454\n    2750      2.707454      2.707454      1.0000      1.0000      0.0060       0.2478\n    2800      2.707440      2.707440      1.0000      1.0000      0.0060       0.2502\n    2850      2.707425      2.707425      1.0000      1.0000      0.0061       0.2527\n    2900      2.707411      2.707411      1.0000      1.0000      0.0062       0.2552\n    2950      2.707396      2.707396      1.0000      1.0000      0.0062       0.2577\n    3000      2.707380      2.707380      1.0000      1.0000      0.0063       0.2603\n    3050      2.707365      2.707365      1.0000      1.0000      0.0064       0.2629\n    3100      2.707349      2.707349      1.0000      1.0000      0.0064       0.2655\n    3150      2.707333      2.707333      1.0000      1.0000      0.0065       0.2681\n    3200      2.707317      2.707317      1.0000      1.0000      0.0066       0.2708\n    3250      2.707300      2.707300      1.0000      1.0000      0.0066       0.2736\n    3300      2.707283      2.707283      1.0000      1.0000      0.0067       0.2763\n    3350      2.707265      2.707265      1.0000      1.0000      0.0068       0.2791\n    3400      2.707247      2.707247      1.0000      1.0000      0.0069       0.2819\n    3450      2.707229      2.707229      1.0000      1.0000      0.0069       0.2848\n    3500      2.707211      2.707211      1.0000      1.0000      0.0070       0.2877\n    3550      2.707192      2.707192      1.0000      1.0000      0.0071       0.2906\n    3600      2.707173      2.707173      1.0000      1.0000      0.0072       0.2936\n    3650      2.707153      2.707153      1.0000      1.0000      0.0073       0.2966\n    3700      2.707133      2.707133      1.0000      1.0000      0.0073       0.2997\n    3750      2.707113      2.707113      1.0000      1.0000      0.0074       0.3027\n    3800      2.707092      2.707092      1.0000      1.0000      0.0075       0.3058\n    3850      2.707071      2.707071      1.0000      1.0000      0.0076       0.3090\n    3900      2.707049      2.707049      1.0000      1.0000      0.0077       0.3122\n    3950      2.707027      2.707027      1.0000      1.0000      0.0078       0.3154\n    4000      2.707005      2.707005      1.0000      1.0000      0.0079       0.3187\n    4050      2.706982      2.706982      1.0000      1.0000      0.0080       0.3220\n    4100      2.706959      2.706959      1.0000      1.0000      0.0081       0.3253\n    4150      2.706934      2.706934      1.0000      1.0000      0.0082       0.3287\n    4200      2.706910      2.706910      1.0000      1.0000      0.0083       0.3321\n    4250      2.706885      2.706885      1.0000      1.0000      0.0084       0.3355\n    4300      2.706860      2.706860      1.0000      1.0000      0.0084       0.3390\n    4350      2.706834      2.706834      1.0000      1.0000      0.0085       0.3426\n    4400      2.706807      2.706807      1.0000      1.0000      0.0086       0.3461\n    4450      2.706780      2.706780      1.0000      1.0000      0.0087       0.3498\n    4500      2.706752      2.706752      1.0000      1.0000      0.0088       0.3534\n    4550      2.706724      2.706724      1.0000      1.0000      0.0089       0.3571\n    4600      2.706695      2.706695      1.0000      1.0000      0.0090       0.3608\n    4650      2.706666      2.706666      1.0000      1.0000      0.0091       0.3646\n    4700      2.706635      2.706635      1.0000      1.0000      0.0092       0.3684\n    4750      2.706604      2.706604      1.0000      1.0000      0.0093       0.3723\n    4800      2.706573      2.706573      1.0000      1.0000      0.0094       0.3762\n    4850      2.706541      2.706541      1.0000      1.0000      0.0095       0.3801\n    4900      2.706508      2.706508      1.0000      1.0000      0.0096       0.3841\n    4950      2.706474      2.706474      1.0000      1.0000      0.0097       0.3882\n    4999      2.706441      2.706441      1.0000      1.0000      0.0099       0.3922\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 2.706441\n  Test Loss   = 2.706441\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 0.3922\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 2.708055, "test_losses": 2.708055, "train_accs": 0.048889, "test_accs": 0.048889, "grad_norms": 0.004524, "param_norms": 0.175271}, {"epoch": 50, "train_losses": 2.708045, "test_losses": 2.708045, "train_accs": 0.066667, "test_accs": 0.066667, "grad_norms": 0.004519, "param_norms": 0.175272}, {"epoch": 100, "train_losses": 2.708035, "test_losses": 2.708035, "train_accs": 0.093333, "test_accs": 0.093333, "grad_norms": 0.004521, "param_norms": 0.175328}, {"epoch": 150, "train_losses": 2.708025, "test_losses": 2.708025, "train_accs": 0.177778, "test_accs": 0.177778, "grad_norms": 0.004521, "param_norms": 0.175442}, {"epoch": 200, "train_losses": 2.708015, "test_losses": 2.708015, "train_accs": 0.244444, "test_accs": 0.244444, "grad_norms": 0.004512, "param_norms": 0.175614}, {"epoch": 250, "train_losses": 2.708004, "test_losses": 2.708004, "train_accs": 0.306667, "test_accs": 0.306667, "grad_norms": 0.004509, "param_norms": 0.175843}, {"epoch": 300, "train_losses": 2.707995, "test_losses": 2.707995, "train_accs": 0.355556, "test_accs": 0.355556, "grad_norms": 0.004511, "param_norms": 0.17613}, {"epoch": 350, "train_losses": 2.707985, "test_losses": 2.707985, "train_accs": 0.466667, "test_accs": 0.466667, "grad_norms": 0.004511, "param_norms": 0.176473}, {"epoch": 400, "train_losses": 2.707975, "test_losses": 2.707975, "train_accs": 0.564444, "test_accs": 0.564444, "grad_norms": 0.004516, "param_norms": 0.176872}, {"epoch": 450, "train_losses": 2.707964, "test_losses": 2.707964, "train_accs": 0.626667, "test_accs": 0.626667, "grad_norms": 0.004519, "param_norms": 0.177327}, {"epoch": 500, "train_losses": 2.707954, "test_losses": 2.707954, "train_accs": 0.688889, "test_accs": 0.688889, "grad_norms": 0.004526, "param_norms": 0.177838}, {"epoch": 550, "train_losses": 2.707944, "test_losses": 2.707944, "train_accs": 0.764444, "test_accs": 0.764444, "grad_norms": 0.004518, "param_norms": 0.178403}, {"epoch": 600, "train_losses": 2.707935, "test_losses": 2.707935, "train_accs": 0.817778, "test_accs": 0.817778, "grad_norms": 0.004514, "param_norms": 0.179023}, {"epoch": 650, "train_losses": 2.707924, "test_losses": 2.707924, "train_accs": 0.862222, "test_accs": 0.862222, "grad_norms": 0.00452, "param_norms": 0.179696}, {"epoch": 700, "train_losses": 2.707915, "test_losses": 2.707915, "train_accs": 0.88, "test_accs": 0.88, "grad_norms": 0.004521, "param_norms": 0.180422}, {"epoch": 750, "train_losses": 2.707905, "test_losses": 2.707905, "train_accs": 0.915556, "test_accs": 0.915556, "grad_norms": 0.004548, "param_norms": 0.181199}, {"epoch": 800, "train_losses": 2.707894, "test_losses": 2.707894, "train_accs": 0.942222, "test_accs": 0.942222, "grad_norms": 0.004551, "param_norms": 0.182029}, {"epoch": 850, "train_losses": 2.707885, "test_losses": 2.707885, "train_accs": 0.951111, "test_accs": 0.951111, "grad_norms": 0.004569, "param_norms": 0.182909}, {"epoch": 900, "train_losses": 2.707875, "test_losses": 2.707875, "train_accs": 0.96, "test_accs": 0.96, "grad_norms": 0.004576, "param_norms": 0.183839}, {"epoch": 950, "train_losses": 2.707864, "test_losses": 2.707864, "train_accs": 0.968889, "test_accs": 0.968889, "grad_norms": 0.004592, "param_norms": 0.184819}, {"epoch": 1000, "train_losses": 2.707855, "test_losses": 2.707855, "train_accs": 0.977778, "test_accs": 0.977778, "grad_norms": 0.004599, "param_norms": 0.185848}, {"epoch": 1050, "train_losses": 2.707844, "test_losses": 2.707844, "train_accs": 0.986667, "test_accs": 0.986667, "grad_norms": 0.004616, "param_norms": 0.186925}, {"epoch": 1100, "train_losses": 2.707834, "test_losses": 2.707834, "train_accs": 0.995556, "test_accs": 0.995556, "grad_norms": 0.00463, "param_norms": 0.18805}, {"epoch": 1150, "train_losses": 2.707824, "test_losses": 2.707824, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004653, "param_norms": 0.189221}, {"epoch": 1200, "train_losses": 2.707814, "test_losses": 2.707814, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00467, "param_norms": 0.190438}, {"epoch": 1250, "train_losses": 2.707804, "test_losses": 2.707804, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004696, "param_norms": 0.1917}, {"epoch": 1300, "train_losses": 2.707793, "test_losses": 2.707793, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004709, "param_norms": 0.193006}, {"epoch": 1350, "train_losses": 2.707784, "test_losses": 2.707784, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004719, "param_norms": 0.194357}, {"epoch": 1400, "train_losses": 2.707773, "test_losses": 2.707773, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004745, "param_norms": 0.195749}, {"epoch": 1450, "train_losses": 2.707763, "test_losses": 2.707763, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004777, "param_norms": 0.197185}, {"epoch": 1500, "train_losses": 2.707753, "test_losses": 2.707753, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0048, "param_norms": 0.198662}, {"epoch": 1550, "train_losses": 2.707742, "test_losses": 2.707742, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004832, "param_norms": 0.200179}, {"epoch": 1600, "train_losses": 2.707732, "test_losses": 2.707732, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004863, "param_norms": 0.201738}, {"epoch": 1650, "train_losses": 2.707721, "test_losses": 2.707721, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004892, "param_norms": 0.203336}, {"epoch": 1700, "train_losses": 2.70771, "test_losses": 2.70771, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004927, "param_norms": 0.204974}, {"epoch": 1750, "train_losses": 2.7077, "test_losses": 2.7077, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004958, "param_norms": 0.206651}, {"epoch": 1800, "train_losses": 2.707689, "test_losses": 2.707689, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005007, "param_norms": 0.208366}, {"epoch": 1850, "train_losses": 2.707678, "test_losses": 2.707678, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005064, "param_norms": 0.21012}, {"epoch": 1900, "train_losses": 2.707667, "test_losses": 2.707667, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005109, "param_norms": 0.211911}, {"epoch": 1950, "train_losses": 2.707655, "test_losses": 2.707655, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005156, "param_norms": 0.213739}, {"epoch": 2000, "train_losses": 2.707644, "test_losses": 2.707644, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0052, "param_norms": 0.215604}, {"epoch": 2050, "train_losses": 2.707633, "test_losses": 2.707633, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005241, "param_norms": 0.217506}, {"epoch": 2100, "train_losses": 2.707621, "test_losses": 2.707621, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005277, "param_norms": 0.219444}, {"epoch": 2150, "train_losses": 2.707609, "test_losses": 2.707609, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005324, "param_norms": 0.221418}, {"epoch": 2200, "train_losses": 2.707597, "test_losses": 2.707597, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005383, "param_norms": 0.223428}, {"epoch": 2250, "train_losses": 2.707585, "test_losses": 2.707585, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005417, "param_norms": 0.225473}, {"epoch": 2300, "train_losses": 2.707573, "test_losses": 2.707573, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005477, "param_norms": 0.227553}, {"epoch": 2350, "train_losses": 2.707561, "test_losses": 2.707561, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005532, "param_norms": 0.229668}, {"epoch": 2400, "train_losses": 2.707548, "test_losses": 2.707548, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005589, "param_norms": 0.231817}, {"epoch": 2450, "train_losses": 2.707535, "test_losses": 2.707535, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005631, "param_norms": 0.234001}, {"epoch": 2500, "train_losses": 2.707522, "test_losses": 2.707522, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005699, "param_norms": 0.23622}, {"epoch": 2550, "train_losses": 2.707509, "test_losses": 2.707509, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005755, "param_norms": 0.238472}, {"epoch": 2600, "train_losses": 2.707496, "test_losses": 2.707496, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005798, "param_norms": 0.240759}, {"epoch": 2650, "train_losses": 2.707482, "test_losses": 2.707482, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005856, "param_norms": 0.243079}, {"epoch": 2700, "train_losses": 2.707468, "test_losses": 2.707468, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005914, "param_norms": 0.245433}, {"epoch": 2750, "train_losses": 2.707454, "test_losses": 2.707454, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00601, "param_norms": 0.247821}, {"epoch": 2800, "train_losses": 2.70744, "test_losses": 2.70744, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006048, "param_norms": 0.250243}, {"epoch": 2850, "train_losses": 2.707425, "test_losses": 2.707425, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006101, "param_norms": 0.252698}, {"epoch": 2900, "train_losses": 2.707411, "test_losses": 2.707411, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006177, "param_norms": 0.255188}, {"epoch": 2950, "train_losses": 2.707396, "test_losses": 2.707396, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006206, "param_norms": 0.25771}, {"epoch": 3000, "train_losses": 2.70738, "test_losses": 2.70738, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006307, "param_norms": 0.260267}, {"epoch": 3050, "train_losses": 2.707365, "test_losses": 2.707365, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006379, "param_norms": 0.262857}, {"epoch": 3100, "train_losses": 2.707349, "test_losses": 2.707349, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00644, "param_norms": 0.265482}, {"epoch": 3150, "train_losses": 2.707333, "test_losses": 2.707333, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006487, "param_norms": 0.26814}, {"epoch": 3200, "train_losses": 2.707317, "test_losses": 2.707317, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006562, "param_norms": 0.270832}, {"epoch": 3250, "train_losses": 2.7073, "test_losses": 2.7073, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006628, "param_norms": 0.273558}, {"epoch": 3300, "train_losses": 2.707283, "test_losses": 2.707283, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006727, "param_norms": 0.276319}, {"epoch": 3350, "train_losses": 2.707265, "test_losses": 2.707265, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006811, "param_norms": 0.279114}, {"epoch": 3400, "train_losses": 2.707247, "test_losses": 2.707247, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006852, "param_norms": 0.281943}, {"epoch": 3450, "train_losses": 2.707229, "test_losses": 2.707229, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006944, "param_norms": 0.284808}, {"epoch": 3500, "train_losses": 2.707211, "test_losses": 2.707211, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007005, "param_norms": 0.287707}, {"epoch": 3550, "train_losses": 2.707192, "test_losses": 2.707192, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007096, "param_norms": 0.290641}, {"epoch": 3600, "train_losses": 2.707173, "test_losses": 2.707173, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007194, "param_norms": 0.293611}, {"epoch": 3650, "train_losses": 2.707153, "test_losses": 2.707153, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007264, "param_norms": 0.296617}, {"epoch": 3700, "train_losses": 2.707133, "test_losses": 2.707133, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007323, "param_norms": 0.299658}, {"epoch": 3750, "train_losses": 2.707113, "test_losses": 2.707113, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007405, "param_norms": 0.302735}, {"epoch": 3800, "train_losses": 2.707092, "test_losses": 2.707092, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00751, "param_norms": 0.305848}, {"epoch": 3850, "train_losses": 2.707071, "test_losses": 2.707071, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007574, "param_norms": 0.308998}, {"epoch": 3900, "train_losses": 2.707049, "test_losses": 2.707049, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007714, "param_norms": 0.312185}, {"epoch": 3950, "train_losses": 2.707027, "test_losses": 2.707027, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007772, "param_norms": 0.315409}, {"epoch": 4000, "train_losses": 2.707005, "test_losses": 2.707005, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007879, "param_norms": 0.31867}, {"epoch": 4050, "train_losses": 2.706982, "test_losses": 2.706982, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007955, "param_norms": 0.321969}, {"epoch": 4100, "train_losses": 2.706959, "test_losses": 2.706959, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008085, "param_norms": 0.325305}, {"epoch": 4150, "train_losses": 2.706934, "test_losses": 2.706934, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008158, "param_norms": 0.32868}, {"epoch": 4200, "train_losses": 2.70691, "test_losses": 2.70691, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008255, "param_norms": 0.332094}, {"epoch": 4250, "train_losses": 2.706885, "test_losses": 2.706885, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008367, "param_norms": 0.335546}, {"epoch": 4300, "train_losses": 2.70686, "test_losses": 2.70686, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00842, "param_norms": 0.339038}, {"epoch": 4350, "train_losses": 2.706834, "test_losses": 2.706834, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008507, "param_norms": 0.342569}, {"epoch": 4400, "train_losses": 2.706807, "test_losses": 2.706807, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008604, "param_norms": 0.34614}, {"epoch": 4450, "train_losses": 2.70678, "test_losses": 2.70678, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008705, "param_norms": 0.349752}, {"epoch": 4500, "train_losses": 2.706752, "test_losses": 2.706752, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008814, "param_norms": 0.353404}, {"epoch": 4550, "train_losses": 2.706724, "test_losses": 2.706724, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008924, "param_norms": 0.357097}, {"epoch": 4600, "train_losses": 2.706695, "test_losses": 2.706695, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009006, "param_norms": 0.360832}, {"epoch": 4650, "train_losses": 2.706666, "test_losses": 2.706666, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00913, "param_norms": 0.364609}, {"epoch": 4700, "train_losses": 2.706635, "test_losses": 2.706635, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009206, "param_norms": 0.368428}, {"epoch": 4750, "train_losses": 2.706604, "test_losses": 2.706604, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00931, "param_norms": 0.37229}, {"epoch": 4800, "train_losses": 2.706573, "test_losses": 2.706573, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009409, "param_norms": 0.376194}, {"epoch": 4850, "train_losses": 2.706541, "test_losses": 2.706541, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009512, "param_norms": 0.380143}, {"epoch": 4900, "train_losses": 2.706508, "test_losses": 2.706508, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009644, "param_norms": 0.384136}, {"epoch": 4950, "train_losses": 2.706474, "test_losses": 2.706474, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009744, "param_norms": 0.388173}, {"epoch": 4999, "train_losses": 2.706441, "test_losses": 2.706441, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009858, "param_norms": 0.392173}], "total_epochs": 5000}}