{"standard": {"config": {"prime": 31, "d_mlp": 931, "act_type": "ReLU", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 5e-05, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=31, run=standard\n======================================================================\n\nConfiguration:\n  prime (p)       = 31\n  d_mlp           = 931\n  activation      = ReLU\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 5e-05\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.433967      3.433967      0.0364      0.0364      0.0160       4.3206\n      50      3.427824      3.427824      0.1186      0.1186      0.0157       4.3785\n     100      3.420660      3.420660      0.2893      0.2893      0.0166       4.5857\n     150      3.412555      3.412555      0.5702      0.5702      0.0184       4.9007\n     200      3.403128      3.403128      0.7492      0.7492      0.0210       5.2865\n     250      3.392056      3.392056      0.8450      0.8450      0.0241       5.7178\n     300      3.379155      3.379155      0.9074      0.9074      0.0276       6.1786\n     350      3.364295      3.364295      0.9386      0.9386      0.0315       6.6585\n     400      3.347432      3.347432      0.9625      0.9625      0.0354       7.1520\n     450      3.328524      3.328524      0.9698      0.9698      0.0396       7.6550\n     500      3.307514      3.307514      0.9792      0.9792      0.0439       8.1649\n     550      3.284378      3.284378      0.9781      0.9781      0.0481       8.6801\n     600      3.259124      3.259124      0.9771      0.9771      0.0525       9.1996\n     650      3.231740      3.231740      0.9792      0.9792      0.0568       9.7228\n     700      3.202228      3.202228      0.9802      0.9802      0.0612      10.2493\n     750      3.170569      3.170569      0.9802      0.9802      0.0657      10.7786\n     800      3.136745      3.136745      0.9802      0.9802      0.0701      11.3105\n     850      3.100763      3.100763      0.9802      0.9802      0.0745      11.8447\n     900      3.062609      3.062609      0.9802      0.9802      0.0790      12.3810\n     950      3.022317      3.022317      0.9802      0.9802      0.0835      12.9190\n    1000      2.979927      2.979927      0.9802      0.9802      0.0879      13.4582\n    1050      2.935420      2.935420      0.9802      0.9802      0.0923      13.9982\n    1100      2.888808      2.888808      0.9802      0.9802      0.0966      14.5390\n    1150      2.840152      2.840152      0.9802      0.9802      0.1010      15.0806\n    1200      2.789488      2.789488      0.9813      0.9813      0.1052      15.6231\n    1250      2.736830      2.736830      0.9813      0.9813      0.1094      16.1662\n    1300      2.682209      2.682209      0.9813      0.9813      0.1136      16.7097\n    1350      2.625640      2.625640      0.9802      0.9802      0.1177      17.2535\n    1400      2.567185      2.567185      0.9802      0.9802      0.1217      17.7975\n    1450      2.506899      2.506899      0.9802      0.9802      0.1256      18.3415\n    1500      2.444830      2.444830      0.9802      0.9802      0.1294      18.8855\n    1550      2.381073      2.381073      0.9802      0.9802      0.1331      19.4294\n    1600      2.315698      2.315698      0.9802      0.9802      0.1365      19.9729\n    1650      2.248785      2.248785      0.9813      0.9813      0.1399      20.5163\n    1700      2.180366      2.180366      0.9813      0.9813      0.1431      21.0595\n    1750      2.110604      2.110604      0.9813      0.9813      0.1461      21.6023\n    1800      2.039637      2.039637      0.9813      0.9813      0.1488      22.1446\n    1850      1.967574      1.967574      0.9813      0.9813      0.1512      22.6864\n    1900      1.894513      1.894513      0.9813      0.9813      0.1534      23.2272\n    1950      1.820601      1.820601      0.9834      0.9834      0.1553      23.7673\n    2000      1.745982      1.745982      0.9844      0.9844      0.1569      24.3062\n    2050      1.670836      1.670836      0.9844      0.9844      0.1583      24.8439\n    2100      1.595367      1.595367      0.9844      0.9844      0.1591      25.3804\n    2150      1.519828      1.519828      0.9844      0.9844      0.1595      25.9158\n    2200      1.444423      1.444423      0.9844      0.9844      0.1595      26.4498\n    2250      1.369354      1.369354      0.9844      0.9844      0.1590      26.9820\n    2300      1.294820      1.294820      0.9865      0.9865      0.1580      27.5124\n    2350      1.221064      1.221064      0.9886      0.9886      0.1566      28.0407\n    2400      1.148333      1.148333      0.9906      0.9906      0.1547      28.5667\n    2450      1.076893      1.076893      0.9917      0.9917      0.1522      29.0904\n    2500      1.007010      1.007010      0.9917      0.9917      0.1490      29.6119\n    2550      0.938928      0.938928      0.9927      0.9927      0.1455      30.1309\n    2600      0.872845      0.872845      0.9948      0.9948      0.1415      30.6473\n    2650      0.808981      0.808981      0.9948      0.9948      0.1369      31.1608\n    2700      0.747558      0.747558      0.9958      0.9958      0.1320      31.6714\n    2750      0.688734      0.688734      0.9958      0.9958      0.1268      32.1783\n    2800      0.632618      0.632618      0.9969      0.9969      0.1212      32.6813\n    2850      0.579339      0.579339      0.9969      0.9969      0.1153      33.1804\n    2900      0.528942      0.528942      0.9990      0.9990      0.1094      33.6753\n    2950      0.481467      0.481467      0.9990      0.9990      0.1032      34.1661\n    3000      0.436916      0.436916      1.0000      1.0000      0.0971      34.6527\n    3050      0.395254      0.395254      1.0000      1.0000      0.0910      35.1350\n    3100      0.356426      0.356426      1.0000      1.0000      0.0850      35.6129\n    3150      0.320372      0.320372      1.0000      1.0000      0.0790      36.0870\n    3200      0.287025      0.287025      1.0000      1.0000      0.0733      36.5577\n    3250      0.256310      0.256310      1.0000      1.0000      0.0676      37.0257\n    3300      0.228136      0.228136      1.0000      1.0000      0.0621      37.4916\n    3350      0.202416      0.202416      1.0000      1.0000      0.0568      37.9563\n    3400      0.179039      0.179039      1.0000      1.0000      0.0517      38.4205\n    3450      0.157908      0.157908      1.0000      1.0000      0.0469      38.8850\n    3500      0.138891      0.138891      1.0000      1.0000      0.0423      39.3496\n    3550      0.121853      0.121853      1.0000      1.0000      0.0380      39.8147\n    3600      0.106648      0.106648      1.0000      1.0000      0.0340      40.2800\n    3650      0.093129      0.093129      1.0000      1.0000      0.0304      40.7454\n    3700      0.081151      0.081151      1.0000      1.0000      0.0270      41.2103\n    3750      0.070570      0.070570      1.0000      1.0000      0.0239      41.6745\n    3800      0.061248      0.061248      1.0000      1.0000      0.0211      42.1378\n    3850      0.053059      0.053059      1.0000      1.0000      0.0186      42.6003\n    3900      0.045884      0.045884      1.0000      1.0000      0.0164      43.0619\n    3950      0.039611      0.039611      1.0000      1.0000      0.0143      43.5224\n    4000      0.034141      0.034141      1.0000      1.0000      0.0125      43.9819\n    4050      0.029380      0.029380      1.0000      1.0000      0.0110      44.4403\n    4100      0.025244      0.025244      1.0000      1.0000      0.0095      44.8976\n    4150      0.021657      0.021657      1.0000      1.0000      0.0083      45.3537\n    4200      0.018552      0.018552      1.0000      1.0000      0.0072      45.8086\n    4250      0.015870      0.015870      1.0000      1.0000      0.0062      46.2623\n    4300      0.013556      0.013556      1.0000      1.0000      0.0054      46.7149\n    4350      0.011562      0.011562      1.0000      1.0000      0.0047      47.1664\n    4400      0.009848      0.009848      1.0000      1.0000      0.0040      47.6168\n    4450      0.008377      0.008377      1.0000      1.0000      0.0034      48.0661\n    4500      0.007116      0.007116      1.0000      1.0000      0.0030      48.5143\n    4550      0.006037      0.006037      1.0000      1.0000      0.0025      48.9611\n    4600      0.005115      0.005115      1.0000      1.0000      0.0022      49.4068\n    4650      0.004329      0.004329      1.0000      1.0000      0.0019      49.8516\n    4700      0.003658      0.003658      1.0000      1.0000      0.0016      50.2953\n    4750      0.003088      0.003088      1.0000      1.0000      0.0014      50.7379\n    4800      0.002604      0.002604      1.0000      1.0000      0.0012      51.1795\n    4850      0.002193      0.002193      1.0000      1.0000      0.0010      51.6200\n    4900      0.001844      0.001844      1.0000      1.0000      0.0008      52.0595\n    4950      0.001549      0.001549      1.0000      1.0000      0.0007      52.4978\n    4999      0.001305      0.001305      1.0000      1.0000      0.0006      52.9262\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.001305\n  Test Loss   = 0.001305\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 52.9262\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.433967, "test_losses": 3.433967, "train_accs": 0.03642, "test_accs": 0.03642, "grad_norms": 0.016042, "param_norms": 4.320604}, {"epoch": 50, "train_losses": 3.427824, "test_losses": 3.427824, "train_accs": 0.118626, "test_accs": 0.118626, "grad_norms": 0.015733, "param_norms": 4.378544}, {"epoch": 100, "train_losses": 3.42066, "test_losses": 3.42066, "train_accs": 0.289282, "test_accs": 0.289282, "grad_norms": 0.016647, "param_norms": 4.585708}, {"epoch": 150, "train_losses": 3.412555, "test_losses": 3.412555, "train_accs": 0.570239, "test_accs": 0.570239, "grad_norms": 0.01843, "param_norms": 4.900692}, {"epoch": 200, "train_losses": 3.403128, "test_losses": 3.403128, "train_accs": 0.74922, "test_accs": 0.74922, "grad_norms": 0.020985, "param_norms": 5.28654}, {"epoch": 250, "train_losses": 3.392056, "test_losses": 3.392056, "train_accs": 0.844953, "test_accs": 0.844953, "grad_norms": 0.024126, "param_norms": 5.717837}, {"epoch": 300, "train_losses": 3.379155, "test_losses": 3.379155, "train_accs": 0.907388, "test_accs": 0.907388, "grad_norms": 0.027649, "param_norms": 6.17855}, {"epoch": 350, "train_losses": 3.364295, "test_losses": 3.364295, "train_accs": 0.938606, "test_accs": 0.938606, "grad_norms": 0.03148, "param_norms": 6.658492}, {"epoch": 400, "train_losses": 3.347432, "test_losses": 3.347432, "train_accs": 0.962539, "test_accs": 0.962539, "grad_norms": 0.035411, "param_norms": 7.152044}, {"epoch": 450, "train_losses": 3.328524, "test_losses": 3.328524, "train_accs": 0.969823, "test_accs": 0.969823, "grad_norms": 0.039583, "param_norms": 7.655042}, {"epoch": 500, "train_losses": 3.307514, "test_losses": 3.307514, "train_accs": 0.979188, "test_accs": 0.979188, "grad_norms": 0.043864, "param_norms": 8.16488}, {"epoch": 550, "train_losses": 3.284378, "test_losses": 3.284378, "train_accs": 0.978148, "test_accs": 0.978148, "grad_norms": 0.048118, "param_norms": 8.680094}, {"epoch": 600, "train_losses": 3.259124, "test_losses": 3.259124, "train_accs": 0.977107, "test_accs": 0.977107, "grad_norms": 0.052456, "param_norms": 9.199633}, {"epoch": 650, "train_losses": 3.23174, "test_losses": 3.23174, "train_accs": 0.979188, "test_accs": 0.979188, "grad_norms": 0.056833, "param_norms": 9.722793}, {"epoch": 700, "train_losses": 3.202228, "test_losses": 3.202228, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.061217, "param_norms": 10.249294}, {"epoch": 750, "train_losses": 3.170569, "test_losses": 3.170569, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.06565, "param_norms": 10.778623}, {"epoch": 800, "train_losses": 3.136745, "test_losses": 3.136745, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.07008, "param_norms": 11.310487}, {"epoch": 850, "train_losses": 3.100763, "test_losses": 3.100763, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.074534, "param_norms": 11.844713}, {"epoch": 900, "train_losses": 3.062609, "test_losses": 3.062609, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.079028, "param_norms": 12.380989}, {"epoch": 950, "train_losses": 3.022317, "test_losses": 3.022317, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.08346, "param_norms": 12.918983}, {"epoch": 1000, "train_losses": 2.979927, "test_losses": 2.979927, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.087879, "param_norms": 13.458171}, {"epoch": 1050, "train_losses": 2.93542, "test_losses": 2.93542, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.092265, "param_norms": 13.998214}, {"epoch": 1100, "train_losses": 2.888808, "test_losses": 2.888808, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.09663, "param_norms": 14.539041}, {"epoch": 1150, "train_losses": 2.840152, "test_losses": 2.840152, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.101008, "param_norms": 15.080607}, {"epoch": 1200, "train_losses": 2.789488, "test_losses": 2.789488, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.105235, "param_norms": 15.623085}, {"epoch": 1250, "train_losses": 2.73683, "test_losses": 2.73683, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.109426, "param_norms": 16.166234}, {"epoch": 1300, "train_losses": 2.682209, "test_losses": 2.682209, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.113579, "param_norms": 16.70967}, {"epoch": 1350, "train_losses": 2.62564, "test_losses": 2.62564, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.117701, "param_norms": 17.25347}, {"epoch": 1400, "train_losses": 2.567185, "test_losses": 2.567185, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.121688, "param_norms": 17.797548}, {"epoch": 1450, "train_losses": 2.506899, "test_losses": 2.506899, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.1256, "param_norms": 18.341536}, {"epoch": 1500, "train_losses": 2.44483, "test_losses": 2.44483, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.129391, "param_norms": 18.885547}, {"epoch": 1550, "train_losses": 2.381073, "test_losses": 2.381073, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.133101, "param_norms": 19.429369}, {"epoch": 1600, "train_losses": 2.315698, "test_losses": 2.315698, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.13654, "param_norms": 19.972914}, {"epoch": 1650, "train_losses": 2.248785, "test_losses": 2.248785, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.139906, "param_norms": 20.516298}, {"epoch": 1700, "train_losses": 2.180366, "test_losses": 2.180366, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.143138, "param_norms": 21.059505}, {"epoch": 1750, "train_losses": 2.110604, "test_losses": 2.110604, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.146089, "param_norms": 21.602313}, {"epoch": 1800, "train_losses": 2.039637, "test_losses": 2.039637, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.148754, "param_norms": 22.144613}, {"epoch": 1850, "train_losses": 1.967574, "test_losses": 1.967574, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.151224, "param_norms": 22.68635}, {"epoch": 1900, "train_losses": 1.894513, "test_losses": 1.894513, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.153438, "param_norms": 23.227236}, {"epoch": 1950, "train_losses": 1.820601, "test_losses": 1.820601, "train_accs": 0.983351, "test_accs": 0.983351, "grad_norms": 0.15534, "param_norms": 23.767255}, {"epoch": 2000, "train_losses": 1.745982, "test_losses": 1.745982, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.156947, "param_norms": 24.30616}, {"epoch": 2050, "train_losses": 1.670836, "test_losses": 1.670836, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.158253, "param_norms": 24.843899}, {"epoch": 2100, "train_losses": 1.595367, "test_losses": 1.595367, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.159101, "param_norms": 25.380444}, {"epoch": 2150, "train_losses": 1.519828, "test_losses": 1.519828, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.159523, "param_norms": 25.915804}, {"epoch": 2200, "train_losses": 1.444423, "test_losses": 1.444423, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.15951, "param_norms": 26.449794}, {"epoch": 2250, "train_losses": 1.369354, "test_losses": 1.369354, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.158969, "param_norms": 26.98199}, {"epoch": 2300, "train_losses": 1.29482, "test_losses": 1.29482, "train_accs": 0.986472, "test_accs": 0.986472, "grad_norms": 0.158037, "param_norms": 27.512421}, {"epoch": 2350, "train_losses": 1.221064, "test_losses": 1.221064, "train_accs": 0.988554, "test_accs": 0.988554, "grad_norms": 0.156575, "param_norms": 28.040684}, {"epoch": 2400, "train_losses": 1.148333, "test_losses": 1.148333, "train_accs": 0.990635, "test_accs": 0.990635, "grad_norms": 0.154659, "param_norms": 28.566708}, {"epoch": 2450, "train_losses": 1.076893, "test_losses": 1.076893, "train_accs": 0.991675, "test_accs": 0.991675, "grad_norms": 0.152151, "param_norms": 29.09036}, {"epoch": 2500, "train_losses": 1.00701, "test_losses": 1.00701, "train_accs": 0.991675, "test_accs": 0.991675, "grad_norms": 0.149025, "param_norms": 29.61185}, {"epoch": 2550, "train_losses": 0.938928, "test_losses": 0.938928, "train_accs": 0.992716, "test_accs": 0.992716, "grad_norms": 0.14552, "param_norms": 30.130903}, {"epoch": 2600, "train_losses": 0.872845, "test_losses": 0.872845, "train_accs": 0.994797, "test_accs": 0.994797, "grad_norms": 0.141541, "param_norms": 30.647263}, {"epoch": 2650, "train_losses": 0.808981, "test_losses": 0.808981, "train_accs": 0.994797, "test_accs": 0.994797, "grad_norms": 0.136948, "param_norms": 31.160846}, {"epoch": 2700, "train_losses": 0.747558, "test_losses": 0.747558, "train_accs": 0.995838, "test_accs": 0.995838, "grad_norms": 0.132035, "param_norms": 31.671403}, {"epoch": 2750, "train_losses": 0.688734, "test_losses": 0.688734, "train_accs": 0.995838, "test_accs": 0.995838, "grad_norms": 0.126793, "param_norms": 32.178323}, {"epoch": 2800, "train_losses": 0.632618, "test_losses": 0.632618, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.121173, "param_norms": 32.681304}, {"epoch": 2850, "train_losses": 0.579339, "test_losses": 0.579339, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.115282, "param_norms": 33.180419}, {"epoch": 2900, "train_losses": 0.528942, "test_losses": 0.528942, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.109362, "param_norms": 33.675273}, {"epoch": 2950, "train_losses": 0.481467, "test_losses": 0.481467, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.103242, "param_norms": 34.166096}, {"epoch": 3000, "train_losses": 0.436916, "test_losses": 0.436916, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.097136, "param_norms": 34.652655}, {"epoch": 3050, "train_losses": 0.395254, "test_losses": 0.395254, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.090967, "param_norms": 35.13498}, {"epoch": 3100, "train_losses": 0.356426, "test_losses": 0.356426, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.084982, "param_norms": 35.612927}, {"epoch": 3150, "train_losses": 0.320372, "test_losses": 0.320372, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.079043, "param_norms": 36.086987}, {"epoch": 3200, "train_losses": 0.287025, "test_losses": 0.287025, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.073255, "param_norms": 36.55775}, {"epoch": 3250, "train_losses": 0.25631, "test_losses": 0.25631, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.06759, "param_norms": 37.025741}, {"epoch": 3300, "train_losses": 0.228136, "test_losses": 0.228136, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.062115, "param_norms": 37.491645}, {"epoch": 3350, "train_losses": 0.202416, "test_losses": 0.202416, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.056803, "param_norms": 37.956251}, {"epoch": 3400, "train_losses": 0.179039, "test_losses": 0.179039, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.051718, "param_norms": 38.420511}, {"epoch": 3450, "train_losses": 0.157908, "test_losses": 0.157908, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.046896, "param_norms": 38.884956}, {"epoch": 3500, "train_losses": 0.138891, "test_losses": 0.138891, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.04232, "param_norms": 39.349611}, {"epoch": 3550, "train_losses": 0.121853, "test_losses": 0.121853, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.038034, "param_norms": 39.814663}, {"epoch": 3600, "train_losses": 0.106648, "test_losses": 0.106648, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.034047, "param_norms": 40.280043}, {"epoch": 3650, "train_losses": 0.093129, "test_losses": 0.093129, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.03038, "param_norms": 40.7454}, {"epoch": 3700, "train_losses": 0.081151, "test_losses": 0.081151, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.027007, "param_norms": 41.210332}, {"epoch": 3750, "train_losses": 0.07057, "test_losses": 0.07057, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.023932, "param_norms": 41.674477}, {"epoch": 3800, "train_losses": 0.061248, "test_losses": 0.061248, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.021138, "param_norms": 42.137814}, {"epoch": 3850, "train_losses": 0.053059, "test_losses": 0.053059, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.018636, "param_norms": 42.600305}, {"epoch": 3900, "train_losses": 0.045884, "test_losses": 0.045884, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016367, "param_norms": 43.061898}, {"epoch": 3950, "train_losses": 0.039611, "test_losses": 0.039611, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014338, "param_norms": 43.5224}, {"epoch": 4000, "train_losses": 0.034141, "test_losses": 0.034141, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012537, "param_norms": 43.981901}, {"epoch": 4050, "train_losses": 0.02938, "test_losses": 0.02938, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010953, "param_norms": 44.440273}, {"epoch": 4100, "train_losses": 0.025244, "test_losses": 0.025244, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009535, "param_norms": 44.897564}, {"epoch": 4150, "train_losses": 0.021657, "test_losses": 0.021657, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00829, "param_norms": 45.353697}, {"epoch": 4200, "train_losses": 0.018552, "test_losses": 0.018552, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007189, "param_norms": 45.808558}, {"epoch": 4250, "train_losses": 0.01587, "test_losses": 0.01587, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006226, "param_norms": 46.262309}, {"epoch": 4300, "train_losses": 0.013556, "test_losses": 0.013556, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005388, "param_norms": 46.714893}, {"epoch": 4350, "train_losses": 0.011562, "test_losses": 0.011562, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004654, "param_norms": 47.166412}, {"epoch": 4400, "train_losses": 0.009848, "test_losses": 0.009848, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004006, "param_norms": 47.616849}, {"epoch": 4450, "train_losses": 0.008377, "test_losses": 0.008377, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003442, "param_norms": 48.066108}, {"epoch": 4500, "train_losses": 0.007116, "test_losses": 0.007116, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002958, "param_norms": 48.514257}, {"epoch": 4550, "train_losses": 0.006037, "test_losses": 0.006037, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002538, "param_norms": 48.961083}, {"epoch": 4600, "train_losses": 0.005115, "test_losses": 0.005115, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002173, "param_norms": 49.406797}, {"epoch": 4650, "train_losses": 0.004329, "test_losses": 0.004329, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001858, "param_norms": 49.851575}, {"epoch": 4700, "train_losses": 0.003658, "test_losses": 0.003658, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001586, "param_norms": 50.295266}, {"epoch": 4750, "train_losses": 0.003088, "test_losses": 0.003088, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001354, "param_norms": 50.737867}, {"epoch": 4800, "train_losses": 0.002604, "test_losses": 0.002604, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001153, "param_norms": 51.179478}, {"epoch": 4850, "train_losses": 0.002193, "test_losses": 0.002193, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000981, "param_norms": 51.620022}, {"epoch": 4900, "train_losses": 0.001844, "test_losses": 0.001844, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000832, "param_norms": 52.059458}, {"epoch": 4950, "train_losses": 0.001549, "test_losses": 0.001549, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000705, "param_norms": 52.497843}, {"epoch": 4999, "train_losses": 0.001305, "test_losses": 0.001305, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0006, "param_norms": 52.926193}], "total_epochs": 5000}, "grokking": {"config": {"prime": 31, "d_mlp": 931, "act_type": "ReLU", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 0.0001, "weight_decay": 2.0, "frac_train": 0.75, "num_epochs": 50000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=31, run=grokking\n======================================================================\n\nConfiguration:\n  prime (p)       = 31\n  d_mlp           = 931\n  activation      = ReLU\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 0.0001\n  weight_decay    = 2.0\n  frac_train      = 0.75\n  num_epochs      = 50000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.433796      3.434476      0.0347      0.0415      0.0217       4.3206\n     160      3.359693      3.500791      0.2722      0.0000      0.0328       6.1154\n     320      3.218821      3.572307      0.5458      0.0000      0.0553       9.2510\n     480      3.012760      3.611321      0.7028      0.0000      0.0785      12.4257\n     640      2.750883      3.610343      0.7236      0.0083      0.0999      15.5184\n     800      2.445758      3.565212      0.7319      0.0166      0.1181      18.5032\n     960      2.112228      3.469792      0.7722      0.1079      0.1320      21.3553\n    1120      1.766802      3.312002      0.8458      0.3154      0.1397      24.0447\n    1280      1.427801      3.083353      0.9403      0.5892      0.1404      26.5556\n    1440      1.111195      2.784705      0.9931      0.7427      0.1341      28.8853\n    1600      0.828393      2.434698      1.0000      0.7635      0.1222      31.0509\n    1760      0.587308      2.070415      1.0000      0.7635      0.1057      33.0822\n    1920      0.394491      1.741714      1.0000      0.7635      0.0855      35.0143\n    2080      0.252681      1.483562      1.0000      0.7635      0.0640      36.8698\n    2240      0.156420      1.297863      1.0000      0.7635      0.0448      38.6493\n    2400      0.094781      1.166014      1.0000      0.7635      0.0298      40.3554\n    2560      0.056719      1.067707      1.0000      0.7635      0.0192      41.9878\n    2720      0.033678      0.988995      1.0000      0.7718      0.0121      43.5497\n    2880      0.019910      0.922261      1.0000      0.7801      0.0075      45.0486\n    3040      0.011746      0.861412      1.0000      0.7884      0.0046      46.4926\n    3200      0.006922      0.803099      1.0000      0.7884      0.0028      47.8852\n    3360      0.004082      0.745924      1.0000      0.7967      0.0017      49.2300\n    3520      0.002410      0.689070      1.0000      0.8133      0.0011      50.5294\n    3680      0.001427      0.633991      1.0000      0.8382      0.0006      51.7844\n    3840      0.000849      0.581244      1.0000      0.8382      0.0004      52.9950\n    4000      0.000507      0.531472      1.0000      0.8631      0.0002      54.1590\n    4160      0.000305      0.484340      1.0000      0.9046      0.0002      55.2705\n    4320      0.000186      0.439543      1.0000      0.9129      0.0001      56.3216\n    4480      0.000115      0.397230      1.0000      0.9295      0.0001      57.3020\n    4640      0.000072      0.357695      1.0000      0.9295      0.0000      58.1998\n    4800      0.000047      0.321501      1.0000      0.9295      0.0000      58.9991\n    4960      0.000031      0.288838      1.0000      0.9378      0.0000      59.6846\n    5120      0.000022      0.259378      1.0000      0.9544      0.0000      60.2443\n    5280      0.000016      0.233029      1.0000      0.9627      0.0000      60.6737\n    5440      0.000012      0.209636      1.0000      0.9627      0.0000      60.9776\n    5600      0.000010      0.189324      1.0000      0.9710      0.0000      61.1687\n    5760      0.000009      0.171648      1.0000      0.9793      0.0000      61.2644\n    5920      0.000008      0.156484      1.0000      0.9793      0.0000      61.2849\n    6080      0.000007      0.143366      1.0000      0.9793      0.0000      61.2492\n    6240      0.000007      0.131991      1.0000      0.9793      0.0000      61.1731\n    6400      0.000006      0.122066      1.0000      0.9793      0.0000      61.0693\n    6560      0.000006      0.113249      1.0000      0.9793      0.0000      60.9470\n    6720      0.000006      0.105409      1.0000      0.9793      0.0000      60.8149\n    6880      0.000006      0.098430      1.0000      0.9793      0.0000      60.6764\n    7040      0.000006      0.092103      1.0000      0.9793      0.0000      60.5371\n    7200      0.000006      0.086490      1.0000      0.9793      0.0000      60.3994\n    7360      0.000006      0.081466      1.0000      0.9834      0.0000      60.2638\n    7520      0.000006      0.076916      1.0000      0.9834      0.0000      60.1325\n    7680      0.000006      0.072821      1.0000      0.9876      0.0000      60.0061\n    7840      0.000005      0.069113      1.0000      0.9876      0.0000      59.8845\n    8000      0.000005      0.065706      1.0000      0.9917      0.0000      59.7673\n    8160      0.000005      0.062567      1.0000      0.9917      0.0000      59.6547\n    8320      0.000005      0.059696      1.0000      0.9917      0.0000      59.5468\n    8480      0.000005      0.057074      1.0000      0.9917      0.0000      59.4436\n    8640      0.000005      0.054709      1.0000      0.9917      0.0000      59.3442\n    8800      0.000005      0.052568      1.0000      0.9917      0.0000      59.2491\n    8960      0.000005      0.050616      1.0000      0.9917      0.0000      59.1580\n    9120      0.000005      0.048814      1.0000      0.9959      0.0000      59.0704\n    9280      0.000005      0.047148      1.0000      0.9959      0.0000      58.9863\n    9440      0.000005      0.045596      1.0000      0.9959      0.0000      58.9053\n    9600      0.000005      0.044132      1.0000      0.9959      0.0000      58.8273\n    9760      0.000005      0.042768      1.0000      0.9959      0.0000      58.7530\n    9920      0.000005      0.041482      1.0000      1.0000      0.0000      58.6815\n   10080      0.000005      0.040319      1.0000      1.0000      0.0000      58.6130\n   10240      0.000005      0.039224      1.0000      1.0000      0.0000      58.5472\n   10400      0.000005      0.038240      1.0000      1.0000      0.0000      58.4839\n   10560      0.000005      0.037323      1.0000      1.0000      0.0000      58.4230\n   10720      0.000005      0.036500      1.0000      1.0000      0.0000      58.3638\n   10880      0.000005      0.035764      1.0000      1.0000      0.0000      58.3065\n   11040      0.000005      0.035054      1.0000      1.0000      0.0000      58.2516\n   11200      0.000005      0.034345      1.0000      1.0000      0.0000      58.1985\n   11360      0.000005      0.033665      1.0000      1.0000      0.0000      58.1474\n   11520      0.000005      0.033017      1.0000      1.0000      0.0000      58.0979\n   11680      0.000005      0.032421      1.0000      1.0000      0.0000      58.0500\n   11840      0.000005      0.031865      1.0000      1.0000      0.0000      58.0038\n   12000      0.000005      0.031361      1.0000      1.0000      0.0000      57.9597\n   12160      0.000005      0.030912      1.0000      1.0000      0.0000      57.9169\n   12320      0.000005      0.030486      1.0000      1.0000      0.0000      57.8760\n   12480      0.000005      0.030079      1.0000      1.0000      0.0000      57.8365\n   12640      0.000005      0.029694      1.0000      1.0000      0.0000      57.7982\n   12800      0.000005      0.029370      1.0000      1.0000      0.0000      57.7614\n   12960      0.000005      0.029062      1.0000      1.0000      0.0000      57.7262\n   13120      0.000005      0.028754      1.0000      1.0000      0.0000      57.6923\n   13280      0.000005      0.028471      1.0000      1.0000      0.0000      57.6595\n   13440      0.000005      0.028204      1.0000      1.0000      0.0000      57.6280\n   13600      0.000005      0.027951      1.0000      1.0000      0.0000      57.5975\n   13760      0.000005      0.027714      1.0000      1.0000      0.0000      57.5682\n   13920      0.000005      0.027492      1.0000      1.0000      0.0000      57.5396\n   14080      0.000005      0.027270      1.0000      1.0000      0.0000      57.5123\n   14240      0.000005      0.027068      1.0000      1.0000      0.0000      57.4860\n   14400      0.000005      0.026880      1.0000      1.0000      0.0000      57.4606\n   14560      0.000005      0.026706      1.0000      1.0000      0.0000      57.4363\n   14720      0.000005      0.026547      1.0000      1.0000      0.0000      57.4125\n   14880      0.000005      0.026410      1.0000      1.0000      0.0000      57.3900\n   15040      0.000005      0.026292      1.0000      1.0000      0.0000      57.3687\n   15200      0.000005      0.026169      1.0000      1.0000      0.0000      57.3484\n   15360      0.000005      0.026044      1.0000      1.0000      0.0000      57.3286\n   15520      0.000005      0.025927      1.0000      1.0000      0.0000      57.3093\n   15680      0.000005      0.025806      1.0000      1.0000      0.0000      57.2910\n   15840      0.000005      0.025702      1.0000      1.0000      0.0000      57.2734\n   16000      0.000005      0.025626      1.0000      1.0000      0.0000      57.2563\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.000005\n  Test Loss   = 0.025626\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 57.2563\n\nTotal epochs trained: 16001\n", "table": [{"epoch": 0, "train_losses": 3.433796, "test_losses": 3.434476, "train_accs": 0.034722, "test_accs": 0.041494, "grad_norms": 0.021734, "param_norms": 4.320604}, {"epoch": 160, "train_losses": 3.359693, "test_losses": 3.500791, "train_accs": 0.272222, "test_accs": 0.0, "grad_norms": 0.032828, "param_norms": 6.11541}, {"epoch": 320, "train_losses": 3.218821, "test_losses": 3.572307, "train_accs": 0.545833, "test_accs": 0.0, "grad_norms": 0.055327, "param_norms": 9.25099}, {"epoch": 480, "train_losses": 3.01276, "test_losses": 3.611321, "train_accs": 0.702778, "test_accs": 0.0, "grad_norms": 0.078507, "param_norms": 12.425744}, {"epoch": 640, "train_losses": 2.750883, "test_losses": 3.610343, "train_accs": 0.723611, "test_accs": 0.008299, "grad_norms": 0.099917, "param_norms": 15.518415}, {"epoch": 800, "train_losses": 2.445758, "test_losses": 3.565212, "train_accs": 0.731944, "test_accs": 0.016598, "grad_norms": 0.118116, "param_norms": 18.503197}, {"epoch": 960, "train_losses": 2.112228, "test_losses": 3.469792, "train_accs": 0.772222, "test_accs": 0.107884, "grad_norms": 0.131998, "param_norms": 21.355289}, {"epoch": 1120, "train_losses": 1.766802, "test_losses": 3.312002, "train_accs": 0.845833, "test_accs": 0.315353, "grad_norms": 0.139742, "param_norms": 24.044698}, {"epoch": 1280, "train_losses": 1.427801, "test_losses": 3.083353, "train_accs": 0.940278, "test_accs": 0.589212, "grad_norms": 0.140423, "param_norms": 26.555597}, {"epoch": 1440, "train_losses": 1.111195, "test_losses": 2.784705, "train_accs": 0.993056, "test_accs": 0.742739, "grad_norms": 0.134079, "param_norms": 28.885275}, {"epoch": 1600, "train_losses": 0.828393, "test_losses": 2.434698, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.122154, "param_norms": 31.050878}, {"epoch": 1760, "train_losses": 0.587308, "test_losses": 2.070415, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.105654, "param_norms": 33.082213}, {"epoch": 1920, "train_losses": 0.394491, "test_losses": 1.741714, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.085478, "param_norms": 35.014293}, {"epoch": 2080, "train_losses": 0.252681, "test_losses": 1.483562, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.064044, "param_norms": 36.869793}, {"epoch": 2240, "train_losses": 0.15642, "test_losses": 1.297863, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.044774, "param_norms": 38.64934}, {"epoch": 2400, "train_losses": 0.094781, "test_losses": 1.166014, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.029778, "param_norms": 40.355413}, {"epoch": 2560, "train_losses": 0.056719, "test_losses": 1.067707, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.019182, "param_norms": 41.987839}, {"epoch": 2720, "train_losses": 0.033678, "test_losses": 0.988995, "train_accs": 1.0, "test_accs": 0.771784, "grad_norms": 0.012099, "param_norms": 43.549695}, {"epoch": 2880, "train_losses": 0.01991, "test_losses": 0.922261, "train_accs": 1.0, "test_accs": 0.780083, "grad_norms": 0.007514, "param_norms": 45.0486}, {"epoch": 3040, "train_losses": 0.011746, "test_losses": 0.861412, "train_accs": 1.0, "test_accs": 0.788382, "grad_norms": 0.004634, "param_norms": 46.492592}, {"epoch": 3200, "train_losses": 0.006922, "test_losses": 0.803099, "train_accs": 1.0, "test_accs": 0.788382, "grad_norms": 0.002844, "param_norms": 47.885233}, {"epoch": 3360, "train_losses": 0.004082, "test_losses": 0.745924, "train_accs": 1.0, "test_accs": 0.79668, "grad_norms": 0.001737, "param_norms": 49.23001}, {"epoch": 3520, "train_losses": 0.00241, "test_losses": 0.68907, "train_accs": 1.0, "test_accs": 0.813278, "grad_norms": 0.00106, "param_norms": 50.529434}, {"epoch": 3680, "train_losses": 0.001427, "test_losses": 0.633991, "train_accs": 1.0, "test_accs": 0.838174, "grad_norms": 0.000648, "param_norms": 51.784414}, {"epoch": 3840, "train_losses": 0.000849, "test_losses": 0.581244, "train_accs": 1.0, "test_accs": 0.838174, "grad_norms": 0.000396, "param_norms": 52.995014}, {"epoch": 4000, "train_losses": 0.000507, "test_losses": 0.531472, "train_accs": 1.0, "test_accs": 0.863071, "grad_norms": 0.000243, "param_norms": 54.15903}, {"epoch": 4160, "train_losses": 0.000305, "test_losses": 0.48434, "train_accs": 1.0, "test_accs": 0.904564, "grad_norms": 0.00015, "param_norms": 55.270453}, {"epoch": 4320, "train_losses": 0.000186, "test_losses": 0.439543, "train_accs": 1.0, "test_accs": 0.912863, "grad_norms": 9.4e-05, "param_norms": 56.321627}, {"epoch": 4480, "train_losses": 0.000115, "test_losses": 0.39723, "train_accs": 1.0, "test_accs": 0.929461, "grad_norms": 5.9e-05, "param_norms": 57.302012}, {"epoch": 4640, "train_losses": 7.2e-05, "test_losses": 0.357695, "train_accs": 1.0, "test_accs": 0.929461, "grad_norms": 3.8e-05, "param_norms": 58.199848}, {"epoch": 4800, "train_losses": 4.7e-05, "test_losses": 0.321501, "train_accs": 1.0, "test_accs": 0.929461, "grad_norms": 2.5e-05, "param_norms": 58.999085}, {"epoch": 4960, "train_losses": 3.1e-05, "test_losses": 0.288838, "train_accs": 1.0, "test_accs": 0.937759, "grad_norms": 1.7e-05, "param_norms": 59.684594}, {"epoch": 5120, "train_losses": 2.2e-05, "test_losses": 0.259378, "train_accs": 1.0, "test_accs": 0.954357, "grad_norms": 1.2e-05, "param_norms": 60.244335}, {"epoch": 5280, "train_losses": 1.6e-05, "test_losses": 0.233029, "train_accs": 1.0, "test_accs": 0.962656, "grad_norms": 9e-06, "param_norms": 60.673715}, {"epoch": 5440, "train_losses": 1.2e-05, "test_losses": 0.209636, "train_accs": 1.0, "test_accs": 0.962656, "grad_norms": 7e-06, "param_norms": 60.977591}, {"epoch": 5600, "train_losses": 1e-05, "test_losses": 0.189324, "train_accs": 1.0, "test_accs": 0.970954, "grad_norms": 6e-06, "param_norms": 61.168747}, {"epoch": 5760, "train_losses": 9e-06, "test_losses": 0.171648, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 5e-06, "param_norms": 61.264434}, {"epoch": 5920, "train_losses": 8e-06, "test_losses": 0.156484, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 4e-06, "param_norms": 61.284897}, {"epoch": 6080, "train_losses": 7e-06, "test_losses": 0.143366, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 4e-06, "param_norms": 61.249249}, {"epoch": 6240, "train_losses": 7e-06, "test_losses": 0.131991, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 4e-06, "param_norms": 61.173149}, {"epoch": 6400, "train_losses": 6e-06, "test_losses": 0.122066, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 4e-06, "param_norms": 61.069294}, {"epoch": 6560, "train_losses": 6e-06, "test_losses": 0.113249, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 3e-06, "param_norms": 60.947022}, {"epoch": 6720, "train_losses": 6e-06, "test_losses": 0.105409, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 3e-06, "param_norms": 60.814913}, {"epoch": 6880, "train_losses": 6e-06, "test_losses": 0.09843, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 3e-06, "param_norms": 60.676395}, {"epoch": 7040, "train_losses": 6e-06, "test_losses": 0.092103, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 3e-06, "param_norms": 60.537098}, {"epoch": 7200, "train_losses": 6e-06, "test_losses": 0.08649, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 3e-06, "param_norms": 60.399366}, {"epoch": 7360, "train_losses": 6e-06, "test_losses": 0.081466, "train_accs": 1.0, "test_accs": 0.983402, "grad_norms": 3e-06, "param_norms": 60.26378}, {"epoch": 7520, "train_losses": 6e-06, "test_losses": 0.076916, "train_accs": 1.0, "test_accs": 0.983402, "grad_norms": 3e-06, "param_norms": 60.132504}, {"epoch": 7680, "train_losses": 6e-06, "test_losses": 0.072821, "train_accs": 1.0, "test_accs": 0.987552, "grad_norms": 3e-06, "param_norms": 60.006086}, {"epoch": 7840, "train_losses": 5e-06, "test_losses": 0.069113, "train_accs": 1.0, "test_accs": 0.987552, "grad_norms": 3e-06, "param_norms": 59.884452}, {"epoch": 8000, "train_losses": 5e-06, "test_losses": 0.065706, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.767275}, {"epoch": 8160, "train_losses": 5e-06, "test_losses": 0.062567, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.654717}, {"epoch": 8320, "train_losses": 5e-06, "test_losses": 0.059696, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.546846}, {"epoch": 8480, "train_losses": 5e-06, "test_losses": 0.057074, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.443583}, {"epoch": 8640, "train_losses": 5e-06, "test_losses": 0.054709, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.344189}, {"epoch": 8800, "train_losses": 5e-06, "test_losses": 0.052568, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.249091}, {"epoch": 8960, "train_losses": 5e-06, "test_losses": 0.050616, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.158045}, {"epoch": 9120, "train_losses": 5e-06, "test_losses": 0.048814, "train_accs": 1.0, "test_accs": 0.995851, "grad_norms": 3e-06, "param_norms": 59.070374}, {"epoch": 9280, "train_losses": 5e-06, "test_losses": 0.047148, "train_accs": 1.0, "test_accs": 0.995851, "grad_norms": 3e-06, "param_norms": 58.986265}, {"epoch": 9440, "train_losses": 5e-06, "test_losses": 0.045596, "train_accs": 1.0, "test_accs": 0.995851, "grad_norms": 3e-06, "param_norms": 58.905258}, {"epoch": 9600, "train_losses": 5e-06, "test_losses": 0.044132, "train_accs": 1.0, "test_accs": 0.995851, "grad_norms": 3e-06, "param_norms": 58.827261}, {"epoch": 9760, "train_losses": 5e-06, "test_losses": 0.042768, "train_accs": 1.0, "test_accs": 0.995851, "grad_norms": 3e-06, "param_norms": 58.753029}, {"epoch": 9920, "train_losses": 5e-06, "test_losses": 0.041482, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.681505}, {"epoch": 10080, "train_losses": 5e-06, "test_losses": 0.040319, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.612996}, {"epoch": 10240, "train_losses": 5e-06, "test_losses": 0.039224, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.54716}, {"epoch": 10400, "train_losses": 5e-06, "test_losses": 0.03824, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.483947}, {"epoch": 10560, "train_losses": 5e-06, "test_losses": 0.037323, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.423}, {"epoch": 10720, "train_losses": 5e-06, "test_losses": 0.0365, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.36379}, {"epoch": 10880, "train_losses": 5e-06, "test_losses": 0.035764, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.306498}, {"epoch": 11040, "train_losses": 5e-06, "test_losses": 0.035054, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.251603}, {"epoch": 11200, "train_losses": 5e-06, "test_losses": 0.034345, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.198483}, {"epoch": 11360, "train_losses": 5e-06, "test_losses": 0.033665, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.147399}, {"epoch": 11520, "train_losses": 5e-06, "test_losses": 0.033017, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.097886}, {"epoch": 11680, "train_losses": 5e-06, "test_losses": 0.032421, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.04999}, {"epoch": 11840, "train_losses": 5e-06, "test_losses": 0.031865, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.003801}, {"epoch": 12000, "train_losses": 5e-06, "test_losses": 0.031361, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.959658}, {"epoch": 12160, "train_losses": 5e-06, "test_losses": 0.030912, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.916896}, {"epoch": 12320, "train_losses": 5e-06, "test_losses": 0.030486, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.875984}, {"epoch": 12480, "train_losses": 5e-06, "test_losses": 0.030079, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.836528}, {"epoch": 12640, "train_losses": 5e-06, "test_losses": 0.029694, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.798203}, {"epoch": 12800, "train_losses": 5e-06, "test_losses": 0.02937, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.76137}, {"epoch": 12960, "train_losses": 5e-06, "test_losses": 0.029062, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.726206}, {"epoch": 13120, "train_losses": 5e-06, "test_losses": 0.028754, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.692328}, {"epoch": 13280, "train_losses": 5e-06, "test_losses": 0.028471, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.659509}, {"epoch": 13440, "train_losses": 5e-06, "test_losses": 0.028204, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.627987}, {"epoch": 13600, "train_losses": 5e-06, "test_losses": 0.027951, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.59753}, {"epoch": 13760, "train_losses": 5e-06, "test_losses": 0.027714, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.568181}, {"epoch": 13920, "train_losses": 5e-06, "test_losses": 0.027492, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.539588}, {"epoch": 14080, "train_losses": 5e-06, "test_losses": 0.02727, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.512271}, {"epoch": 14240, "train_losses": 5e-06, "test_losses": 0.027068, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.485976}, {"epoch": 14400, "train_losses": 5e-06, "test_losses": 0.02688, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.460641}, {"epoch": 14560, "train_losses": 5e-06, "test_losses": 0.026706, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.436292}, {"epoch": 14720, "train_losses": 5e-06, "test_losses": 0.026547, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.412488}, {"epoch": 14880, "train_losses": 5e-06, "test_losses": 0.02641, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.389999}, {"epoch": 15040, "train_losses": 5e-06, "test_losses": 0.026292, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.368741}, {"epoch": 15200, "train_losses": 5e-06, "test_losses": 0.026169, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.348428}, {"epoch": 15360, "train_losses": 5e-06, "test_losses": 0.026044, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.328604}, {"epoch": 15520, "train_losses": 5e-06, "test_losses": 0.025927, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.309324}, {"epoch": 15680, "train_losses": 5e-06, "test_losses": 0.025806, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.290981}, {"epoch": 15840, "train_losses": 5e-06, "test_losses": 0.025702, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.27344}, {"epoch": 16000, "train_losses": 5e-06, "test_losses": 0.025626, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.256305}], "total_epochs": 16001}, "quad_random": {"config": {"prime": 31, "d_mlp": 931, "act_type": "Quad", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 5e-05, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=31, run=quad_random\n======================================================================\n\nConfiguration:\n  prime (p)       = 31\n  d_mlp           = 931\n  activation      = Quad\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 5e-05\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.433976      3.433976      0.0281      0.0281      0.0015       4.3206\n      50      3.433318      3.433318      0.1426      0.1426      0.0016       4.3996\n     100      3.432411      3.432411      0.4422      0.4422      0.0020       4.6612\n     150      3.431169      3.431169      0.7336      0.7336      0.0026       5.0426\n     200      3.429463      3.429463      0.8970      0.8970      0.0035       5.4946\n     250      3.427181      3.427181      0.9719      0.9719      0.0046       5.9861\n     300      3.424223      3.424223      0.9813      0.9813      0.0059       6.5006\n     350      3.420488      3.420488      0.9927      0.9927      0.0073       7.0288\n     400      3.415882      3.415882      0.9990      0.9990      0.0090       7.5657\n     450      3.410305      3.410305      0.9990      0.9990      0.0109       8.1086\n     500      3.403662      3.403662      0.9990      0.9990      0.0130       8.6560\n     550      3.395854      3.395854      0.9990      0.9990      0.0153       9.2074\n     600      3.386791      3.386791      0.9990      0.9990      0.0178       9.7628\n     650      3.376379      3.376379      0.9979      0.9979      0.0204      10.3217\n     700      3.364527      3.364527      0.9979      0.9979      0.0233      10.8830\n     750      3.351143      3.351143      0.9979      0.9979      0.0263      11.4464\n     800      3.336136      3.336136      0.9990      0.9990      0.0295      12.0120\n     850      3.319419      3.319419      0.9990      0.9990      0.0329      12.5793\n     900      3.300901      3.300901      0.9990      0.9990      0.0364      13.1485\n     950      3.280498      3.280498      0.9990      0.9990      0.0402      13.7193\n    1000      3.258123      3.258123      0.9990      0.9990      0.0441      14.2916\n    1050      3.233696      3.233696      0.9990      0.9990      0.0482      14.8654\n    1100      3.207133      3.207133      0.9990      0.9990      0.0524      15.4407\n    1150      3.178358      3.178358      0.9990      0.9990      0.0568      16.0173\n    1200      3.147292      3.147292      0.9990      0.9990      0.0614      16.5951\n    1250      3.113861      3.113861      1.0000      1.0000      0.0661      17.1740\n    1300      3.077994      3.077994      1.0000      1.0000      0.0710      17.7539\n    1350      3.039621      3.039621      1.0000      1.0000      0.0760      18.3347\n    1400      2.998676      2.998676      1.0000      1.0000      0.0811      18.9162\n    1450      2.955099      2.955099      1.0000      1.0000      0.0864      19.4985\n    1500      2.908832      2.908832      1.0000      1.0000      0.0918      20.0814\n    1550      2.859824      2.859824      1.0000      1.0000      0.0973      20.6648\n    1600      2.808027      2.808027      1.0000      1.0000      0.1029      21.2484\n    1650      2.753402      2.753402      1.0000      1.0000      0.1086      21.8323\n    1700      2.695916      2.695916      1.0000      1.0000      0.1144      22.4163\n    1750      2.635547      2.635547      1.0000      1.0000      0.1202      23.0003\n    1800      2.572280      2.572280      1.0000      1.0000      0.1260      23.5843\n    1850      2.506116      2.506116      1.0000      1.0000      0.1319      24.1682\n    1900      2.437068      2.437068      1.0000      1.0000      0.1377      24.7522\n    1950      2.365163      2.365163      1.0000      1.0000      0.1436      25.3361\n    2000      2.290450      2.290450      1.0000      1.0000      0.1493      25.9199\n    2050      2.212999      2.212999      1.0000      1.0000      0.1549      26.5034\n    2100      2.132903      2.132903      1.0000      1.0000      0.1603      27.0865\n    2150      2.050286      2.050286      1.0000      1.0000      0.1655      27.6691\n    2200      1.965298      1.965298      1.0000      1.0000      0.1705      28.2511\n    2250      1.878130      1.878130      1.0000      1.0000      0.1750      28.8322\n    2300      1.789011      1.789011      1.0000      1.0000      0.1792      29.4124\n    2350      1.698212      1.698212      1.0000      1.0000      0.1828      29.9913\n    2400      1.606053      1.606053      1.0000      1.0000      0.1859      30.5688\n    2450      1.512903      1.512903      1.0000      1.0000      0.1882      31.1443\n    2500      1.419181      1.419181      1.0000      1.0000      0.1898      31.7176\n    2550      1.325357      1.325357      1.0000      1.0000      0.1904      32.2887\n    2600      1.231949      1.231949      1.0000      1.0000      0.1901      32.8570\n    2650      1.139515      1.139515      1.0000      1.0000      0.1887      33.4223\n    2700      1.048646      1.048646      1.0000      1.0000      0.1862      33.9840\n    2750      0.959943      0.959943      1.0000      1.0000      0.1826      34.5413\n    2800      0.874004      0.874004      1.0000      1.0000      0.1778      35.0935\n    2850      0.791402      0.791402      1.0000      1.0000      0.1718      35.6396\n    2900      0.712658      0.712658      1.0000      1.0000      0.1649      36.1790\n    2950      0.638223      0.638223      1.0000      1.0000      0.1570      36.7108\n    3000      0.568452      0.568452      1.0000      1.0000      0.1484      37.2342\n    3050      0.503593      0.503593      1.0000      1.0000      0.1392      37.7482\n    3100      0.443774      0.443774      1.0000      1.0000      0.1297      38.2519\n    3150      0.389009      0.389009      1.0000      1.0000      0.1199      38.7446\n    3200      0.339215      0.339215      1.0000      1.0000      0.1102      39.2263\n    3250      0.294224      0.294224      1.0000      1.0000      0.1007      39.6968\n    3300      0.253817      0.253817      1.0000      1.0000      0.0913      40.1565\n    3350      0.217739      0.217739      1.0000      1.0000      0.0823      40.6063\n    3400      0.185732      0.185732      1.0000      1.0000      0.0736      41.0479\n    3450      0.157538      0.157538      1.0000      1.0000      0.0653      41.4838\n    3500      0.132898      0.132898      1.0000      1.0000      0.0574      41.9167\n    3550      0.111536      0.111536      1.0000      1.0000      0.0501      42.3493\n    3600      0.093163      0.093163      1.0000      1.0000      0.0433      42.7837\n    3650      0.077472      0.077472      1.0000      1.0000      0.0372      43.2208\n    3700      0.064157      0.064157      1.0000      1.0000      0.0317      43.6602\n    3750      0.052923      0.052923      1.0000      1.0000      0.0269      44.1012\n    3800      0.043494      0.043494      1.0000      1.0000      0.0227      44.5429\n    3850      0.035619      0.035619      1.0000      1.0000      0.0191      44.9848\n    3900      0.029071      0.029071      1.0000      1.0000      0.0159      45.4264\n    3950      0.023649      0.023649      1.0000      1.0000      0.0132      45.8675\n    4000      0.019178      0.019178      1.0000      1.0000      0.0110      46.3078\n    4050      0.015505      0.015505      1.0000      1.0000      0.0090      46.7472\n    4100      0.012498      0.012498      1.0000      1.0000      0.0074      47.1856\n    4150      0.010045      0.010045      1.0000      1.0000      0.0061      47.6228\n    4200      0.008050      0.008050      1.0000      1.0000      0.0050      48.0587\n    4250      0.006434      0.006434      1.0000      1.0000      0.0040      48.4933\n    4300      0.005129      0.005129      1.0000      1.0000      0.0033      48.9264\n    4350      0.004077      0.004077      1.0000      1.0000      0.0026      49.3581\n    4400      0.003233      0.003233      1.0000      1.0000      0.0021      49.7882\n    4450      0.002557      0.002557      1.0000      1.0000      0.0017      50.2167\n    4500      0.002017      0.002017      1.0000      1.0000      0.0014      50.6436\n    4550      0.001588      0.001588      1.0000      1.0000      0.0011      51.0689\n    4600      0.001247      0.001247      1.0000      1.0000      0.0009      51.4923\n    4650      0.000977      0.000977      1.0000      1.0000      0.0007      51.9140\n    4700      0.000763      0.000763      1.0000      1.0000      0.0006      52.3337\n    4750      0.000595      0.000595      1.0000      1.0000      0.0004      52.7514\n    4800      0.000463      0.000463      1.0000      1.0000      0.0003      53.1669\n    4850      0.000360      0.000360      1.0000      1.0000      0.0003      53.5801\n    4900      0.000279      0.000279      1.0000      1.0000      0.0002      53.9907\n    4950      0.000216      0.000216      1.0000      1.0000      0.0002      54.3985\n    4999      0.000168      0.000168      1.0000      1.0000      0.0001      54.7951\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.000168\n  Test Loss   = 0.000168\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 54.7951\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.433976, "test_losses": 3.433976, "train_accs": 0.028096, "test_accs": 0.028096, "grad_norms": 0.001537, "param_norms": 4.320604}, {"epoch": 50, "train_losses": 3.433318, "test_losses": 3.433318, "train_accs": 0.14256, "test_accs": 0.14256, "grad_norms": 0.001644, "param_norms": 4.39965}, {"epoch": 100, "train_losses": 3.432411, "test_losses": 3.432411, "train_accs": 0.442248, "test_accs": 0.442248, "grad_norms": 0.002019, "param_norms": 4.661192}, {"epoch": 150, "train_losses": 3.431169, "test_losses": 3.431169, "train_accs": 0.733611, "test_accs": 0.733611, "grad_norms": 0.002639, "param_norms": 5.042629}, {"epoch": 200, "train_losses": 3.429463, "test_losses": 3.429463, "train_accs": 0.896982, "test_accs": 0.896982, "grad_norms": 0.003491, "param_norms": 5.494605}, {"epoch": 250, "train_losses": 3.427181, "test_losses": 3.427181, "train_accs": 0.971904, "test_accs": 0.971904, "grad_norms": 0.004565, "param_norms": 5.986137}, {"epoch": 300, "train_losses": 3.424223, "test_losses": 3.424223, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.005851, "param_norms": 6.500601}, {"epoch": 350, "train_losses": 3.420488, "test_losses": 3.420488, "train_accs": 0.992716, "test_accs": 0.992716, "grad_norms": 0.007343, "param_norms": 7.028817}, {"epoch": 400, "train_losses": 3.415882, "test_losses": 3.415882, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.009036, "param_norms": 7.565736}, {"epoch": 450, "train_losses": 3.410305, "test_losses": 3.410305, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.010927, "param_norms": 8.108597}, {"epoch": 500, "train_losses": 3.403662, "test_losses": 3.403662, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.013014, "param_norms": 8.655955}, {"epoch": 550, "train_losses": 3.395854, "test_losses": 3.395854, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.015294, "param_norms": 9.207363}, {"epoch": 600, "train_losses": 3.386791, "test_losses": 3.386791, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.017763, "param_norms": 9.762824}, {"epoch": 650, "train_losses": 3.376379, "test_losses": 3.376379, "train_accs": 0.997919, "test_accs": 0.997919, "grad_norms": 0.02042, "param_norms": 10.321682}, {"epoch": 700, "train_losses": 3.364527, "test_losses": 3.364527, "train_accs": 0.997919, "test_accs": 0.997919, "grad_norms": 0.023261, "param_norms": 10.882997}, {"epoch": 750, "train_losses": 3.351143, "test_losses": 3.351143, "train_accs": 0.997919, "test_accs": 0.997919, "grad_norms": 0.026286, "param_norms": 11.446406}, {"epoch": 800, "train_losses": 3.336136, "test_losses": 3.336136, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.029493, "param_norms": 12.01196}, {"epoch": 850, "train_losses": 3.319419, "test_losses": 3.319419, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.032879, "param_norms": 12.579335}, {"epoch": 900, "train_losses": 3.300901, "test_losses": 3.300901, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.036443, "param_norms": 13.148459}, {"epoch": 950, "train_losses": 3.280498, "test_losses": 3.280498, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.040181, "param_norms": 13.719296}, {"epoch": 1000, "train_losses": 3.258123, "test_losses": 3.258123, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.044091, "param_norms": 14.291623}, {"epoch": 1050, "train_losses": 3.233696, "test_losses": 3.233696, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.04817, "param_norms": 14.865403}, {"epoch": 1100, "train_losses": 3.207133, "test_losses": 3.207133, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.052414, "param_norms": 15.440701}, {"epoch": 1150, "train_losses": 3.178358, "test_losses": 3.178358, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.056821, "param_norms": 16.017318}, {"epoch": 1200, "train_losses": 3.147292, "test_losses": 3.147292, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.061385, "param_norms": 16.595146}, {"epoch": 1250, "train_losses": 3.113861, "test_losses": 3.113861, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.066103, "param_norms": 17.174026}, {"epoch": 1300, "train_losses": 3.077994, "test_losses": 3.077994, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.070969, "param_norms": 17.753899}, {"epoch": 1350, "train_losses": 3.039621, "test_losses": 3.039621, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.075978, "param_norms": 18.334704}, {"epoch": 1400, "train_losses": 2.998676, "test_losses": 2.998676, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.081124, "param_norms": 18.916236}, {"epoch": 1450, "train_losses": 2.955099, "test_losses": 2.955099, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.086398, "param_norms": 19.498484}, {"epoch": 1500, "train_losses": 2.908832, "test_losses": 2.908832, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.091792, "param_norms": 20.08139}, {"epoch": 1550, "train_losses": 2.859824, "test_losses": 2.859824, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.097297, "param_norms": 20.664789}, {"epoch": 1600, "train_losses": 2.808027, "test_losses": 2.808027, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.102902, "param_norms": 21.24844}, {"epoch": 1650, "train_losses": 2.753402, "test_losses": 2.753402, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.108593, "param_norms": 21.832322}, {"epoch": 1700, "train_losses": 2.695916, "test_losses": 2.695916, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.114357, "param_norms": 22.416285}, {"epoch": 1750, "train_losses": 2.635547, "test_losses": 2.635547, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.120175, "param_norms": 23.000253}, {"epoch": 1800, "train_losses": 2.57228, "test_losses": 2.57228, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.126028, "param_norms": 23.584251}, {"epoch": 1850, "train_losses": 2.506116, "test_losses": 2.506116, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.131894, "param_norms": 24.16824}, {"epoch": 1900, "train_losses": 2.437068, "test_losses": 2.437068, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.137745, "param_norms": 24.752185}, {"epoch": 1950, "train_losses": 2.365163, "test_losses": 2.365163, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.143551, "param_norms": 25.336133}, {"epoch": 2000, "train_losses": 2.29045, "test_losses": 2.29045, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.149276, "param_norms": 25.919927}, {"epoch": 2050, "train_losses": 2.212999, "test_losses": 2.212999, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.15488, "param_norms": 26.50345}, {"epoch": 2100, "train_losses": 2.132903, "test_losses": 2.132903, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.160316, "param_norms": 27.086543}, {"epoch": 2150, "train_losses": 2.050286, "test_losses": 2.050286, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.165529, "param_norms": 27.669144}, {"epoch": 2200, "train_losses": 1.965298, "test_losses": 1.965298, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.17046, "param_norms": 28.251096}, {"epoch": 2250, "train_losses": 1.87813, "test_losses": 1.87813, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.17504, "param_norms": 28.832217}, {"epoch": 2300, "train_losses": 1.789011, "test_losses": 1.789011, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.179193, "param_norms": 29.412361}, {"epoch": 2350, "train_losses": 1.698212, "test_losses": 1.698212, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.182835, "param_norms": 29.991299}, {"epoch": 2400, "train_losses": 1.606053, "test_losses": 1.606053, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.185878, "param_norms": 30.568755}, {"epoch": 2450, "train_losses": 1.512903, "test_losses": 1.512903, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.188224, "param_norms": 31.144278}, {"epoch": 2500, "train_losses": 1.419181, "test_losses": 1.419181, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.189777, "param_norms": 31.717649}, {"epoch": 2550, "train_losses": 1.325357, "test_losses": 1.325357, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.190438, "param_norms": 32.288661}, {"epoch": 2600, "train_losses": 1.231949, "test_losses": 1.231949, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.190118, "param_norms": 32.857029}, {"epoch": 2650, "train_losses": 1.139515, "test_losses": 1.139515, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.188736, "param_norms": 33.422348}, {"epoch": 2700, "train_losses": 1.048646, "test_losses": 1.048646, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.186233, "param_norms": 33.98403}, {"epoch": 2750, "train_losses": 0.959943, "test_losses": 0.959943, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.182575, "param_norms": 34.541291}, {"epoch": 2800, "train_losses": 0.874004, "test_losses": 0.874004, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.177762, "param_norms": 35.093452}, {"epoch": 2850, "train_losses": 0.791402, "test_losses": 0.791402, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.171835, "param_norms": 35.639644}, {"epoch": 2900, "train_losses": 0.712658, "test_losses": 0.712658, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.164875, "param_norms": 36.179001}, {"epoch": 2950, "train_losses": 0.638223, "test_losses": 0.638223, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.157006, "param_norms": 36.71078}, {"epoch": 3000, "train_losses": 0.568452, "test_losses": 0.568452, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.148387, "param_norms": 37.234186}, {"epoch": 3050, "train_losses": 0.503593, "test_losses": 0.503593, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.139205, "param_norms": 37.748213}, {"epoch": 3100, "train_losses": 0.443774, "test_losses": 0.443774, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.129658, "param_norms": 38.251883}, {"epoch": 3150, "train_losses": 0.389009, "test_losses": 0.389009, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.119941, "param_norms": 38.744643}, {"epoch": 3200, "train_losses": 0.339215, "test_losses": 0.339215, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.110226, "param_norms": 39.22627}, {"epoch": 3250, "train_losses": 0.294224, "test_losses": 0.294224, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.100653, "param_norms": 39.696761}, {"epoch": 3300, "train_losses": 0.253817, "test_losses": 0.253817, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.091322, "param_norms": 40.156481}, {"epoch": 3350, "train_losses": 0.217739, "test_losses": 0.217739, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.082294, "param_norms": 40.60628}, {"epoch": 3400, "train_losses": 0.185732, "test_losses": 0.185732, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.073607, "param_norms": 41.047917}, {"epoch": 3450, "train_losses": 0.157538, "test_losses": 0.157538, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0653, "param_norms": 41.483811}, {"epoch": 3500, "train_losses": 0.132898, "test_losses": 0.132898, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.057431, "param_norms": 41.916694}, {"epoch": 3550, "train_losses": 0.111536, "test_losses": 0.111536, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.050077, "param_norms": 42.349312}, {"epoch": 3600, "train_losses": 0.093163, "test_losses": 0.093163, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.043314, "param_norms": 42.783689}, {"epoch": 3650, "train_losses": 0.077472, "test_losses": 0.077472, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.037195, "param_norms": 43.220777}, {"epoch": 3700, "train_losses": 0.064157, "test_losses": 0.064157, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.031734, "param_norms": 43.660225}, {"epoch": 3750, "train_losses": 0.052923, "test_losses": 0.052923, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.026916, "param_norms": 44.101204}, {"epoch": 3800, "train_losses": 0.043494, "test_losses": 0.043494, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.022706, "param_norms": 44.542922}, {"epoch": 3850, "train_losses": 0.035619, "test_losses": 0.035619, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.019057, "param_norms": 44.984802}, {"epoch": 3900, "train_losses": 0.029071, "test_losses": 0.029071, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01592, "param_norms": 45.426439}, {"epoch": 3950, "train_losses": 0.023649, "test_losses": 0.023649, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01324, "param_norms": 45.8675}, {"epoch": 4000, "train_losses": 0.019178, "test_losses": 0.019178, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010966, "param_norms": 46.307815}, {"epoch": 4050, "train_losses": 0.015505, "test_losses": 0.015505, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009046, "param_norms": 46.747248}, {"epoch": 4100, "train_losses": 0.012498, "test_losses": 0.012498, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007435, "param_norms": 47.1856}, {"epoch": 4150, "train_losses": 0.010045, "test_losses": 0.010045, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006088, "param_norms": 47.622774}, {"epoch": 4200, "train_losses": 0.00805, "test_losses": 0.00805, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004969, "param_norms": 48.058708}, {"epoch": 4250, "train_losses": 0.006434, "test_losses": 0.006434, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004041, "param_norms": 48.493282}, {"epoch": 4300, "train_losses": 0.005129, "test_losses": 0.005129, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003277, "param_norms": 48.926445}, {"epoch": 4350, "train_losses": 0.004077, "test_losses": 0.004077, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002648, "param_norms": 49.358095}, {"epoch": 4400, "train_losses": 0.003233, "test_losses": 0.003233, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002134, "param_norms": 49.788248}, {"epoch": 4450, "train_losses": 0.002557, "test_losses": 0.002557, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001715, "param_norms": 50.21675}, {"epoch": 4500, "train_losses": 0.002017, "test_losses": 0.002017, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001374, "param_norms": 50.643646}, {"epoch": 4550, "train_losses": 0.001588, "test_losses": 0.001588, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001097, "param_norms": 51.068871}, {"epoch": 4600, "train_losses": 0.001247, "test_losses": 0.001247, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000874, "param_norms": 51.492309}, {"epoch": 4650, "train_losses": 0.000977, "test_losses": 0.000977, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000695, "param_norms": 51.913958}, {"epoch": 4700, "train_losses": 0.000763, "test_losses": 0.000763, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000551, "param_norms": 52.333676}, {"epoch": 4750, "train_losses": 0.000595, "test_losses": 0.000595, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000435, "param_norms": 52.751372}, {"epoch": 4800, "train_losses": 0.000463, "test_losses": 0.000463, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000343, "param_norms": 53.166887}, {"epoch": 4850, "train_losses": 0.00036, "test_losses": 0.00036, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00027, "param_norms": 53.580091}, {"epoch": 4900, "train_losses": 0.000279, "test_losses": 0.000279, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000212, "param_norms": 53.990689}, {"epoch": 4950, "train_losses": 0.000216, "test_losses": 0.000216, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000166, "param_norms": 54.398488}, {"epoch": 4999, "train_losses": 0.000168, "test_losses": 0.000168, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000131, "param_norms": 54.795093}], "total_epochs": 5000}, "quad_single_freq": {"config": {"prime": 31, "d_mlp": 931, "act_type": "Quad", "init_type": "single-freq", "init_scale": 0.02, "optimizer": "SGD", "lr": 0.1, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 10000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=31, run=quad_single_freq\n======================================================================\n\nConfiguration:\n  prime (p)       = 31\n  d_mlp           = 931\n  activation      = Quad\n  init_type       = single-freq\n  init_scale      = 0.02\n  optimizer       = SGD\n  learning_rate   = 0.1\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 10000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.433966      3.433966      0.0302      0.0302      0.0035       3.3977\n     100      3.433852      3.433852      0.1041      0.1041      0.0035       3.3984\n     200      3.433732      3.433732      0.1811      0.1811      0.0035       3.4001\n     300      3.433610      3.433610      0.3351      0.3351      0.0035       3.4029\n     400      3.433490      3.433490      0.5120      0.5120      0.0035       3.4067\n     500      3.433367      3.433367      0.7055      0.7055      0.0035       3.4116\n     600      3.433244      3.433244      0.8314      0.8314      0.0035       3.4176\n     700      3.433119      3.433119      0.9126      0.9126      0.0035       3.4247\n     800      3.432993      3.432993      0.9646      0.9646      0.0036       3.4328\n     900      3.432865      3.432865      0.9792      0.9792      0.0036       3.4420\n    1000      3.432735      3.432735      0.9917      0.9917      0.0036       3.4524\n    1100      3.432603      3.432603      1.0000      1.0000      0.0037       3.4638\n    1200      3.432468      3.432468      1.0000      1.0000      0.0037       3.4763\n    1300      3.432331      3.432331      1.0000      1.0000      0.0037       3.4900\n    1400      3.432190      3.432190      1.0000      1.0000      0.0038       3.5048\n    1500      3.432047      3.432047      1.0000      1.0000      0.0038       3.5208\n    1600      3.431899      3.431899      1.0000      1.0000      0.0039       3.5379\n    1700      3.431748      3.431748      1.0000      1.0000      0.0039       3.5562\n    1800      3.431592      3.431592      1.0000      1.0000      0.0040       3.5756\n    1900      3.431432      3.431432      1.0000      1.0000      0.0040       3.5963\n    2000      3.431267      3.431267      1.0000      1.0000      0.0041       3.6183\n    2100      3.431096      3.431096      1.0000      1.0000      0.0042       3.6415\n    2200      3.430919      3.430919      1.0000      1.0000      0.0042       3.6659\n    2300      3.430736      3.430736      1.0000      1.0000      0.0043       3.6917\n    2400      3.430547      3.430547      1.0000      1.0000      0.0044       3.7187\n    2500      3.430349      3.430349      1.0000      1.0000      0.0045       3.7472\n    2600      3.430144      3.430144      1.0000      1.0000      0.0046       3.7770\n    2700      3.429930      3.429930      1.0000      1.0000      0.0047       3.8082\n    2800      3.429707      3.429707      1.0000      1.0000      0.0048       3.8409\n    2900      3.429475      3.429475      1.0000      1.0000      0.0049       3.8751\n    3000      3.429231      3.429231      1.0000      1.0000      0.0050       3.9108\n    3100      3.428977      3.428977      1.0000      1.0000      0.0051       3.9480\n    3200      3.428710      3.428710      1.0000      1.0000      0.0052       3.9869\n    3300      3.428429      3.428429      1.0000      1.0000      0.0054       4.0274\n    3400      3.428134      3.428134      1.0000      1.0000      0.0055       4.0697\n    3500      3.427823      3.427823      1.0000      1.0000      0.0056       4.1137\n    3600      3.427496      3.427496      1.0000      1.0000      0.0058       4.1596\n    3700      3.427151      3.427151      1.0000      1.0000      0.0060       4.2074\n    3800      3.426786      3.426786      1.0000      1.0000      0.0061       4.2571\n    3900      3.426400      3.426400      1.0000      1.0000      0.0063       4.3088\n    4000      3.425990      3.425990      1.0000      1.0000      0.0065       4.3627\n    4100      3.425555      3.425555      1.0000      1.0000      0.0067       4.4188\n    4200      3.425094      3.425094      1.0000      1.0000      0.0069       4.4772\n    4300      3.424603      3.424603      1.0000      1.0000      0.0071       4.5380\n    4400      3.424079      3.424079      1.0000      1.0000      0.0074       4.6013\n    4500      3.423519      3.423519      1.0000      1.0000      0.0076       4.6672\n    4600      3.422922      3.422922      1.0000      1.0000      0.0079       4.7358\n    4700      3.422282      3.422282      1.0000      1.0000      0.0081       4.8073\n    4800      3.421596      3.421596      1.0000      1.0000      0.0084       4.8819\n    4900      3.420859      3.420859      1.0000      1.0000      0.0087       4.9596\n    5000      3.420066      3.420066      1.0000      1.0000      0.0091       5.0407\n    5100      3.419212      3.419212      1.0000      1.0000      0.0094       5.1253\n    5200      3.418289      3.418289      1.0000      1.0000      0.0098       5.2136\n    5300      3.417292      3.417292      1.0000      1.0000      0.0102       5.3059\n    5400      3.416210      3.416210      1.0000      1.0000      0.0106       5.4023\n    5500      3.415035      3.415035      1.0000      1.0000      0.0111       5.5033\n    5600      3.413758      3.413758      1.0000      1.0000      0.0115       5.6089\n    5700      3.412364      3.412364      1.0000      1.0000      0.0121       5.7196\n    5800      3.410841      3.410841      1.0000      1.0000      0.0126       5.8357\n    5900      3.409172      3.409172      1.0000      1.0000      0.0132       5.9575\n    6000      3.407340      3.407340      1.0000      1.0000      0.0139       6.0855\n    6100      3.405321      3.405321      1.0000      1.0000      0.0146       6.2201\n    6200      3.403093      3.403093      1.0000      1.0000      0.0153       6.3619\n    6300      3.400626      3.400626      1.0000      1.0000      0.0161       6.5113\n    6400      3.397887      3.397887      1.0000      1.0000      0.0170       6.6691\n    6500      3.394834      3.394834      1.0000      1.0000      0.0180       6.8360\n    6600      3.391422      3.391422      1.0000      1.0000      0.0190       7.0126\n    6700      3.387593      3.387593      1.0000      1.0000      0.0201       7.1999\n    6800      3.383281      3.383281      1.0000      1.0000      0.0214       7.3989\n    6900      3.378406      3.378406      1.0000      1.0000      0.0228       7.6107\n    7000      3.372869      3.372869      1.0000      1.0000      0.0243       7.8367\n    7100      3.366553      3.366553      1.0000      1.0000      0.0260       8.0782\n    7200      3.359312      3.359312      1.0000      1.0000      0.0279       8.3370\n    7300      3.350964      3.350964      1.0000      1.0000      0.0300       8.6149\n    7400      3.341286      3.341286      1.0000      1.0000      0.0323       8.9143\n    7500      3.329998      3.329998      1.0000      1.0000      0.0349       9.2378\n    7600      3.316742      3.316742      1.0000      1.0000      0.0379       9.5885\n    7700      3.301061      3.301061      1.0000      1.0000      0.0413       9.9699\n    7800      3.282364      3.282364      1.0000      1.0000      0.0452      10.3863\n    7900      3.259879      3.259879      1.0000      1.0000      0.0497      10.8431\n    8000      3.232580      3.232580      1.0000      1.0000      0.0549      11.3462\n    8100      3.199095      3.199095      1.0000      1.0000      0.0610      11.9034\n    8200      3.157554      3.157554      1.0000      1.0000      0.0681      12.5238\n    8300      3.105381      3.105381      1.0000      1.0000      0.0766      13.2188\n    8400      3.038977      3.038977      1.0000      1.0000      0.0867      14.0026\n    8500      2.953250      2.953250      1.0000      1.0000      0.0988      14.8927\n    8600      2.840955      2.840955      1.0000      1.0000      0.1135      15.9107\n    8700      2.691828      2.691828      1.0000      1.0000      0.1312      17.0831\n    8800      2.491833      2.491833      1.0000      1.0000      0.1521      18.4397\n    8900      2.223897      2.223897      1.0000      1.0000      0.1754      20.0085\n    9000      1.874376      1.874376      1.0000      1.0000      0.1977      21.7986\n    9100      1.451950      1.451950      1.0000      1.0000      0.2109      23.7637\n    9200      1.013087      1.013087      1.0000      1.0000      0.2039      25.7610\n    9300      0.650258      0.650258      1.0000      1.0000      0.1735      27.5679\n    9400      0.411853      0.411853      1.0000      1.0000      0.1345      29.0232\n    9500      0.273023      0.273023      1.0000      1.0000      0.1018      30.1277\n    9600      0.192603      0.192603      1.0000      1.0000      0.0784      30.9649\n    9700      0.143698      0.143698      1.0000      1.0000      0.0621      31.6157\n    9800      0.112136      0.112136      1.0000      1.0000      0.0507      32.1371\n    9900      0.090629      0.090629      1.0000      1.0000      0.0424      32.5665\n    9999      0.075417      0.075417      1.0000      1.0000      0.0362      32.9251\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.075417\n  Test Loss   = 0.075417\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 32.9251\n\nTotal epochs trained: 10000\n", "table": [{"epoch": 0, "train_losses": 3.433966, "test_losses": 3.433966, "train_accs": 0.030177, "test_accs": 0.030177, "grad_norms": 0.003466, "param_norms": 3.397703}, {"epoch": 100, "train_losses": 3.433852, "test_losses": 3.433852, "train_accs": 0.104058, "test_accs": 0.104058, "grad_norms": 0.003468, "param_norms": 3.398355}, {"epoch": 200, "train_losses": 3.433732, "test_losses": 3.433732, "train_accs": 0.181061, "test_accs": 0.181061, "grad_norms": 0.003472, "param_norms": 3.400075}, {"epoch": 300, "train_losses": 3.43361, "test_losses": 3.43361, "train_accs": 0.335068, "test_accs": 0.335068, "grad_norms": 0.00348, "param_norms": 3.402855}, {"epoch": 400, "train_losses": 3.43349, "test_losses": 3.43349, "train_accs": 0.511967, "test_accs": 0.511967, "grad_norms": 0.003491, "param_norms": 3.4067}, {"epoch": 500, "train_losses": 3.433367, "test_losses": 3.433367, "train_accs": 0.705515, "test_accs": 0.705515, "grad_norms": 0.003505, "param_norms": 3.411617}, {"epoch": 600, "train_losses": 3.433244, "test_losses": 3.433244, "train_accs": 0.831426, "test_accs": 0.831426, "grad_norms": 0.003521, "param_norms": 3.4176}, {"epoch": 700, "train_losses": 3.433119, "test_losses": 3.433119, "train_accs": 0.912591, "test_accs": 0.912591, "grad_norms": 0.003541, "param_norms": 3.424663}, {"epoch": 800, "train_losses": 3.432993, "test_losses": 3.432993, "train_accs": 0.96462, "test_accs": 0.96462, "grad_norms": 0.003564, "param_norms": 3.432805}, {"epoch": 900, "train_losses": 3.432865, "test_losses": 3.432865, "train_accs": 0.979188, "test_accs": 0.979188, "grad_norms": 0.00359, "param_norms": 3.442035}, {"epoch": 1000, "train_losses": 3.432735, "test_losses": 3.432735, "train_accs": 0.991675, "test_accs": 0.991675, "grad_norms": 0.003619, "param_norms": 3.452361}, {"epoch": 1100, "train_losses": 3.432603, "test_losses": 3.432603, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003652, "param_norms": 3.463793}, {"epoch": 1200, "train_losses": 3.432468, "test_losses": 3.432468, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003688, "param_norms": 3.476335}, {"epoch": 1300, "train_losses": 3.432331, "test_losses": 3.432331, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003727, "param_norms": 3.490004}, {"epoch": 1400, "train_losses": 3.43219, "test_losses": 3.43219, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003769, "param_norms": 3.504806}, {"epoch": 1500, "train_losses": 3.432047, "test_losses": 3.432047, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003815, "param_norms": 3.520757}, {"epoch": 1600, "train_losses": 3.431899, "test_losses": 3.431899, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003864, "param_norms": 3.537873}, {"epoch": 1700, "train_losses": 3.431748, "test_losses": 3.431748, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003917, "param_norms": 3.556164}, {"epoch": 1800, "train_losses": 3.431592, "test_losses": 3.431592, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003974, "param_norms": 3.575649}, {"epoch": 1900, "train_losses": 3.431432, "test_losses": 3.431432, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004035, "param_norms": 3.596346}, {"epoch": 2000, "train_losses": 3.431267, "test_losses": 3.431267, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004099, "param_norms": 3.618276}, {"epoch": 2100, "train_losses": 3.431096, "test_losses": 3.431096, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004167, "param_norms": 3.641458}, {"epoch": 2200, "train_losses": 3.430919, "test_losses": 3.430919, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00424, "param_norms": 3.665913}, {"epoch": 2300, "train_losses": 3.430736, "test_losses": 3.430736, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004317, "param_norms": 3.691668}, {"epoch": 2400, "train_losses": 3.430547, "test_losses": 3.430547, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004398, "param_norms": 3.718747}, {"epoch": 2500, "train_losses": 3.430349, "test_losses": 3.430349, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004484, "param_norms": 3.747178}, {"epoch": 2600, "train_losses": 3.430144, "test_losses": 3.430144, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004575, "param_norms": 3.776994}, {"epoch": 2700, "train_losses": 3.42993, "test_losses": 3.42993, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004671, "param_norms": 3.808226}, {"epoch": 2800, "train_losses": 3.429707, "test_losses": 3.429707, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004771, "param_norms": 3.840907}, {"epoch": 2900, "train_losses": 3.429475, "test_losses": 3.429475, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004878, "param_norms": 3.875073}, {"epoch": 3000, "train_losses": 3.429231, "test_losses": 3.429231, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00499, "param_norms": 3.910768}, {"epoch": 3100, "train_losses": 3.428977, "test_losses": 3.428977, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005108, "param_norms": 3.948029}, {"epoch": 3200, "train_losses": 3.42871, "test_losses": 3.42871, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005232, "param_norms": 3.986909}, {"epoch": 3300, "train_losses": 3.428429, "test_losses": 3.428429, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005363, "param_norms": 4.02745}, {"epoch": 3400, "train_losses": 3.428134, "test_losses": 3.428134, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0055, "param_norms": 4.069706}, {"epoch": 3500, "train_losses": 3.427823, "test_losses": 3.427823, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005645, "param_norms": 4.113736}, {"epoch": 3600, "train_losses": 3.427496, "test_losses": 3.427496, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005798, "param_norms": 4.159597}, {"epoch": 3700, "train_losses": 3.427151, "test_losses": 3.427151, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005958, "param_norms": 4.207352}, {"epoch": 3800, "train_losses": 3.426786, "test_losses": 3.426786, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006127, "param_norms": 4.257076}, {"epoch": 3900, "train_losses": 3.4264, "test_losses": 3.4264, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006306, "param_norms": 4.30884}, {"epoch": 4000, "train_losses": 3.42599, "test_losses": 3.42599, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006494, "param_norms": 4.362721}, {"epoch": 4100, "train_losses": 3.425555, "test_losses": 3.425555, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006692, "param_norms": 4.418808}, {"epoch": 4200, "train_losses": 3.425094, "test_losses": 3.425094, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006901, "param_norms": 4.477195}, {"epoch": 4300, "train_losses": 3.424603, "test_losses": 3.424603, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007122, "param_norms": 4.537976}, {"epoch": 4400, "train_losses": 3.424079, "test_losses": 3.424079, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007355, "param_norms": 4.60126}, {"epoch": 4500, "train_losses": 3.423519, "test_losses": 3.423519, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007601, "param_norms": 4.667164}, {"epoch": 4600, "train_losses": 3.422922, "test_losses": 3.422922, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007862, "param_norms": 4.735807}, {"epoch": 4700, "train_losses": 3.422282, "test_losses": 3.422282, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008138, "param_norms": 4.807327}, {"epoch": 4800, "train_losses": 3.421596, "test_losses": 3.421596, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008431, "param_norms": 4.88187}, {"epoch": 4900, "train_losses": 3.420859, "test_losses": 3.420859, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008741, "param_norms": 4.959588}, {"epoch": 5000, "train_losses": 3.420066, "test_losses": 3.420066, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009071, "param_norms": 5.040659}, {"epoch": 5100, "train_losses": 3.419212, "test_losses": 3.419212, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009421, "param_norms": 5.125254}, {"epoch": 5200, "train_losses": 3.418289, "test_losses": 3.418289, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009793, "param_norms": 5.213588}, {"epoch": 5300, "train_losses": 3.417292, "test_losses": 3.417292, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01019, "param_norms": 5.305866}, {"epoch": 5400, "train_losses": 3.41621, "test_losses": 3.41621, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010614, "param_norms": 5.402336}, {"epoch": 5500, "train_losses": 3.415035, "test_losses": 3.415035, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011066, "param_norms": 5.503254}, {"epoch": 5600, "train_losses": 3.413758, "test_losses": 3.413758, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01155, "param_norms": 5.6089}, {"epoch": 5700, "train_losses": 3.412364, "test_losses": 3.412364, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012068, "param_norms": 5.71959}, {"epoch": 5800, "train_losses": 3.410841, "test_losses": 3.410841, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012623, "param_norms": 5.83566}, {"epoch": 5900, "train_losses": 3.409172, "test_losses": 3.409172, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013221, "param_norms": 5.957488}, {"epoch": 6000, "train_losses": 3.40734, "test_losses": 3.40734, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013864, "param_norms": 6.085487}, {"epoch": 6100, "train_losses": 3.405321, "test_losses": 3.405321, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014557, "param_norms": 6.220117}, {"epoch": 6200, "train_losses": 3.403093, "test_losses": 3.403093, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.015307, "param_norms": 6.361883}, {"epoch": 6300, "train_losses": 3.400626, "test_losses": 3.400626, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016118, "param_norms": 6.511344}, {"epoch": 6400, "train_losses": 3.397887, "test_losses": 3.397887, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016999, "param_norms": 6.669132}, {"epoch": 6500, "train_losses": 3.394834, "test_losses": 3.394834, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.017958, "param_norms": 6.83595}, {"epoch": 6600, "train_losses": 3.391422, "test_losses": 3.391422, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.019004, "param_norms": 7.012575}, {"epoch": 6700, "train_losses": 3.387593, "test_losses": 3.387593, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.020147, "param_norms": 7.199903}, {"epoch": 6800, "train_losses": 3.383281, "test_losses": 3.383281, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.021401, "param_norms": 7.398915}, {"epoch": 6900, "train_losses": 3.378406, "test_losses": 3.378406, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.022781, "param_norms": 7.610749}, {"epoch": 7000, "train_losses": 3.372869, "test_losses": 3.372869, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.024303, "param_norms": 7.836693}, {"epoch": 7100, "train_losses": 3.366553, "test_losses": 3.366553, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.02599, "param_norms": 8.0782}, {"epoch": 7200, "train_losses": 3.359312, "test_losses": 3.359312, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.027864, "param_norms": 8.336974}, {"epoch": 7300, "train_losses": 3.350964, "test_losses": 3.350964, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.029957, "param_norms": 8.614942}, {"epoch": 7400, "train_losses": 3.341286, "test_losses": 3.341286, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.032302, "param_norms": 8.914349}, {"epoch": 7500, "train_losses": 3.329998, "test_losses": 3.329998, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.034943, "param_norms": 9.237835}, {"epoch": 7600, "train_losses": 3.316742, "test_losses": 3.316742, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.037932, "param_norms": 9.588464}, {"epoch": 7700, "train_losses": 3.301061, "test_losses": 3.301061, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.041333, "param_norms": 9.969863}, {"epoch": 7800, "train_losses": 3.282364, "test_losses": 3.282364, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.045225, "param_norms": 10.386341}, {"epoch": 7900, "train_losses": 3.259879, "test_losses": 3.259879, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.049707, "param_norms": 10.843058}, {"epoch": 8000, "train_losses": 3.23258, "test_losses": 3.23258, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.054903, "param_norms": 11.346232}, {"epoch": 8100, "train_losses": 3.199095, "test_losses": 3.199095, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.060968, "param_norms": 11.9034}, {"epoch": 8200, "train_losses": 3.157554, "test_losses": 3.157554, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.068104, "param_norms": 12.523807}, {"epoch": 8300, "train_losses": 3.105381, "test_losses": 3.105381, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.076562, "param_norms": 13.218836}, {"epoch": 8400, "train_losses": 3.038977, "test_losses": 3.038977, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.086666, "param_norms": 14.0026}, {"epoch": 8500, "train_losses": 2.95325, "test_losses": 2.95325, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.098817, "param_norms": 14.892653}, {"epoch": 8600, "train_losses": 2.840955, "test_losses": 2.840955, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.113487, "param_norms": 15.910699}, {"epoch": 8700, "train_losses": 2.691828, "test_losses": 2.691828, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.131156, "param_norms": 17.083085}, {"epoch": 8800, "train_losses": 2.491833, "test_losses": 2.491833, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.152052, "param_norms": 18.43968}, {"epoch": 8900, "train_losses": 2.223897, "test_losses": 2.223897, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.175396, "param_norms": 20.008541}, {"epoch": 9000, "train_losses": 1.874376, "test_losses": 1.874376, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.197708, "param_norms": 21.798644}, {"epoch": 9100, "train_losses": 1.45195, "test_losses": 1.45195, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.210903, "param_norms": 23.7637}, {"epoch": 9200, "train_losses": 1.013087, "test_losses": 1.013087, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.203858, "param_norms": 25.761006}, {"epoch": 9300, "train_losses": 0.650258, "test_losses": 0.650258, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.173453, "param_norms": 27.56792}, {"epoch": 9400, "train_losses": 0.411853, "test_losses": 0.411853, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.13451, "param_norms": 29.02321}, {"epoch": 9500, "train_losses": 0.273023, "test_losses": 0.273023, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.101792, "param_norms": 30.127685}, {"epoch": 9600, "train_losses": 0.192603, "test_losses": 0.192603, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.078352, "param_norms": 30.964884}, {"epoch": 9700, "train_losses": 0.143698, "test_losses": 0.143698, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.062097, "param_norms": 31.61568}, {"epoch": 9800, "train_losses": 0.112136, "test_losses": 0.112136, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.050652, "param_norms": 32.137096}, {"epoch": 9900, "train_losses": 0.090629, "test_losses": 0.090629, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.042356, "param_norms": 32.566528}, {"epoch": 9999, "train_losses": 0.075417, "test_losses": 0.075417, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.036212, "param_norms": 32.925131}], "total_epochs": 10000}, "relu_single_freq": {"config": {"prime": 31, "d_mlp": 931, "act_type": "ReLU", "init_type": "single-freq", "init_scale": 0.002, "optimizer": "SGD", "lr": 0.01, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 10000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=31, run=relu_single_freq\n======================================================================\n\nConfiguration:\n  prime (p)       = 31\n  d_mlp           = 931\n  activation      = ReLU\n  init_type       = single-freq\n  init_scale      = 0.002\n  optimizer       = SGD\n  learning_rate   = 0.01\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 10000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.433985      3.433985      0.0323      0.0323      0.0041       0.3398\n     100      3.433969      3.433969      0.0572      0.0572      0.0041       0.3398\n     200      3.433951      3.433951      0.1061      0.1061      0.0041       0.3400\n     300      3.433935      3.433935      0.1821      0.1821      0.0041       0.3403\n     400      3.433918      3.433918      0.2466      0.2466      0.0041       0.3406\n     500      3.433901      3.433901      0.3132      0.3132      0.0041       0.3411\n     600      3.433884      3.433884      0.4194      0.4194      0.0041       0.3416\n     700      3.433867      3.433867      0.5182      0.5182      0.0041       0.3423\n     800      3.433850      3.433850      0.6202      0.6202      0.0041       0.3430\n     900      3.433833      3.433833      0.7180      0.7180      0.0041       0.3439\n    1000      3.433817      3.433817      0.8044      0.8044      0.0041       0.3448\n    1100      3.433800      3.433800      0.8585      0.8585      0.0041       0.3458\n    1200      3.433783      3.433783      0.9011      0.9011      0.0041       0.3470\n    1300      3.433766      3.433766      0.9355      0.9355      0.0041       0.3482\n    1400      3.433749      3.433749      0.9584      0.9584      0.0041       0.3495\n    1500      3.433733      3.433733      0.9761      0.9761      0.0041       0.3509\n    1600      3.433716      3.433716      0.9844      0.9844      0.0041       0.3524\n    1700      3.433699      3.433699      0.9906      0.9906      0.0041       0.3540\n    1800      3.433682      3.433682      0.9938      0.9938      0.0041       0.3557\n    1900      3.433665      3.433665      0.9938      0.9938      0.0041       0.3574\n    2000      3.433649      3.433649      0.9969      0.9969      0.0041       0.3593\n    2100      3.433632      3.433632      0.9969      0.9969      0.0042       0.3612\n    2200      3.433615      3.433615      0.9969      0.9969      0.0042       0.3632\n    2300      3.433598      3.433598      0.9969      0.9969      0.0042       0.3653\n    2400      3.433581      3.433581      0.9969      0.9969      0.0042       0.3675\n    2500      3.433564      3.433564      1.0000      1.0000      0.0042       0.3697\n    2600      3.433547      3.433547      1.0000      1.0000      0.0042       0.3721\n    2700      3.433530      3.433530      1.0000      1.0000      0.0042       0.3745\n    2800      3.433513      3.433513      1.0000      1.0000      0.0042       0.3769\n    2900      3.433496      3.433496      1.0000      1.0000      0.0042       0.3795\n    3000      3.433478      3.433478      1.0000      1.0000      0.0043       0.3821\n    3100      3.433461      3.433461      1.0000      1.0000      0.0043       0.3848\n    3200      3.433444      3.433444      1.0000      1.0000      0.0043       0.3876\n    3300      3.433426      3.433426      1.0000      1.0000      0.0043       0.3904\n    3400      3.433409      3.433409      1.0000      1.0000      0.0044       0.3933\n    3500      3.433391      3.433391      1.0000      1.0000      0.0044       0.3963\n    3600      3.433373      3.433373      1.0000      1.0000      0.0044       0.3993\n    3700      3.433356      3.433356      1.0000      1.0000      0.0044       0.4024\n    3800      3.433337      3.433337      1.0000      1.0000      0.0045       0.4056\n    3900      3.433319      3.433319      1.0000      1.0000      0.0045       0.4089\n    4000      3.433300      3.433300      1.0000      1.0000      0.0045       0.4122\n    4100      3.433282      3.433282      1.0000      1.0000      0.0045       0.4155\n    4200      3.433264      3.433264      1.0000      1.0000      0.0046       0.4189\n    4300      3.433244      3.433244      1.0000      1.0000      0.0046       0.4224\n    4400      3.433225      3.433225      1.0000      1.0000      0.0047       0.4260\n    4500      3.433206      3.433206      1.0000      1.0000      0.0047       0.4296\n    4600      3.433187      3.433187      1.0000      1.0000      0.0047       0.4332\n    4700      3.433167      3.433167      1.0000      1.0000      0.0048       0.4370\n    4800      3.433147      3.433147      1.0000      1.0000      0.0048       0.4408\n    4900      3.433127      3.433127      1.0000      1.0000      0.0048       0.4446\n    5000      3.433106      3.433106      1.0000      1.0000      0.0049       0.4485\n    5100      3.433086      3.433086      1.0000      1.0000      0.0049       0.4525\n    5200      3.433065      3.433065      1.0000      1.0000      0.0050       0.4565\n    5300      3.433044      3.433044      1.0000      1.0000      0.0050       0.4605\n    5400      3.433022      3.433022      1.0000      1.0000      0.0050       0.4647\n    5500      3.433001      3.433001      1.0000      1.0000      0.0051       0.4688\n    5600      3.432978      3.432978      1.0000      1.0000      0.0051       0.4731\n    5700      3.432956      3.432956      1.0000      1.0000      0.0052       0.4774\n    5800      3.432933      3.432933      1.0000      1.0000      0.0052       0.4817\n    5900      3.432910      3.432910      1.0000      1.0000      0.0053       0.4861\n    6000      3.432888      3.432888      1.0000      1.0000      0.0053       0.4906\n    6100      3.432864      3.432864      1.0000      1.0000      0.0054       0.4951\n    6200      3.432840      3.432840      1.0000      1.0000      0.0054       0.4997\n    6300      3.432815      3.432815      1.0000      1.0000      0.0055       0.5043\n    6400      3.432791      3.432791      1.0000      1.0000      0.0055       0.5089\n    6500      3.432765      3.432765      1.0000      1.0000      0.0056       0.5137\n    6600      3.432740      3.432740      1.0000      1.0000      0.0056       0.5185\n    6700      3.432714      3.432714      1.0000      1.0000      0.0057       0.5233\n    6800      3.432687      3.432687      1.0000      1.0000      0.0058       0.5282\n    6900      3.432660      3.432660      1.0000      1.0000      0.0058       0.5331\n    7000      3.432633      3.432633      1.0000      1.0000      0.0059       0.5381\n    7100      3.432606      3.432606      1.0000      1.0000      0.0059       0.5432\n    7200      3.432577      3.432577      1.0000      1.0000      0.0060       0.5483\n    7300      3.432548      3.432548      1.0000      1.0000      0.0060       0.5535\n    7400      3.432519      3.432519      1.0000      1.0000      0.0061       0.5587\n    7500      3.432489      3.432489      1.0000      1.0000      0.0061       0.5640\n    7600      3.432459      3.432459      1.0000      1.0000      0.0062       0.5693\n    7700      3.432428      3.432428      1.0000      1.0000      0.0063       0.5747\n    7800      3.432396      3.432396      1.0000      1.0000      0.0063       0.5802\n    7900      3.432364      3.432364      1.0000      1.0000      0.0064       0.5857\n    8000      3.432332      3.432332      1.0000      1.0000      0.0065       0.5913\n    8100      3.432298      3.432298      1.0000      1.0000      0.0065       0.5969\n    8200      3.432264      3.432264      1.0000      1.0000      0.0066       0.6026\n    8300      3.432230      3.432230      1.0000      1.0000      0.0066       0.6083\n    8400      3.432195      3.432195      1.0000      1.0000      0.0067       0.6142\n    8500      3.432159      3.432159      1.0000      1.0000      0.0068       0.6200\n    8600      3.432122      3.432122      1.0000      1.0000      0.0068       0.6259\n    8700      3.432085      3.432085      1.0000      1.0000      0.0069       0.6319\n    8800      3.432047      3.432047      1.0000      1.0000      0.0070       0.6380\n    8900      3.432009      3.432009      1.0000      1.0000      0.0071       0.6441\n    9000      3.431970      3.431970      1.0000      1.0000      0.0071       0.6503\n    9100      3.431930      3.431930      1.0000      1.0000      0.0072       0.6565\n    9200      3.431889      3.431889      1.0000      1.0000      0.0073       0.6628\n    9300      3.431847      3.431847      1.0000      1.0000      0.0074       0.6692\n    9400      3.431805      3.431805      1.0000      1.0000      0.0074       0.6756\n    9500      3.431761      3.431761      1.0000      1.0000      0.0075       0.6821\n    9600      3.431718      3.431718      1.0000      1.0000      0.0076       0.6886\n    9700      3.431673      3.431673      1.0000      1.0000      0.0076       0.6953\n    9800      3.431627      3.431627      1.0000      1.0000      0.0077       0.7020\n    9900      3.431581      3.431581      1.0000      1.0000      0.0078       0.7087\n    9999      3.431533      3.431533      1.0000      1.0000      0.0079       0.7155\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 3.431533\n  Test Loss   = 3.431533\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 0.7155\n\nTotal epochs trained: 10000\n", "table": [{"epoch": 0, "train_losses": 3.433985, "test_losses": 3.433985, "train_accs": 0.032258, "test_accs": 0.032258, "grad_norms": 0.004112, "param_norms": 0.339771}, {"epoch": 100, "train_losses": 3.433969, "test_losses": 3.433969, "train_accs": 0.057232, "test_accs": 0.057232, "grad_norms": 0.004114, "param_norms": 0.339831}, {"epoch": 200, "train_losses": 3.433951, "test_losses": 3.433951, "train_accs": 0.106139, "test_accs": 0.106139, "grad_norms": 0.004109, "param_norms": 0.339992}, {"epoch": 300, "train_losses": 3.433935, "test_losses": 3.433935, "train_accs": 0.182102, "test_accs": 0.182102, "grad_norms": 0.004106, "param_norms": 0.340252}, {"epoch": 400, "train_losses": 3.433918, "test_losses": 3.433918, "train_accs": 0.246618, "test_accs": 0.246618, "grad_norms": 0.004106, "param_norms": 0.34061}, {"epoch": 500, "train_losses": 3.433901, "test_losses": 3.433901, "train_accs": 0.313215, "test_accs": 0.313215, "grad_norms": 0.004107, "param_norms": 0.341068}, {"epoch": 600, "train_losses": 3.433884, "test_losses": 3.433884, "train_accs": 0.419355, "test_accs": 0.419355, "grad_norms": 0.004107, "param_norms": 0.341623}, {"epoch": 700, "train_losses": 3.433867, "test_losses": 3.433867, "train_accs": 0.51821, "test_accs": 0.51821, "grad_norms": 0.004107, "param_norms": 0.342276}, {"epoch": 800, "train_losses": 3.43385, "test_losses": 3.43385, "train_accs": 0.620187, "test_accs": 0.620187, "grad_norms": 0.004107, "param_norms": 0.343026}, {"epoch": 900, "train_losses": 3.433833, "test_losses": 3.433833, "train_accs": 0.718002, "test_accs": 0.718002, "grad_norms": 0.004106, "param_norms": 0.343872}, {"epoch": 1000, "train_losses": 3.433817, "test_losses": 3.433817, "train_accs": 0.80437, "test_accs": 0.80437, "grad_norms": 0.004106, "param_norms": 0.344813}, {"epoch": 1100, "train_losses": 3.4338, "test_losses": 3.4338, "train_accs": 0.858481, "test_accs": 0.858481, "grad_norms": 0.004107, "param_norms": 0.34585}, {"epoch": 1200, "train_losses": 3.433783, "test_losses": 3.433783, "train_accs": 0.901145, "test_accs": 0.901145, "grad_norms": 0.004108, "param_norms": 0.34698}, {"epoch": 1300, "train_losses": 3.433766, "test_losses": 3.433766, "train_accs": 0.935484, "test_accs": 0.935484, "grad_norms": 0.004112, "param_norms": 0.348203}, {"epoch": 1400, "train_losses": 3.433749, "test_losses": 3.433749, "train_accs": 0.958377, "test_accs": 0.958377, "grad_norms": 0.004115, "param_norms": 0.349518}, {"epoch": 1500, "train_losses": 3.433733, "test_losses": 3.433733, "train_accs": 0.976067, "test_accs": 0.976067, "grad_norms": 0.004117, "param_norms": 0.350924}, {"epoch": 1600, "train_losses": 3.433716, "test_losses": 3.433716, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.00412, "param_norms": 0.352419}, {"epoch": 1700, "train_losses": 3.433699, "test_losses": 3.433699, "train_accs": 0.990635, "test_accs": 0.990635, "grad_norms": 0.004126, "param_norms": 0.354004}, {"epoch": 1800, "train_losses": 3.433682, "test_losses": 3.433682, "train_accs": 0.993757, "test_accs": 0.993757, "grad_norms": 0.004132, "param_norms": 0.355675}, {"epoch": 1900, "train_losses": 3.433665, "test_losses": 3.433665, "train_accs": 0.993757, "test_accs": 0.993757, "grad_norms": 0.004137, "param_norms": 0.357433}, {"epoch": 2000, "train_losses": 3.433649, "test_losses": 3.433649, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004144, "param_norms": 0.359276}, {"epoch": 2100, "train_losses": 3.433632, "test_losses": 3.433632, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004152, "param_norms": 0.361202}, {"epoch": 2200, "train_losses": 3.433615, "test_losses": 3.433615, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004159, "param_norms": 0.363211}, {"epoch": 2300, "train_losses": 3.433598, "test_losses": 3.433598, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004172, "param_norms": 0.365302}, {"epoch": 2400, "train_losses": 3.433581, "test_losses": 3.433581, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004183, "param_norms": 0.367473}, {"epoch": 2500, "train_losses": 3.433564, "test_losses": 3.433564, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004193, "param_norms": 0.369723}, {"epoch": 2600, "train_losses": 3.433547, "test_losses": 3.433547, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004208, "param_norms": 0.37205}, {"epoch": 2700, "train_losses": 3.43353, "test_losses": 3.43353, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004219, "param_norms": 0.374454}, {"epoch": 2800, "train_losses": 3.433513, "test_losses": 3.433513, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004233, "param_norms": 0.376933}, {"epoch": 2900, "train_losses": 3.433496, "test_losses": 3.433496, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004249, "param_norms": 0.379486}, {"epoch": 3000, "train_losses": 3.433478, "test_losses": 3.433478, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00427, "param_norms": 0.382112}, {"epoch": 3100, "train_losses": 3.433461, "test_losses": 3.433461, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004292, "param_norms": 0.38481}, {"epoch": 3200, "train_losses": 3.433444, "test_losses": 3.433444, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004313, "param_norms": 0.387578}, {"epoch": 3300, "train_losses": 3.433426, "test_losses": 3.433426, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004335, "param_norms": 0.390416}, {"epoch": 3400, "train_losses": 3.433409, "test_losses": 3.433409, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004358, "param_norms": 0.393323}, {"epoch": 3500, "train_losses": 3.433391, "test_losses": 3.433391, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004382, "param_norms": 0.396298}, {"epoch": 3600, "train_losses": 3.433373, "test_losses": 3.433373, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004405, "param_norms": 0.39934}, {"epoch": 3700, "train_losses": 3.433356, "test_losses": 3.433356, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004433, "param_norms": 0.402447}, {"epoch": 3800, "train_losses": 3.433337, "test_losses": 3.433337, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004455, "param_norms": 0.40562}, {"epoch": 3900, "train_losses": 3.433319, "test_losses": 3.433319, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004487, "param_norms": 0.408857}, {"epoch": 4000, "train_losses": 3.4333, "test_losses": 3.4333, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004514, "param_norms": 0.412157}, {"epoch": 4100, "train_losses": 3.433282, "test_losses": 3.433282, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00455, "param_norms": 0.415519}, {"epoch": 4200, "train_losses": 3.433264, "test_losses": 3.433264, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004584, "param_norms": 0.418945}, {"epoch": 4300, "train_losses": 3.433244, "test_losses": 3.433244, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004613, "param_norms": 0.42243}, {"epoch": 4400, "train_losses": 3.433225, "test_losses": 3.433225, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004652, "param_norms": 0.425977}, {"epoch": 4500, "train_losses": 3.433206, "test_losses": 3.433206, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004694, "param_norms": 0.429584}, {"epoch": 4600, "train_losses": 3.433187, "test_losses": 3.433187, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004725, "param_norms": 0.43325}, {"epoch": 4700, "train_losses": 3.433167, "test_losses": 3.433167, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004763, "param_norms": 0.436974}, {"epoch": 4800, "train_losses": 3.433147, "test_losses": 3.433147, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004804, "param_norms": 0.440758}, {"epoch": 4900, "train_losses": 3.433127, "test_losses": 3.433127, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004838, "param_norms": 0.444599}, {"epoch": 5000, "train_losses": 3.433106, "test_losses": 3.433106, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004879, "param_norms": 0.448498}, {"epoch": 5100, "train_losses": 3.433086, "test_losses": 3.433086, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004915, "param_norms": 0.452454}, {"epoch": 5200, "train_losses": 3.433065, "test_losses": 3.433065, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004959, "param_norms": 0.456467}, {"epoch": 5300, "train_losses": 3.433044, "test_losses": 3.433044, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004992, "param_norms": 0.460536}, {"epoch": 5400, "train_losses": 3.433022, "test_losses": 3.433022, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005042, "param_norms": 0.464661}, {"epoch": 5500, "train_losses": 3.433001, "test_losses": 3.433001, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005081, "param_norms": 0.468842}, {"epoch": 5600, "train_losses": 3.432978, "test_losses": 3.432978, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005126, "param_norms": 0.473078}, {"epoch": 5700, "train_losses": 3.432956, "test_losses": 3.432956, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005176, "param_norms": 0.47737}, {"epoch": 5800, "train_losses": 3.432933, "test_losses": 3.432933, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005232, "param_norms": 0.481717}, {"epoch": 5900, "train_losses": 3.43291, "test_losses": 3.43291, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005273, "param_norms": 0.486118}, {"epoch": 6000, "train_losses": 3.432888, "test_losses": 3.432888, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005329, "param_norms": 0.490574}, {"epoch": 6100, "train_losses": 3.432864, "test_losses": 3.432864, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005371, "param_norms": 0.495085}, {"epoch": 6200, "train_losses": 3.43284, "test_losses": 3.43284, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005424, "param_norms": 0.499651}, {"epoch": 6300, "train_losses": 3.432815, "test_losses": 3.432815, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005472, "param_norms": 0.504271}, {"epoch": 6400, "train_losses": 3.432791, "test_losses": 3.432791, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005527, "param_norms": 0.508946}, {"epoch": 6500, "train_losses": 3.432765, "test_losses": 3.432765, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005575, "param_norms": 0.513675}, {"epoch": 6600, "train_losses": 3.43274, "test_losses": 3.43274, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005631, "param_norms": 0.518459}, {"epoch": 6700, "train_losses": 3.432714, "test_losses": 3.432714, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005683, "param_norms": 0.523298}, {"epoch": 6800, "train_losses": 3.432687, "test_losses": 3.432687, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005751, "param_norms": 0.528193}, {"epoch": 6900, "train_losses": 3.43266, "test_losses": 3.43266, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005804, "param_norms": 0.533141}, {"epoch": 7000, "train_losses": 3.432633, "test_losses": 3.432633, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005851, "param_norms": 0.538146}, {"epoch": 7100, "train_losses": 3.432606, "test_losses": 3.432606, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005918, "param_norms": 0.543205}, {"epoch": 7200, "train_losses": 3.432577, "test_losses": 3.432577, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005969, "param_norms": 0.54832}, {"epoch": 7300, "train_losses": 3.432548, "test_losses": 3.432548, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006023, "param_norms": 0.553492}, {"epoch": 7400, "train_losses": 3.432519, "test_losses": 3.432519, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006088, "param_norms": 0.558719}, {"epoch": 7500, "train_losses": 3.432489, "test_losses": 3.432489, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006137, "param_norms": 0.564003}, {"epoch": 7600, "train_losses": 3.432459, "test_losses": 3.432459, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006195, "param_norms": 0.569343}, {"epoch": 7700, "train_losses": 3.432428, "test_losses": 3.432428, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006269, "param_norms": 0.574741}, {"epoch": 7800, "train_losses": 3.432396, "test_losses": 3.432396, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006326, "param_norms": 0.580196}, {"epoch": 7900, "train_losses": 3.432364, "test_losses": 3.432364, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006396, "param_norms": 0.585709}, {"epoch": 8000, "train_losses": 3.432332, "test_losses": 3.432332, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00645, "param_norms": 0.591279}, {"epoch": 8100, "train_losses": 3.432298, "test_losses": 3.432298, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006513, "param_norms": 0.596909}, {"epoch": 8200, "train_losses": 3.432264, "test_losses": 3.432264, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006565, "param_norms": 0.602597}, {"epoch": 8300, "train_losses": 3.43223, "test_losses": 3.43223, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006629, "param_norms": 0.608344}, {"epoch": 8400, "train_losses": 3.432195, "test_losses": 3.432195, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006701, "param_norms": 0.614151}, {"epoch": 8500, "train_losses": 3.432159, "test_losses": 3.432159, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006774, "param_norms": 0.620019}, {"epoch": 8600, "train_losses": 3.432122, "test_losses": 3.432122, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006842, "param_norms": 0.625946}, {"epoch": 8700, "train_losses": 3.432085, "test_losses": 3.432085, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00692, "param_norms": 0.631934}, {"epoch": 8800, "train_losses": 3.432047, "test_losses": 3.432047, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006993, "param_norms": 0.637984}, {"epoch": 8900, "train_losses": 3.432009, "test_losses": 3.432009, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007059, "param_norms": 0.644096}, {"epoch": 9000, "train_losses": 3.43197, "test_losses": 3.43197, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007132, "param_norms": 0.65027}, {"epoch": 9100, "train_losses": 3.43193, "test_losses": 3.43193, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0072, "param_norms": 0.656507}, {"epoch": 9200, "train_losses": 3.431889, "test_losses": 3.431889, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00727, "param_norms": 0.662807}, {"epoch": 9300, "train_losses": 3.431847, "test_losses": 3.431847, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007355, "param_norms": 0.669171}, {"epoch": 9400, "train_losses": 3.431805, "test_losses": 3.431805, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007432, "param_norms": 0.675598}, {"epoch": 9500, "train_losses": 3.431761, "test_losses": 3.431761, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007492, "param_norms": 0.682091}, {"epoch": 9600, "train_losses": 3.431718, "test_losses": 3.431718, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00757, "param_norms": 0.688649}, {"epoch": 9700, "train_losses": 3.431673, "test_losses": 3.431673, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007634, "param_norms": 0.695274}, {"epoch": 9800, "train_losses": 3.431627, "test_losses": 3.431627, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007712, "param_norms": 0.701964}, {"epoch": 9900, "train_losses": 3.431581, "test_losses": 3.431581, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007794, "param_norms": 0.708721}, {"epoch": 9999, "train_losses": 3.431533, "test_losses": 3.431533, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007866, "param_norms": 0.715478}], "total_epochs": 10000}}