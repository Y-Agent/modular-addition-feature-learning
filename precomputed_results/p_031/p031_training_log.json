{"standard": {"config": {"prime": 31, "d_mlp": 931, "act_type": "ReLU", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 5e-05, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=31, run=standard\n======================================================================\n\nConfiguration:\n  prime (p)       = 31\n  d_mlp           = 931\n  activation      = ReLU\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 5e-05\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.433967      3.433967      0.0364      0.0364      0.0160       4.3206\n      50      3.427824      3.427824      0.1186      0.1186      0.0157       4.3785\n     100      3.420660      3.420660      0.2893      0.2893      0.0166       4.5857\n     150      3.412555      3.412555      0.5702      0.5702      0.0184       4.9007\n     200      3.403128      3.403128      0.7492      0.7492      0.0210       5.2865\n     250      3.392056      3.392056      0.8450      0.8450      0.0241       5.7178\n     300      3.379155      3.379155      0.9074      0.9074      0.0276       6.1786\n     350      3.364295      3.364295      0.9386      0.9386      0.0315       6.6585\n     400      3.347432      3.347432      0.9625      0.9625      0.0354       7.1520\n     450      3.328524      3.328524      0.9698      0.9698      0.0396       7.6550\n     500      3.307514      3.307514      0.9792      0.9792      0.0439       8.1649\n     550      3.284378      3.284378      0.9781      0.9781      0.0481       8.6801\n     600      3.259124      3.259124      0.9771      0.9771      0.0525       9.1996\n     650      3.231740      3.231740      0.9792      0.9792      0.0568       9.7228\n     700      3.202228      3.202228      0.9802      0.9802      0.0612      10.2493\n     750      3.170569      3.170569      0.9802      0.9802      0.0657      10.7786\n     800      3.136745      3.136745      0.9802      0.9802      0.0701      11.3105\n     850      3.100763      3.100763      0.9802      0.9802      0.0745      11.8447\n     900      3.062609      3.062609      0.9802      0.9802      0.0790      12.3810\n     950      3.022317      3.022317      0.9802      0.9802      0.0835      12.9190\n    1000      2.979927      2.979927      0.9802      0.9802      0.0879      13.4582\n    1050      2.935420      2.935420      0.9802      0.9802      0.0923      13.9982\n    1100      2.888808      2.888808      0.9802      0.9802      0.0966      14.5390\n    1150      2.840152      2.840152      0.9802      0.9802      0.1010      15.0806\n    1200      2.789488      2.789488      0.9813      0.9813      0.1052      15.6231\n    1250      2.736830      2.736830      0.9813      0.9813      0.1094      16.1662\n    1300      2.682209      2.682209      0.9813      0.9813      0.1136      16.7097\n    1350      2.625640      2.625640      0.9802      0.9802      0.1177      17.2535\n    1400      2.567185      2.567185      0.9802      0.9802      0.1217      17.7975\n    1450      2.506899      2.506899      0.9802      0.9802      0.1256      18.3415\n    1500      2.444830      2.444830      0.9802      0.9802      0.1294      18.8855\n    1550      2.381073      2.381073      0.9802      0.9802      0.1331      19.4294\n    1600      2.315698      2.315698      0.9802      0.9802      0.1365      19.9729\n    1650      2.248785      2.248785      0.9813      0.9813      0.1399      20.5163\n    1700      2.180366      2.180366      0.9813      0.9813      0.1431      21.0595\n    1750      2.110604      2.110604      0.9813      0.9813      0.1461      21.6023\n    1800      2.039637      2.039637      0.9813      0.9813      0.1488      22.1446\n    1850      1.967574      1.967574      0.9813      0.9813      0.1512      22.6864\n    1900      1.894513      1.894513      0.9813      0.9813      0.1534      23.2272\n    1950      1.820601      1.820601      0.9834      0.9834      0.1553      23.7673\n    2000      1.745982      1.745982      0.9844      0.9844      0.1569      24.3062\n    2050      1.670836      1.670836      0.9844      0.9844      0.1583      24.8439\n    2100      1.595367      1.595367      0.9844      0.9844      0.1591      25.3804\n    2150      1.519828      1.519828      0.9844      0.9844      0.1595      25.9158\n    2200      1.444423      1.444423      0.9844      0.9844      0.1595      26.4498\n    2250      1.369354      1.369354      0.9844      0.9844      0.1590      26.9820\n    2300      1.294820      1.294820      0.9865      0.9865      0.1580      27.5124\n    2350      1.221064      1.221064      0.9886      0.9886      0.1566      28.0407\n    2400      1.148333      1.148333      0.9906      0.9906      0.1547      28.5667\n    2450      1.076893      1.076893      0.9917      0.9917      0.1522      29.0904\n    2500      1.007010      1.007010      0.9917      0.9917      0.1490      29.6119\n    2550      0.938928      0.938928      0.9927      0.9927      0.1455      30.1309\n    2600      0.872845      0.872845      0.9948      0.9948      0.1415      30.6473\n    2650      0.808981      0.808981      0.9948      0.9948      0.1369      31.1608\n    2700      0.747558      0.747558      0.9958      0.9958      0.1320      31.6714\n    2750      0.688734      0.688734      0.9958      0.9958      0.1268      32.1783\n    2800      0.632618      0.632618      0.9969      0.9969      0.1212      32.6813\n    2850      0.579339      0.579339      0.9969      0.9969      0.1153      33.1804\n    2900      0.528942      0.528942      0.9990      0.9990      0.1094      33.6753\n    2950      0.481467      0.481467      0.9990      0.9990      0.1032      34.1661\n    3000      0.436916      0.436916      1.0000      1.0000      0.0971      34.6527\n    3050      0.395254      0.395254      1.0000      1.0000      0.0910      35.1350\n    3100      0.356426      0.356426      1.0000      1.0000      0.0850      35.6129\n    3150      0.320372      0.320372      1.0000      1.0000      0.0790      36.0870\n    3200      0.287025      0.287025      1.0000      1.0000      0.0733      36.5577\n    3250      0.256310      0.256310      1.0000      1.0000      0.0676      37.0257\n    3300      0.228136      0.228136      1.0000      1.0000      0.0621      37.4916\n    3350      0.202416      0.202416      1.0000      1.0000      0.0568      37.9563\n    3400      0.179039      0.179039      1.0000      1.0000      0.0517      38.4205\n    3450      0.157908      0.157908      1.0000      1.0000      0.0469      38.8850\n    3500      0.138891      0.138891      1.0000      1.0000      0.0423      39.3496\n    3550      0.121853      0.121853      1.0000      1.0000      0.0380      39.8147\n    3600      0.106648      0.106648      1.0000      1.0000      0.0340      40.2800\n    3650      0.093129      0.093129      1.0000      1.0000      0.0304      40.7454\n    3700      0.081151      0.081151      1.0000      1.0000      0.0270      41.2103\n    3750      0.070570      0.070570      1.0000      1.0000      0.0239      41.6745\n    3800      0.061248      0.061248      1.0000      1.0000      0.0211      42.1378\n    3850      0.053059      0.053059      1.0000      1.0000      0.0186      42.6003\n    3900      0.045884      0.045884      1.0000      1.0000      0.0164      43.0619\n    3950      0.039611      0.039611      1.0000      1.0000      0.0143      43.5224\n    4000      0.034141      0.034141      1.0000      1.0000      0.0125      43.9819\n    4050      0.029380      0.029380      1.0000      1.0000      0.0110      44.4403\n    4100      0.025244      0.025244      1.0000      1.0000      0.0095      44.8976\n    4150      0.021657      0.021657      1.0000      1.0000      0.0083      45.3537\n    4200      0.018552      0.018552      1.0000      1.0000      0.0072      45.8086\n    4250      0.015870      0.015870      1.0000      1.0000      0.0062      46.2623\n    4300      0.013556      0.013556      1.0000      1.0000      0.0054      46.7149\n    4350      0.011562      0.011562      1.0000      1.0000      0.0047      47.1664\n    4400      0.009848      0.009848      1.0000      1.0000      0.0040      47.6168\n    4450      0.008377      0.008377      1.0000      1.0000      0.0034      48.0661\n    4500      0.007116      0.007116      1.0000      1.0000      0.0030      48.5143\n    4550      0.006037      0.006037      1.0000      1.0000      0.0025      48.9611\n    4600      0.005115      0.005115      1.0000      1.0000      0.0022      49.4068\n    4650      0.004329      0.004329      1.0000      1.0000      0.0019      49.8516\n    4700      0.003658      0.003658      1.0000      1.0000      0.0016      50.2953\n    4750      0.003088      0.003088      1.0000      1.0000      0.0014      50.7379\n    4800      0.002604      0.002604      1.0000      1.0000      0.0012      51.1795\n    4850      0.002193      0.002193      1.0000      1.0000      0.0010      51.6200\n    4900      0.001844      0.001844      1.0000      1.0000      0.0008      52.0595\n    4950      0.001549      0.001549      1.0000      1.0000      0.0007      52.4978\n    4999      0.001305      0.001305      1.0000      1.0000      0.0006      52.9262\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.001305\n  Test Loss   = 0.001305\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 52.9262\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.433967, "test_losses": 3.433967, "train_accs": 0.03642, "test_accs": 0.03642, "grad_norms": 0.016042, "param_norms": 4.320604}, {"epoch": 50, "train_losses": 3.427824, "test_losses": 3.427824, "train_accs": 0.118626, "test_accs": 0.118626, "grad_norms": 0.015733, "param_norms": 4.378544}, {"epoch": 100, "train_losses": 3.42066, "test_losses": 3.42066, "train_accs": 0.289282, "test_accs": 0.289282, "grad_norms": 0.016647, "param_norms": 4.585708}, {"epoch": 150, "train_losses": 3.412555, "test_losses": 3.412555, "train_accs": 0.570239, "test_accs": 0.570239, "grad_norms": 0.01843, "param_norms": 4.900692}, {"epoch": 200, "train_losses": 3.403128, "test_losses": 3.403128, "train_accs": 0.74922, "test_accs": 0.74922, "grad_norms": 0.020985, "param_norms": 5.28654}, {"epoch": 250, "train_losses": 3.392056, "test_losses": 3.392056, "train_accs": 0.844953, "test_accs": 0.844953, "grad_norms": 0.024126, "param_norms": 5.717837}, {"epoch": 300, "train_losses": 3.379155, "test_losses": 3.379155, "train_accs": 0.907388, "test_accs": 0.907388, "grad_norms": 0.027649, "param_norms": 6.17855}, {"epoch": 350, "train_losses": 3.364295, "test_losses": 3.364295, "train_accs": 0.938606, "test_accs": 0.938606, "grad_norms": 0.03148, "param_norms": 6.658492}, {"epoch": 400, "train_losses": 3.347432, "test_losses": 3.347432, "train_accs": 0.962539, "test_accs": 0.962539, "grad_norms": 0.035411, "param_norms": 7.152044}, {"epoch": 450, "train_losses": 3.328524, "test_losses": 3.328524, "train_accs": 0.969823, "test_accs": 0.969823, "grad_norms": 0.039583, "param_norms": 7.655042}, {"epoch": 500, "train_losses": 3.307514, "test_losses": 3.307514, "train_accs": 0.979188, "test_accs": 0.979188, "grad_norms": 0.043864, "param_norms": 8.16488}, {"epoch": 550, "train_losses": 3.284378, "test_losses": 3.284378, "train_accs": 0.978148, "test_accs": 0.978148, "grad_norms": 0.048118, "param_norms": 8.680094}, {"epoch": 600, "train_losses": 3.259124, "test_losses": 3.259124, "train_accs": 0.977107, "test_accs": 0.977107, "grad_norms": 0.052456, "param_norms": 9.199633}, {"epoch": 650, "train_losses": 3.23174, "test_losses": 3.23174, "train_accs": 0.979188, "test_accs": 0.979188, "grad_norms": 0.056833, "param_norms": 9.722793}, {"epoch": 700, "train_losses": 3.202228, "test_losses": 3.202228, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.061217, "param_norms": 10.249294}, {"epoch": 750, "train_losses": 3.170569, "test_losses": 3.170569, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.06565, "param_norms": 10.778623}, {"epoch": 800, "train_losses": 3.136745, "test_losses": 3.136745, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.07008, "param_norms": 11.310487}, {"epoch": 850, "train_losses": 3.100763, "test_losses": 3.100763, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.074534, "param_norms": 11.844713}, {"epoch": 900, "train_losses": 3.062609, "test_losses": 3.062609, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.079028, "param_norms": 12.380989}, {"epoch": 950, "train_losses": 3.022317, "test_losses": 3.022317, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.08346, "param_norms": 12.918983}, {"epoch": 1000, "train_losses": 2.979927, "test_losses": 2.979927, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.087879, "param_norms": 13.458171}, {"epoch": 1050, "train_losses": 2.93542, "test_losses": 2.93542, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.092265, "param_norms": 13.998214}, {"epoch": 1100, "train_losses": 2.888808, "test_losses": 2.888808, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.09663, "param_norms": 14.539041}, {"epoch": 1150, "train_losses": 2.840152, "test_losses": 2.840152, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.101008, "param_norms": 15.080607}, {"epoch": 1200, "train_losses": 2.789488, "test_losses": 2.789488, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.105235, "param_norms": 15.623085}, {"epoch": 1250, "train_losses": 2.73683, "test_losses": 2.73683, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.109426, "param_norms": 16.166234}, {"epoch": 1300, "train_losses": 2.682209, "test_losses": 2.682209, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.113579, "param_norms": 16.70967}, {"epoch": 1350, "train_losses": 2.62564, "test_losses": 2.62564, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.117701, "param_norms": 17.25347}, {"epoch": 1400, "train_losses": 2.567185, "test_losses": 2.567185, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.121688, "param_norms": 17.797548}, {"epoch": 1450, "train_losses": 2.506899, "test_losses": 2.506899, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.1256, "param_norms": 18.341536}, {"epoch": 1500, "train_losses": 2.44483, "test_losses": 2.44483, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.129391, "param_norms": 18.885547}, {"epoch": 1550, "train_losses": 2.381073, "test_losses": 2.381073, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.133101, "param_norms": 19.429369}, {"epoch": 1600, "train_losses": 2.315698, "test_losses": 2.315698, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.13654, "param_norms": 19.972914}, {"epoch": 1650, "train_losses": 2.248785, "test_losses": 2.248785, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.139906, "param_norms": 20.516298}, {"epoch": 1700, "train_losses": 2.180366, "test_losses": 2.180366, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.143138, "param_norms": 21.059505}, {"epoch": 1750, "train_losses": 2.110604, "test_losses": 2.110604, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.146089, "param_norms": 21.602313}, {"epoch": 1800, "train_losses": 2.039637, "test_losses": 2.039637, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.148754, "param_norms": 22.144613}, {"epoch": 1850, "train_losses": 1.967574, "test_losses": 1.967574, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.151224, "param_norms": 22.68635}, {"epoch": 1900, "train_losses": 1.894513, "test_losses": 1.894513, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.153438, "param_norms": 23.227236}, {"epoch": 1950, "train_losses": 1.820601, "test_losses": 1.820601, "train_accs": 0.983351, "test_accs": 0.983351, "grad_norms": 0.15534, "param_norms": 23.767255}, {"epoch": 2000, "train_losses": 1.745982, "test_losses": 1.745982, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.156947, "param_norms": 24.30616}, {"epoch": 2050, "train_losses": 1.670836, "test_losses": 1.670836, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.158253, "param_norms": 24.843899}, {"epoch": 2100, "train_losses": 1.595367, "test_losses": 1.595367, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.159101, "param_norms": 25.380444}, {"epoch": 2150, "train_losses": 1.519828, "test_losses": 1.519828, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.159523, "param_norms": 25.915804}, {"epoch": 2200, "train_losses": 1.444423, "test_losses": 1.444423, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.15951, "param_norms": 26.449794}, {"epoch": 2250, "train_losses": 1.369354, "test_losses": 1.369354, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.158969, "param_norms": 26.98199}, {"epoch": 2300, "train_losses": 1.29482, "test_losses": 1.29482, "train_accs": 0.986472, "test_accs": 0.986472, "grad_norms": 0.158037, "param_norms": 27.512421}, {"epoch": 2350, "train_losses": 1.221064, "test_losses": 1.221064, "train_accs": 0.988554, "test_accs": 0.988554, "grad_norms": 0.156575, "param_norms": 28.040684}, {"epoch": 2400, "train_losses": 1.148333, "test_losses": 1.148333, "train_accs": 0.990635, "test_accs": 0.990635, "grad_norms": 0.154659, "param_norms": 28.566708}, {"epoch": 2450, "train_losses": 1.076893, "test_losses": 1.076893, "train_accs": 0.991675, "test_accs": 0.991675, "grad_norms": 0.152151, "param_norms": 29.09036}, {"epoch": 2500, "train_losses": 1.00701, "test_losses": 1.00701, "train_accs": 0.991675, "test_accs": 0.991675, "grad_norms": 0.149025, "param_norms": 29.61185}, {"epoch": 2550, "train_losses": 0.938928, "test_losses": 0.938928, "train_accs": 0.992716, "test_accs": 0.992716, "grad_norms": 0.14552, "param_norms": 30.130903}, {"epoch": 2600, "train_losses": 0.872845, "test_losses": 0.872845, "train_accs": 0.994797, "test_accs": 0.994797, "grad_norms": 0.141541, "param_norms": 30.647263}, {"epoch": 2650, "train_losses": 0.808981, "test_losses": 0.808981, "train_accs": 0.994797, "test_accs": 0.994797, "grad_norms": 0.136948, "param_norms": 31.160846}, {"epoch": 2700, "train_losses": 0.747558, "test_losses": 0.747558, "train_accs": 0.995838, "test_accs": 0.995838, "grad_norms": 0.132035, "param_norms": 31.671403}, {"epoch": 2750, "train_losses": 0.688734, "test_losses": 0.688734, "train_accs": 0.995838, "test_accs": 0.995838, "grad_norms": 0.126793, "param_norms": 32.178323}, {"epoch": 2800, "train_losses": 0.632618, "test_losses": 0.632618, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.121173, "param_norms": 32.681304}, {"epoch": 2850, "train_losses": 0.579339, "test_losses": 0.579339, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.115282, "param_norms": 33.180419}, {"epoch": 2900, "train_losses": 0.528942, "test_losses": 0.528942, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.109362, "param_norms": 33.675273}, {"epoch": 2950, "train_losses": 0.481467, "test_losses": 0.481467, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.103242, "param_norms": 34.166096}, {"epoch": 3000, "train_losses": 0.436916, "test_losses": 0.436916, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.097136, "param_norms": 34.652655}, {"epoch": 3050, "train_losses": 0.395254, "test_losses": 0.395254, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.090967, "param_norms": 35.13498}, {"epoch": 3100, "train_losses": 0.356426, "test_losses": 0.356426, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.084982, "param_norms": 35.612927}, {"epoch": 3150, "train_losses": 0.320372, "test_losses": 0.320372, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.079043, "param_norms": 36.086987}, {"epoch": 3200, "train_losses": 0.287025, "test_losses": 0.287025, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.073255, "param_norms": 36.55775}, {"epoch": 3250, "train_losses": 0.25631, "test_losses": 0.25631, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.06759, "param_norms": 37.025741}, {"epoch": 3300, "train_losses": 0.228136, "test_losses": 0.228136, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.062115, "param_norms": 37.491645}, {"epoch": 3350, "train_losses": 0.202416, "test_losses": 0.202416, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.056803, "param_norms": 37.956251}, {"epoch": 3400, "train_losses": 0.179039, "test_losses": 0.179039, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.051718, "param_norms": 38.420511}, {"epoch": 3450, "train_losses": 0.157908, "test_losses": 0.157908, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.046896, "param_norms": 38.884956}, {"epoch": 3500, "train_losses": 0.138891, "test_losses": 0.138891, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.04232, "param_norms": 39.349611}, {"epoch": 3550, "train_losses": 0.121853, "test_losses": 0.121853, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.038034, "param_norms": 39.814663}, {"epoch": 3600, "train_losses": 0.106648, "test_losses": 0.106648, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.034047, "param_norms": 40.280043}, {"epoch": 3650, "train_losses": 0.093129, "test_losses": 0.093129, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.03038, "param_norms": 40.7454}, {"epoch": 3700, "train_losses": 0.081151, "test_losses": 0.081151, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.027007, "param_norms": 41.210332}, {"epoch": 3750, "train_losses": 0.07057, "test_losses": 0.07057, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.023932, "param_norms": 41.674477}, {"epoch": 3800, "train_losses": 0.061248, "test_losses": 0.061248, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.021138, "param_norms": 42.137814}, {"epoch": 3850, "train_losses": 0.053059, "test_losses": 0.053059, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.018636, "param_norms": 42.600305}, {"epoch": 3900, "train_losses": 0.045884, "test_losses": 0.045884, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016367, "param_norms": 43.061898}, {"epoch": 3950, "train_losses": 0.039611, "test_losses": 0.039611, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014338, "param_norms": 43.5224}, {"epoch": 4000, "train_losses": 0.034141, "test_losses": 0.034141, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012537, "param_norms": 43.981901}, {"epoch": 4050, "train_losses": 0.02938, "test_losses": 0.02938, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010953, "param_norms": 44.440273}, {"epoch": 4100, "train_losses": 0.025244, "test_losses": 0.025244, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009535, "param_norms": 44.897564}, {"epoch": 4150, "train_losses": 0.021657, "test_losses": 0.021657, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00829, "param_norms": 45.353697}, {"epoch": 4200, "train_losses": 0.018552, "test_losses": 0.018552, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007189, "param_norms": 45.808558}, {"epoch": 4250, "train_losses": 0.01587, "test_losses": 0.01587, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006226, "param_norms": 46.262309}, {"epoch": 4300, "train_losses": 0.013556, "test_losses": 0.013556, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005388, "param_norms": 46.714893}, {"epoch": 4350, "train_losses": 0.011562, "test_losses": 0.011562, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004654, "param_norms": 47.166412}, {"epoch": 4400, "train_losses": 0.009848, "test_losses": 0.009848, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004006, "param_norms": 47.616849}, {"epoch": 4450, "train_losses": 0.008377, "test_losses": 0.008377, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003442, "param_norms": 48.066108}, {"epoch": 4500, "train_losses": 0.007116, "test_losses": 0.007116, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002958, "param_norms": 48.514257}, {"epoch": 4550, "train_losses": 0.006037, "test_losses": 0.006037, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002538, "param_norms": 48.961083}, {"epoch": 4600, "train_losses": 0.005115, "test_losses": 0.005115, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002173, "param_norms": 49.406797}, {"epoch": 4650, "train_losses": 0.004329, "test_losses": 0.004329, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001858, "param_norms": 49.851575}, {"epoch": 4700, "train_losses": 0.003658, "test_losses": 0.003658, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001586, "param_norms": 50.295266}, {"epoch": 4750, "train_losses": 0.003088, "test_losses": 0.003088, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001354, "param_norms": 50.737867}, {"epoch": 4800, "train_losses": 0.002604, "test_losses": 0.002604, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001153, "param_norms": 51.179478}, {"epoch": 4850, "train_losses": 0.002193, "test_losses": 0.002193, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000981, "param_norms": 51.620022}, {"epoch": 4900, "train_losses": 0.001844, "test_losses": 0.001844, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000832, "param_norms": 52.059458}, {"epoch": 4950, "train_losses": 0.001549, "test_losses": 0.001549, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000705, "param_norms": 52.497843}, {"epoch": 4999, "train_losses": 0.001305, "test_losses": 0.001305, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0006, "param_norms": 52.926193}], "total_epochs": 5000}, "grokking": {"config": {"prime": 31, "d_mlp": 931, "act_type": "ReLU", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 0.0001, "weight_decay": 2.0, "frac_train": 0.75, "num_epochs": 50000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=31, run=grokking\n======================================================================\n\nConfiguration:\n  prime (p)       = 31\n  d_mlp           = 931\n  activation      = ReLU\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 0.0001\n  weight_decay    = 2.0\n  frac_train      = 0.75\n  num_epochs      = 50000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.433796      3.434476      0.0347      0.0415      0.0217       4.3206\n     160      3.359693      3.500791      0.2722      0.0000      0.0328       6.1154\n     320      3.218821      3.572307      0.5458      0.0000      0.0553       9.2510\n     480      3.012760      3.611321      0.7028      0.0000      0.0785      12.4257\n     640      2.750883      3.610343      0.7236      0.0083      0.0999      15.5184\n     800      2.445758      3.565212      0.7319      0.0166      0.1181      18.5032\n     960      2.112228      3.469792      0.7722      0.1079      0.1320      21.3553\n    1120      1.766802      3.312002      0.8458      0.3154      0.1397      24.0447\n    1280      1.427801      3.083353      0.9403      0.5892      0.1404      26.5556\n    1440      1.111195      2.784705      0.9931      0.7427      0.1341      28.8853\n    1600      0.828393      2.434698      1.0000      0.7635      0.1222      31.0509\n    1760      0.587308      2.070415      1.0000      0.7635      0.1057      33.0822\n    1920      0.394491      1.741714      1.0000      0.7635      0.0855      35.0143\n    2080      0.252681      1.483562      1.0000      0.7635      0.0640      36.8698\n    2240      0.156420      1.297863      1.0000      0.7635      0.0448      38.6493\n    2400      0.094781      1.166014      1.0000      0.7635      0.0298      40.3554\n    2560      0.056719      1.067707      1.0000      0.7635      0.0192      41.9878\n    2720      0.033678      0.988995      1.0000      0.7718      0.0121      43.5497\n    2880      0.019910      0.922261      1.0000      0.7801      0.0075      45.0486\n    3040      0.011746      0.861412      1.0000      0.7884      0.0046      46.4926\n    3200      0.006922      0.803099      1.0000      0.7884      0.0028      47.8852\n    3360      0.004082      0.745924      1.0000      0.7967      0.0017      49.2300\n    3520      0.002410      0.689070      1.0000      0.8133      0.0011      50.5294\n    3680      0.001427      0.633991      1.0000      0.8382      0.0006      51.7844\n    3840      0.000849      0.581244      1.0000      0.8382      0.0004      52.9950\n    4000      0.000507      0.531472      1.0000      0.8631      0.0002      54.1590\n    4160      0.000305      0.484340      1.0000      0.9046      0.0002      55.2705\n    4320      0.000186      0.439543      1.0000      0.9129      0.0001      56.3216\n    4480      0.000115      0.397230      1.0000      0.9295      0.0001      57.3020\n    4640      0.000072      0.357695      1.0000      0.9295      0.0000      58.1998\n    4800      0.000047      0.321501      1.0000      0.9295      0.0000      58.9991\n    4960      0.000031      0.288838      1.0000      0.9378      0.0000      59.6846\n    5120      0.000022      0.259378      1.0000      0.9544      0.0000      60.2443\n    5280      0.000016      0.233029      1.0000      0.9627      0.0000      60.6737\n    5440      0.000012      0.209636      1.0000      0.9627      0.0000      60.9776\n    5600      0.000010      0.189324      1.0000      0.9710      0.0000      61.1687\n    5760      0.000009      0.171648      1.0000      0.9793      0.0000      61.2644\n    5920      0.000008      0.156484      1.0000      0.9793      0.0000      61.2849\n    6080      0.000007      0.143366      1.0000      0.9793      0.0000      61.2492\n    6240      0.000007      0.131991      1.0000      0.9793      0.0000      61.1731\n    6400      0.000006      0.122066      1.0000      0.9793      0.0000      61.0693\n    6560      0.000006      0.113249      1.0000      0.9793      0.0000      60.9470\n    6720      0.000006      0.105409      1.0000      0.9793      0.0000      60.8149\n    6880      0.000006      0.098430      1.0000      0.9793      0.0000      60.6764\n    7040      0.000006      0.092103      1.0000      0.9793      0.0000      60.5371\n    7200      0.000006      0.086490      1.0000      0.9793      0.0000      60.3994\n    7360      0.000006      0.081466      1.0000      0.9834      0.0000      60.2638\n    7520      0.000006      0.076916      1.0000      0.9834      0.0000      60.1325\n    7680      0.000006      0.072821      1.0000      0.9876      0.0000      60.0061\n    7840      0.000005      0.069113      1.0000      0.9876      0.0000      59.8845\n    8000      0.000005      0.065706      1.0000      0.9917      0.0000      59.7673\n    8160      0.000005      0.062567      1.0000      0.9917      0.0000      59.6547\n    8320      0.000005      0.059696      1.0000      0.9917      0.0000      59.5468\n    8480      0.000005      0.057074      1.0000      0.9917      0.0000      59.4436\n    8640      0.000005      0.054709      1.0000      0.9917      0.0000      59.3442\n    8800      0.000005      0.052568      1.0000      0.9917      0.0000      59.2491\n    8960      0.000005      0.050616      1.0000      0.9917      0.0000      59.1580\n    9120      0.000005      0.048814      1.0000      0.9959      0.0000      59.0704\n    9280      0.000005      0.047148      1.0000      0.9959      0.0000      58.9863\n    9440      0.000005      0.045596      1.0000      0.9959      0.0000      58.9053\n    9600      0.000005      0.044132      1.0000      0.9959      0.0000      58.8273\n    9760      0.000005      0.042768      1.0000      0.9959      0.0000      58.7530\n    9920      0.000005      0.041482      1.0000      1.0000      0.0000      58.6815\n   10080      0.000005      0.040319      1.0000      1.0000      0.0000      58.6130\n   10240      0.000005      0.039224      1.0000      1.0000      0.0000      58.5472\n   10400      0.000005      0.038240      1.0000      1.0000      0.0000      58.4839\n   10560      0.000005      0.037323      1.0000      1.0000      0.0000      58.4230\n   10720      0.000005      0.036500      1.0000      1.0000      0.0000      58.3638\n   10880      0.000005      0.035764      1.0000      1.0000      0.0000      58.3065\n   11040      0.000005      0.035054      1.0000      1.0000      0.0000      58.2516\n   11200      0.000005      0.034345      1.0000      1.0000      0.0000      58.1985\n   11360      0.000005      0.033665      1.0000      1.0000      0.0000      58.1474\n   11520      0.000005      0.033017      1.0000      1.0000      0.0000      58.0979\n   11680      0.000005      0.032421      1.0000      1.0000      0.0000      58.0500\n   11840      0.000005      0.031865      1.0000      1.0000      0.0000      58.0038\n   12000      0.000005      0.031361      1.0000      1.0000      0.0000      57.9597\n   12160      0.000005      0.030912      1.0000      1.0000      0.0000      57.9169\n   12320      0.000005      0.030486      1.0000      1.0000      0.0000      57.8760\n   12480      0.000005      0.030079      1.0000      1.0000      0.0000      57.8365\n   12640      0.000005      0.029694      1.0000      1.0000      0.0000      57.7982\n   12800      0.000005      0.029370      1.0000      1.0000      0.0000      57.7614\n   12960      0.000005      0.029062      1.0000      1.0000      0.0000      57.7262\n   13120      0.000005      0.028754      1.0000      1.0000      0.0000      57.6923\n   13280      0.000005      0.028471      1.0000      1.0000      0.0000      57.6595\n   13440      0.000005      0.028204      1.0000      1.0000      0.0000      57.6280\n   13600      0.000005      0.027951      1.0000      1.0000      0.0000      57.5975\n   13760      0.000005      0.027714      1.0000      1.0000      0.0000      57.5682\n   13920      0.000005      0.027492      1.0000      1.0000      0.0000      57.5396\n   14080      0.000005      0.027270      1.0000      1.0000      0.0000      57.5123\n   14240      0.000005      0.027068      1.0000      1.0000      0.0000      57.4860\n   14400      0.000005      0.026880      1.0000      1.0000      0.0000      57.4606\n   14560      0.000005      0.026706      1.0000      1.0000      0.0000      57.4363\n   14720      0.000005      0.026547      1.0000      1.0000      0.0000      57.4125\n   14880      0.000005      0.026410      1.0000      1.0000      0.0000      57.3900\n   15040      0.000005      0.026292      1.0000      1.0000      0.0000      57.3687\n   15200      0.000005      0.026169      1.0000      1.0000      0.0000      57.3484\n   15360      0.000005      0.026044      1.0000      1.0000      0.0000      57.3286\n   15520      0.000005      0.025927      1.0000      1.0000      0.0000      57.3093\n   15680      0.000005      0.025806      1.0000      1.0000      0.0000      57.2910\n   15840      0.000005      0.025702      1.0000      1.0000      0.0000      57.2734\n   16000      0.000005      0.025626      1.0000      1.0000      0.0000      57.2563\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.000005\n  Test Loss   = 0.025626\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 57.2563\n\nTotal epochs trained: 16001\n", "table": [{"epoch": 0, "train_losses": 3.433796, "test_losses": 3.434476, "train_accs": 0.034722, "test_accs": 0.041494, "grad_norms": 0.021734, "param_norms": 4.320604}, {"epoch": 160, "train_losses": 3.359693, "test_losses": 3.500791, "train_accs": 0.272222, "test_accs": 0.0, "grad_norms": 0.032828, "param_norms": 6.11541}, {"epoch": 320, "train_losses": 3.218821, "test_losses": 3.572307, "train_accs": 0.545833, "test_accs": 0.0, "grad_norms": 0.055327, "param_norms": 9.25099}, {"epoch": 480, "train_losses": 3.01276, "test_losses": 3.611321, "train_accs": 0.702778, "test_accs": 0.0, "grad_norms": 0.078507, "param_norms": 12.425744}, {"epoch": 640, "train_losses": 2.750883, "test_losses": 3.610343, "train_accs": 0.723611, "test_accs": 0.008299, "grad_norms": 0.099917, "param_norms": 15.518415}, {"epoch": 800, "train_losses": 2.445758, "test_losses": 3.565212, "train_accs": 0.731944, "test_accs": 0.016598, "grad_norms": 0.118116, "param_norms": 18.503197}, {"epoch": 960, "train_losses": 2.112228, "test_losses": 3.469792, "train_accs": 0.772222, "test_accs": 0.107884, "grad_norms": 0.131998, "param_norms": 21.355289}, {"epoch": 1120, "train_losses": 1.766802, "test_losses": 3.312002, "train_accs": 0.845833, "test_accs": 0.315353, "grad_norms": 0.139742, "param_norms": 24.044698}, {"epoch": 1280, "train_losses": 1.427801, "test_losses": 3.083353, "train_accs": 0.940278, "test_accs": 0.589212, "grad_norms": 0.140423, "param_norms": 26.555597}, {"epoch": 1440, "train_losses": 1.111195, "test_losses": 2.784705, "train_accs": 0.993056, "test_accs": 0.742739, "grad_norms": 0.134079, "param_norms": 28.885275}, {"epoch": 1600, "train_losses": 0.828393, "test_losses": 2.434698, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.122154, "param_norms": 31.050878}, {"epoch": 1760, "train_losses": 0.587308, "test_losses": 2.070415, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.105654, "param_norms": 33.082213}, {"epoch": 1920, "train_losses": 0.394491, "test_losses": 1.741714, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.085478, "param_norms": 35.014293}, {"epoch": 2080, "train_losses": 0.252681, "test_losses": 1.483562, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.064044, "param_norms": 36.869793}, {"epoch": 2240, "train_losses": 0.15642, "test_losses": 1.297863, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.044774, "param_norms": 38.64934}, {"epoch": 2400, "train_losses": 0.094781, "test_losses": 1.166014, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.029778, "param_norms": 40.355413}, {"epoch": 2560, "train_losses": 0.056719, "test_losses": 1.067707, "train_accs": 1.0, "test_accs": 0.763485, "grad_norms": 0.019182, "param_norms": 41.987839}, {"epoch": 2720, "train_losses": 0.033678, "test_losses": 0.988995, "train_accs": 1.0, "test_accs": 0.771784, "grad_norms": 0.012099, "param_norms": 43.549695}, {"epoch": 2880, "train_losses": 0.01991, "test_losses": 0.922261, "train_accs": 1.0, "test_accs": 0.780083, "grad_norms": 0.007514, "param_norms": 45.0486}, {"epoch": 3040, "train_losses": 0.011746, "test_losses": 0.861412, "train_accs": 1.0, "test_accs": 0.788382, "grad_norms": 0.004634, "param_norms": 46.492592}, {"epoch": 3200, "train_losses": 0.006922, "test_losses": 0.803099, "train_accs": 1.0, "test_accs": 0.788382, "grad_norms": 0.002844, "param_norms": 47.885233}, {"epoch": 3360, "train_losses": 0.004082, "test_losses": 0.745924, "train_accs": 1.0, "test_accs": 0.79668, "grad_norms": 0.001737, "param_norms": 49.23001}, {"epoch": 3520, "train_losses": 0.00241, "test_losses": 0.68907, "train_accs": 1.0, "test_accs": 0.813278, "grad_norms": 0.00106, "param_norms": 50.529434}, {"epoch": 3680, "train_losses": 0.001427, "test_losses": 0.633991, "train_accs": 1.0, "test_accs": 0.838174, "grad_norms": 0.000648, "param_norms": 51.784414}, {"epoch": 3840, "train_losses": 0.000849, "test_losses": 0.581244, "train_accs": 1.0, "test_accs": 0.838174, "grad_norms": 0.000396, "param_norms": 52.995014}, {"epoch": 4000, "train_losses": 0.000507, "test_losses": 0.531472, "train_accs": 1.0, "test_accs": 0.863071, "grad_norms": 0.000243, "param_norms": 54.15903}, {"epoch": 4160, "train_losses": 0.000305, "test_losses": 0.48434, "train_accs": 1.0, "test_accs": 0.904564, "grad_norms": 0.00015, "param_norms": 55.270453}, {"epoch": 4320, "train_losses": 0.000186, "test_losses": 0.439543, "train_accs": 1.0, "test_accs": 0.912863, "grad_norms": 9.4e-05, "param_norms": 56.321627}, {"epoch": 4480, "train_losses": 0.000115, "test_losses": 0.39723, "train_accs": 1.0, "test_accs": 0.929461, "grad_norms": 5.9e-05, "param_norms": 57.302012}, {"epoch": 4640, "train_losses": 7.2e-05, "test_losses": 0.357695, "train_accs": 1.0, "test_accs": 0.929461, "grad_norms": 3.8e-05, "param_norms": 58.199848}, {"epoch": 4800, "train_losses": 4.7e-05, "test_losses": 0.321501, "train_accs": 1.0, "test_accs": 0.929461, "grad_norms": 2.5e-05, "param_norms": 58.999085}, {"epoch": 4960, "train_losses": 3.1e-05, "test_losses": 0.288838, "train_accs": 1.0, "test_accs": 0.937759, "grad_norms": 1.7e-05, "param_norms": 59.684594}, {"epoch": 5120, "train_losses": 2.2e-05, "test_losses": 0.259378, "train_accs": 1.0, "test_accs": 0.954357, "grad_norms": 1.2e-05, "param_norms": 60.244335}, {"epoch": 5280, "train_losses": 1.6e-05, "test_losses": 0.233029, "train_accs": 1.0, "test_accs": 0.962656, "grad_norms": 9e-06, "param_norms": 60.673715}, {"epoch": 5440, "train_losses": 1.2e-05, "test_losses": 0.209636, "train_accs": 1.0, "test_accs": 0.962656, "grad_norms": 7e-06, "param_norms": 60.977591}, {"epoch": 5600, "train_losses": 1e-05, "test_losses": 0.189324, "train_accs": 1.0, "test_accs": 0.970954, "grad_norms": 6e-06, "param_norms": 61.168747}, {"epoch": 5760, "train_losses": 9e-06, "test_losses": 0.171648, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 5e-06, "param_norms": 61.264434}, {"epoch": 5920, "train_losses": 8e-06, "test_losses": 0.156484, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 4e-06, "param_norms": 61.284897}, {"epoch": 6080, "train_losses": 7e-06, "test_losses": 0.143366, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 4e-06, "param_norms": 61.249249}, {"epoch": 6240, "train_losses": 7e-06, "test_losses": 0.131991, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 4e-06, "param_norms": 61.173149}, {"epoch": 6400, "train_losses": 6e-06, "test_losses": 0.122066, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 4e-06, "param_norms": 61.069294}, {"epoch": 6560, "train_losses": 6e-06, "test_losses": 0.113249, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 3e-06, "param_norms": 60.947022}, {"epoch": 6720, "train_losses": 6e-06, "test_losses": 0.105409, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 3e-06, "param_norms": 60.814913}, {"epoch": 6880, "train_losses": 6e-06, "test_losses": 0.09843, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 3e-06, "param_norms": 60.676395}, {"epoch": 7040, "train_losses": 6e-06, "test_losses": 0.092103, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 3e-06, "param_norms": 60.537098}, {"epoch": 7200, "train_losses": 6e-06, "test_losses": 0.08649, "train_accs": 1.0, "test_accs": 0.979253, "grad_norms": 3e-06, "param_norms": 60.399366}, {"epoch": 7360, "train_losses": 6e-06, "test_losses": 0.081466, "train_accs": 1.0, "test_accs": 0.983402, "grad_norms": 3e-06, "param_norms": 60.26378}, {"epoch": 7520, "train_losses": 6e-06, "test_losses": 0.076916, "train_accs": 1.0, "test_accs": 0.983402, "grad_norms": 3e-06, "param_norms": 60.132504}, {"epoch": 7680, "train_losses": 6e-06, "test_losses": 0.072821, "train_accs": 1.0, "test_accs": 0.987552, "grad_norms": 3e-06, "param_norms": 60.006086}, {"epoch": 7840, "train_losses": 5e-06, "test_losses": 0.069113, "train_accs": 1.0, "test_accs": 0.987552, "grad_norms": 3e-06, "param_norms": 59.884452}, {"epoch": 8000, "train_losses": 5e-06, "test_losses": 0.065706, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.767275}, {"epoch": 8160, "train_losses": 5e-06, "test_losses": 0.062567, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.654717}, {"epoch": 8320, "train_losses": 5e-06, "test_losses": 0.059696, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.546846}, {"epoch": 8480, "train_losses": 5e-06, "test_losses": 0.057074, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.443583}, {"epoch": 8640, "train_losses": 5e-06, "test_losses": 0.054709, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.344189}, {"epoch": 8800, "train_losses": 5e-06, "test_losses": 0.052568, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.249091}, {"epoch": 8960, "train_losses": 5e-06, "test_losses": 0.050616, "train_accs": 1.0, "test_accs": 0.991701, "grad_norms": 3e-06, "param_norms": 59.158045}, {"epoch": 9120, "train_losses": 5e-06, "test_losses": 0.048814, "train_accs": 1.0, "test_accs": 0.995851, "grad_norms": 3e-06, "param_norms": 59.070374}, {"epoch": 9280, "train_losses": 5e-06, "test_losses": 0.047148, "train_accs": 1.0, "test_accs": 0.995851, "grad_norms": 3e-06, "param_norms": 58.986265}, {"epoch": 9440, "train_losses": 5e-06, "test_losses": 0.045596, "train_accs": 1.0, "test_accs": 0.995851, "grad_norms": 3e-06, "param_norms": 58.905258}, {"epoch": 9600, "train_losses": 5e-06, "test_losses": 0.044132, "train_accs": 1.0, "test_accs": 0.995851, "grad_norms": 3e-06, "param_norms": 58.827261}, {"epoch": 9760, "train_losses": 5e-06, "test_losses": 0.042768, "train_accs": 1.0, "test_accs": 0.995851, "grad_norms": 3e-06, "param_norms": 58.753029}, {"epoch": 9920, "train_losses": 5e-06, "test_losses": 0.041482, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.681505}, {"epoch": 10080, "train_losses": 5e-06, "test_losses": 0.040319, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.612996}, {"epoch": 10240, "train_losses": 5e-06, "test_losses": 0.039224, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.54716}, {"epoch": 10400, "train_losses": 5e-06, "test_losses": 0.03824, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.483947}, {"epoch": 10560, "train_losses": 5e-06, "test_losses": 0.037323, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.423}, {"epoch": 10720, "train_losses": 5e-06, "test_losses": 0.0365, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.36379}, {"epoch": 10880, "train_losses": 5e-06, "test_losses": 0.035764, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.306498}, {"epoch": 11040, "train_losses": 5e-06, "test_losses": 0.035054, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.251603}, {"epoch": 11200, "train_losses": 5e-06, "test_losses": 0.034345, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.198483}, {"epoch": 11360, "train_losses": 5e-06, "test_losses": 0.033665, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.147399}, {"epoch": 11520, "train_losses": 5e-06, "test_losses": 0.033017, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.097886}, {"epoch": 11680, "train_losses": 5e-06, "test_losses": 0.032421, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.04999}, {"epoch": 11840, "train_losses": 5e-06, "test_losses": 0.031865, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 58.003801}, {"epoch": 12000, "train_losses": 5e-06, "test_losses": 0.031361, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.959658}, {"epoch": 12160, "train_losses": 5e-06, "test_losses": 0.030912, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.916896}, {"epoch": 12320, "train_losses": 5e-06, "test_losses": 0.030486, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.875984}, {"epoch": 12480, "train_losses": 5e-06, "test_losses": 0.030079, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.836528}, {"epoch": 12640, "train_losses": 5e-06, "test_losses": 0.029694, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.798203}, {"epoch": 12800, "train_losses": 5e-06, "test_losses": 0.02937, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.76137}, {"epoch": 12960, "train_losses": 5e-06, "test_losses": 0.029062, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.726206}, {"epoch": 13120, "train_losses": 5e-06, "test_losses": 0.028754, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.692328}, {"epoch": 13280, "train_losses": 5e-06, "test_losses": 0.028471, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.659509}, {"epoch": 13440, "train_losses": 5e-06, "test_losses": 0.028204, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.627987}, {"epoch": 13600, "train_losses": 5e-06, "test_losses": 0.027951, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.59753}, {"epoch": 13760, "train_losses": 5e-06, "test_losses": 0.027714, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.568181}, {"epoch": 13920, "train_losses": 5e-06, "test_losses": 0.027492, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.539588}, {"epoch": 14080, "train_losses": 5e-06, "test_losses": 0.02727, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.512271}, {"epoch": 14240, "train_losses": 5e-06, "test_losses": 0.027068, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.485976}, {"epoch": 14400, "train_losses": 5e-06, "test_losses": 0.02688, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.460641}, {"epoch": 14560, "train_losses": 5e-06, "test_losses": 0.026706, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.436292}, {"epoch": 14720, "train_losses": 5e-06, "test_losses": 0.026547, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.412488}, {"epoch": 14880, "train_losses": 5e-06, "test_losses": 0.02641, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.389999}, {"epoch": 15040, "train_losses": 5e-06, "test_losses": 0.026292, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.368741}, {"epoch": 15200, "train_losses": 5e-06, "test_losses": 0.026169, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.348428}, {"epoch": 15360, "train_losses": 5e-06, "test_losses": 0.026044, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.328604}, {"epoch": 15520, "train_losses": 5e-06, "test_losses": 0.025927, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.309324}, {"epoch": 15680, "train_losses": 5e-06, "test_losses": 0.025806, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.290981}, {"epoch": 15840, "train_losses": 5e-06, "test_losses": 0.025702, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.27344}, {"epoch": 16000, "train_losses": 5e-06, "test_losses": 0.025626, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 3e-06, "param_norms": 57.256305}], "total_epochs": 16001}, "quad_random": {"config": {"prime": 31, "d_mlp": 931, "act_type": "Quad", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 5e-05, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=31, run=quad_random\n======================================================================\n\nConfiguration:\n  prime (p)       = 31\n  d_mlp           = 931\n  activation      = Quad\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 5e-05\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.433976      3.433976      0.0281      0.0281      0.0015       4.3206\n      50      3.433318      3.433318      0.1426      0.1426      0.0016       4.3996\n     100      3.432411      3.432411      0.4422      0.4422      0.0020       4.6612\n     150      3.431169      3.431169      0.7336      0.7336      0.0026       5.0426\n     200      3.429463      3.429463      0.8970      0.8970      0.0035       5.4946\n     250      3.427181      3.427181      0.9719      0.9719      0.0046       5.9861\n     300      3.424223      3.424223      0.9813      0.9813      0.0059       6.5006\n     350      3.420488      3.420488      0.9927      0.9927      0.0073       7.0288\n     400      3.415882      3.415882      0.9990      0.9990      0.0090       7.5657\n     450      3.410305      3.410305      0.9990      0.9990      0.0109       8.1086\n     500      3.403662      3.403662      0.9990      0.9990      0.0130       8.6560\n     550      3.395854      3.395854      0.9990      0.9990      0.0153       9.2074\n     600      3.386791      3.386791      0.9990      0.9990      0.0178       9.7628\n     650      3.376379      3.376379      0.9979      0.9979      0.0204      10.3217\n     700      3.364527      3.364527      0.9979      0.9979      0.0233      10.8830\n     750      3.351143      3.351143      0.9979      0.9979      0.0263      11.4464\n     800      3.336136      3.336136      0.9990      0.9990      0.0295      12.0120\n     850      3.319419      3.319419      0.9990      0.9990      0.0329      12.5793\n     900      3.300901      3.300901      0.9990      0.9990      0.0364      13.1485\n     950      3.280498      3.280498      0.9990      0.9990      0.0402      13.7193\n    1000      3.258123      3.258123      0.9990      0.9990      0.0441      14.2916\n    1050      3.233696      3.233696      0.9990      0.9990      0.0482      14.8654\n    1100      3.207133      3.207133      0.9990      0.9990      0.0524      15.4407\n    1150      3.178358      3.178358      0.9990      0.9990      0.0568      16.0173\n    1200      3.147292      3.147292      0.9990      0.9990      0.0614      16.5951\n    1250      3.113861      3.113861      1.0000      1.0000      0.0661      17.1740\n    1300      3.077994      3.077994      1.0000      1.0000      0.0710      17.7539\n    1350      3.039621      3.039621      1.0000      1.0000      0.0760      18.3347\n    1400      2.998676      2.998676      1.0000      1.0000      0.0811      18.9162\n    1450      2.955099      2.955099      1.0000      1.0000      0.0864      19.4985\n    1500      2.908832      2.908832      1.0000      1.0000      0.0918      20.0814\n    1550      2.859824      2.859824      1.0000      1.0000      0.0973      20.6648\n    1600      2.808027      2.808027      1.0000      1.0000      0.1029      21.2484\n    1650      2.753402      2.753402      1.0000      1.0000      0.1086      21.8323\n    1700      2.695916      2.695916      1.0000      1.0000      0.1144      22.4163\n    1750      2.635547      2.635547      1.0000      1.0000      0.1202      23.0003\n    1800      2.572280      2.572280      1.0000      1.0000      0.1260      23.5843\n    1850      2.506116      2.506116      1.0000      1.0000      0.1319      24.1682\n    1900      2.437068      2.437068      1.0000      1.0000      0.1377      24.7522\n    1950      2.365163      2.365163      1.0000      1.0000      0.1436      25.3361\n    2000      2.290450      2.290450      1.0000      1.0000      0.1493      25.9199\n    2050      2.212999      2.212999      1.0000      1.0000      0.1549      26.5034\n    2100      2.132903      2.132903      1.0000      1.0000      0.1603      27.0865\n    2150      2.050286      2.050286      1.0000      1.0000      0.1655      27.6691\n    2200      1.965298      1.965298      1.0000      1.0000      0.1705      28.2511\n    2250      1.878130      1.878130      1.0000      1.0000      0.1750      28.8322\n    2300      1.789011      1.789011      1.0000      1.0000      0.1792      29.4124\n    2350      1.698212      1.698212      1.0000      1.0000      0.1828      29.9913\n    2400      1.606053      1.606053      1.0000      1.0000      0.1859      30.5688\n    2450      1.512903      1.512903      1.0000      1.0000      0.1882      31.1443\n    2500      1.419181      1.419181      1.0000      1.0000      0.1898      31.7176\n    2550      1.325357      1.325357      1.0000      1.0000      0.1904      32.2887\n    2600      1.231949      1.231949      1.0000      1.0000      0.1901      32.8570\n    2650      1.139515      1.139515      1.0000      1.0000      0.1887      33.4223\n    2700      1.048646      1.048646      1.0000      1.0000      0.1862      33.9840\n    2750      0.959943      0.959943      1.0000      1.0000      0.1826      34.5413\n    2800      0.874004      0.874004      1.0000      1.0000      0.1778      35.0935\n    2850      0.791402      0.791402      1.0000      1.0000      0.1718      35.6396\n    2900      0.712658      0.712658      1.0000      1.0000      0.1649      36.1790\n    2950      0.638223      0.638223      1.0000      1.0000      0.1570      36.7108\n    3000      0.568452      0.568452      1.0000      1.0000      0.1484      37.2342\n    3050      0.503593      0.503593      1.0000      1.0000      0.1392      37.7482\n    3100      0.443774      0.443774      1.0000      1.0000      0.1297      38.2519\n    3150      0.389009      0.389009      1.0000      1.0000      0.1199      38.7446\n    3200      0.339215      0.339215      1.0000      1.0000      0.1102      39.2263\n    3250      0.294224      0.294224      1.0000      1.0000      0.1007      39.6968\n    3300      0.253817      0.253817      1.0000      1.0000      0.0913      40.1565\n    3350      0.217739      0.217739      1.0000      1.0000      0.0823      40.6063\n    3400      0.185732      0.185732      1.0000      1.0000      0.0736      41.0479\n    3450      0.157538      0.157538      1.0000      1.0000      0.0653      41.4838\n    3500      0.132898      0.132898      1.0000      1.0000      0.0574      41.9167\n    3550      0.111536      0.111536      1.0000      1.0000      0.0501      42.3493\n    3600      0.093163      0.093163      1.0000      1.0000      0.0433      42.7837\n    3650      0.077472      0.077472      1.0000      1.0000      0.0372      43.2208\n    3700      0.064157      0.064157      1.0000      1.0000      0.0317      43.6602\n    3750      0.052923      0.052923      1.0000      1.0000      0.0269      44.1012\n    3800      0.043494      0.043494      1.0000      1.0000      0.0227      44.5429\n    3850      0.035619      0.035619      1.0000      1.0000      0.0191      44.9848\n    3900      0.029071      0.029071      1.0000      1.0000      0.0159      45.4264\n    3950      0.023649      0.023649      1.0000      1.0000      0.0132      45.8675\n    4000      0.019178      0.019178      1.0000      1.0000      0.0110      46.3078\n    4050      0.015505      0.015505      1.0000      1.0000      0.0090      46.7472\n    4100      0.012498      0.012498      1.0000      1.0000      0.0074      47.1856\n    4150      0.010045      0.010045      1.0000      1.0000      0.0061      47.6228\n    4200      0.008050      0.008050      1.0000      1.0000      0.0050      48.0587\n    4250      0.006434      0.006434      1.0000      1.0000      0.0040      48.4933\n    4300      0.005129      0.005129      1.0000      1.0000      0.0033      48.9264\n    4350      0.004077      0.004077      1.0000      1.0000      0.0026      49.3581\n    4400      0.003233      0.003233      1.0000      1.0000      0.0021      49.7882\n    4450      0.002557      0.002557      1.0000      1.0000      0.0017      50.2167\n    4500      0.002017      0.002017      1.0000      1.0000      0.0014      50.6436\n    4550      0.001588      0.001588      1.0000      1.0000      0.0011      51.0689\n    4600      0.001247      0.001247      1.0000      1.0000      0.0009      51.4923\n    4650      0.000977      0.000977      1.0000      1.0000      0.0007      51.9140\n    4700      0.000763      0.000763      1.0000      1.0000      0.0006      52.3337\n    4750      0.000595      0.000595      1.0000      1.0000      0.0004      52.7514\n    4800      0.000463      0.000463      1.0000      1.0000      0.0003      53.1669\n    4850      0.000360      0.000360      1.0000      1.0000      0.0003      53.5801\n    4900      0.000279      0.000279      1.0000      1.0000      0.0002      53.9907\n    4950      0.000216      0.000216      1.0000      1.0000      0.0002      54.3985\n    4999      0.000168      0.000168      1.0000      1.0000      0.0001      54.7951\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.000168\n  Test Loss   = 0.000168\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 54.7951\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.433976, "test_losses": 3.433976, "train_accs": 0.028096, "test_accs": 0.028096, "grad_norms": 0.001537, "param_norms": 4.320604}, {"epoch": 50, "train_losses": 3.433318, "test_losses": 3.433318, "train_accs": 0.14256, "test_accs": 0.14256, "grad_norms": 0.001644, "param_norms": 4.39965}, {"epoch": 100, "train_losses": 3.432411, "test_losses": 3.432411, "train_accs": 0.442248, "test_accs": 0.442248, "grad_norms": 0.002019, "param_norms": 4.661192}, {"epoch": 150, "train_losses": 3.431169, "test_losses": 3.431169, "train_accs": 0.733611, "test_accs": 0.733611, "grad_norms": 0.002639, "param_norms": 5.042629}, {"epoch": 200, "train_losses": 3.429463, "test_losses": 3.429463, "train_accs": 0.896982, "test_accs": 0.896982, "grad_norms": 0.003491, "param_norms": 5.494605}, {"epoch": 250, "train_losses": 3.427181, "test_losses": 3.427181, "train_accs": 0.971904, "test_accs": 0.971904, "grad_norms": 0.004565, "param_norms": 5.986137}, {"epoch": 300, "train_losses": 3.424223, "test_losses": 3.424223, "train_accs": 0.98127, "test_accs": 0.98127, "grad_norms": 0.005851, "param_norms": 6.500601}, {"epoch": 350, "train_losses": 3.420488, "test_losses": 3.420488, "train_accs": 0.992716, "test_accs": 0.992716, "grad_norms": 0.007343, "param_norms": 7.028817}, {"epoch": 400, "train_losses": 3.415882, "test_losses": 3.415882, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.009036, "param_norms": 7.565736}, {"epoch": 450, "train_losses": 3.410305, "test_losses": 3.410305, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.010927, "param_norms": 8.108597}, {"epoch": 500, "train_losses": 3.403662, "test_losses": 3.403662, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.013014, "param_norms": 8.655955}, {"epoch": 550, "train_losses": 3.395854, "test_losses": 3.395854, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.015294, "param_norms": 9.207363}, {"epoch": 600, "train_losses": 3.386791, "test_losses": 3.386791, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.017763, "param_norms": 9.762824}, {"epoch": 650, "train_losses": 3.376379, "test_losses": 3.376379, "train_accs": 0.997919, "test_accs": 0.997919, "grad_norms": 0.02042, "param_norms": 10.321682}, {"epoch": 700, "train_losses": 3.364527, "test_losses": 3.364527, "train_accs": 0.997919, "test_accs": 0.997919, "grad_norms": 0.023261, "param_norms": 10.882997}, {"epoch": 750, "train_losses": 3.351143, "test_losses": 3.351143, "train_accs": 0.997919, "test_accs": 0.997919, "grad_norms": 0.026286, "param_norms": 11.446406}, {"epoch": 800, "train_losses": 3.336136, "test_losses": 3.336136, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.029493, "param_norms": 12.01196}, {"epoch": 850, "train_losses": 3.319419, "test_losses": 3.319419, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.032879, "param_norms": 12.579335}, {"epoch": 900, "train_losses": 3.300901, "test_losses": 3.300901, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.036443, "param_norms": 13.148459}, {"epoch": 950, "train_losses": 3.280498, "test_losses": 3.280498, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.040181, "param_norms": 13.719296}, {"epoch": 1000, "train_losses": 3.258123, "test_losses": 3.258123, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.044091, "param_norms": 14.291623}, {"epoch": 1050, "train_losses": 3.233696, "test_losses": 3.233696, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.04817, "param_norms": 14.865403}, {"epoch": 1100, "train_losses": 3.207133, "test_losses": 3.207133, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.052414, "param_norms": 15.440701}, {"epoch": 1150, "train_losses": 3.178358, "test_losses": 3.178358, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.056821, "param_norms": 16.017318}, {"epoch": 1200, "train_losses": 3.147292, "test_losses": 3.147292, "train_accs": 0.998959, "test_accs": 0.998959, "grad_norms": 0.061385, "param_norms": 16.595146}, {"epoch": 1250, "train_losses": 3.113861, "test_losses": 3.113861, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.066103, "param_norms": 17.174026}, {"epoch": 1300, "train_losses": 3.077994, "test_losses": 3.077994, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.070969, "param_norms": 17.753899}, {"epoch": 1350, "train_losses": 3.039621, "test_losses": 3.039621, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.075978, "param_norms": 18.334704}, {"epoch": 1400, "train_losses": 2.998676, "test_losses": 2.998676, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.081124, "param_norms": 18.916236}, {"epoch": 1450, "train_losses": 2.955099, "test_losses": 2.955099, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.086398, "param_norms": 19.498484}, {"epoch": 1500, "train_losses": 2.908832, "test_losses": 2.908832, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.091792, "param_norms": 20.08139}, {"epoch": 1550, "train_losses": 2.859824, "test_losses": 2.859824, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.097297, "param_norms": 20.664789}, {"epoch": 1600, "train_losses": 2.808027, "test_losses": 2.808027, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.102902, "param_norms": 21.24844}, {"epoch": 1650, "train_losses": 2.753402, "test_losses": 2.753402, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.108593, "param_norms": 21.832322}, {"epoch": 1700, "train_losses": 2.695916, "test_losses": 2.695916, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.114357, "param_norms": 22.416285}, {"epoch": 1750, "train_losses": 2.635547, "test_losses": 2.635547, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.120175, "param_norms": 23.000253}, {"epoch": 1800, "train_losses": 2.57228, "test_losses": 2.57228, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.126028, "param_norms": 23.584251}, {"epoch": 1850, "train_losses": 2.506116, "test_losses": 2.506116, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.131894, "param_norms": 24.16824}, {"epoch": 1900, "train_losses": 2.437068, "test_losses": 2.437068, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.137745, "param_norms": 24.752185}, {"epoch": 1950, "train_losses": 2.365163, "test_losses": 2.365163, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.143551, "param_norms": 25.336133}, {"epoch": 2000, "train_losses": 2.29045, "test_losses": 2.29045, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.149276, "param_norms": 25.919927}, {"epoch": 2050, "train_losses": 2.212999, "test_losses": 2.212999, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.15488, "param_norms": 26.50345}, {"epoch": 2100, "train_losses": 2.132903, "test_losses": 2.132903, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.160316, "param_norms": 27.086543}, {"epoch": 2150, "train_losses": 2.050286, "test_losses": 2.050286, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.165529, "param_norms": 27.669144}, {"epoch": 2200, "train_losses": 1.965298, "test_losses": 1.965298, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.17046, "param_norms": 28.251096}, {"epoch": 2250, "train_losses": 1.87813, "test_losses": 1.87813, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.17504, "param_norms": 28.832217}, {"epoch": 2300, "train_losses": 1.789011, "test_losses": 1.789011, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.179193, "param_norms": 29.412361}, {"epoch": 2350, "train_losses": 1.698212, "test_losses": 1.698212, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.182835, "param_norms": 29.991299}, {"epoch": 2400, "train_losses": 1.606053, "test_losses": 1.606053, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.185878, "param_norms": 30.568755}, {"epoch": 2450, "train_losses": 1.512903, "test_losses": 1.512903, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.188224, "param_norms": 31.144278}, {"epoch": 2500, "train_losses": 1.419181, "test_losses": 1.419181, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.189777, "param_norms": 31.717649}, {"epoch": 2550, "train_losses": 1.325357, "test_losses": 1.325357, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.190438, "param_norms": 32.288661}, {"epoch": 2600, "train_losses": 1.231949, "test_losses": 1.231949, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.190118, "param_norms": 32.857029}, {"epoch": 2650, "train_losses": 1.139515, "test_losses": 1.139515, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.188736, "param_norms": 33.422348}, {"epoch": 2700, "train_losses": 1.048646, "test_losses": 1.048646, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.186233, "param_norms": 33.98403}, {"epoch": 2750, "train_losses": 0.959943, "test_losses": 0.959943, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.182575, "param_norms": 34.541291}, {"epoch": 2800, "train_losses": 0.874004, "test_losses": 0.874004, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.177762, "param_norms": 35.093452}, {"epoch": 2850, "train_losses": 0.791402, "test_losses": 0.791402, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.171835, "param_norms": 35.639644}, {"epoch": 2900, "train_losses": 0.712658, "test_losses": 0.712658, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.164875, "param_norms": 36.179001}, {"epoch": 2950, "train_losses": 0.638223, "test_losses": 0.638223, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.157006, "param_norms": 36.71078}, {"epoch": 3000, "train_losses": 0.568452, "test_losses": 0.568452, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.148387, "param_norms": 37.234186}, {"epoch": 3050, "train_losses": 0.503593, "test_losses": 0.503593, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.139205, "param_norms": 37.748213}, {"epoch": 3100, "train_losses": 0.443774, "test_losses": 0.443774, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.129658, "param_norms": 38.251883}, {"epoch": 3150, "train_losses": 0.389009, "test_losses": 0.389009, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.119941, "param_norms": 38.744643}, {"epoch": 3200, "train_losses": 0.339215, "test_losses": 0.339215, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.110226, "param_norms": 39.22627}, {"epoch": 3250, "train_losses": 0.294224, "test_losses": 0.294224, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.100653, "param_norms": 39.696761}, {"epoch": 3300, "train_losses": 0.253817, "test_losses": 0.253817, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.091322, "param_norms": 40.156481}, {"epoch": 3350, "train_losses": 0.217739, "test_losses": 0.217739, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.082294, "param_norms": 40.60628}, {"epoch": 3400, "train_losses": 0.185732, "test_losses": 0.185732, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.073607, "param_norms": 41.047917}, {"epoch": 3450, "train_losses": 0.157538, "test_losses": 0.157538, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0653, "param_norms": 41.483811}, {"epoch": 3500, "train_losses": 0.132898, "test_losses": 0.132898, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.057431, "param_norms": 41.916694}, {"epoch": 3550, "train_losses": 0.111536, "test_losses": 0.111536, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.050077, "param_norms": 42.349312}, {"epoch": 3600, "train_losses": 0.093163, "test_losses": 0.093163, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.043314, "param_norms": 42.783689}, {"epoch": 3650, "train_losses": 0.077472, "test_losses": 0.077472, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.037195, "param_norms": 43.220777}, {"epoch": 3700, "train_losses": 0.064157, "test_losses": 0.064157, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.031734, "param_norms": 43.660225}, {"epoch": 3750, "train_losses": 0.052923, "test_losses": 0.052923, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.026916, "param_norms": 44.101204}, {"epoch": 3800, "train_losses": 0.043494, "test_losses": 0.043494, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.022706, "param_norms": 44.542922}, {"epoch": 3850, "train_losses": 0.035619, "test_losses": 0.035619, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.019057, "param_norms": 44.984802}, {"epoch": 3900, "train_losses": 0.029071, "test_losses": 0.029071, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01592, "param_norms": 45.426439}, {"epoch": 3950, "train_losses": 0.023649, "test_losses": 0.023649, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01324, "param_norms": 45.8675}, {"epoch": 4000, "train_losses": 0.019178, "test_losses": 0.019178, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010966, "param_norms": 46.307815}, {"epoch": 4050, "train_losses": 0.015505, "test_losses": 0.015505, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009046, "param_norms": 46.747248}, {"epoch": 4100, "train_losses": 0.012498, "test_losses": 0.012498, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007435, "param_norms": 47.1856}, {"epoch": 4150, "train_losses": 0.010045, "test_losses": 0.010045, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006088, "param_norms": 47.622774}, {"epoch": 4200, "train_losses": 0.00805, "test_losses": 0.00805, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004969, "param_norms": 48.058708}, {"epoch": 4250, "train_losses": 0.006434, "test_losses": 0.006434, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004041, "param_norms": 48.493282}, {"epoch": 4300, "train_losses": 0.005129, "test_losses": 0.005129, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003277, "param_norms": 48.926445}, {"epoch": 4350, "train_losses": 0.004077, "test_losses": 0.004077, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002648, "param_norms": 49.358095}, {"epoch": 4400, "train_losses": 0.003233, "test_losses": 0.003233, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.002134, "param_norms": 49.788248}, {"epoch": 4450, "train_losses": 0.002557, "test_losses": 0.002557, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001715, "param_norms": 50.21675}, {"epoch": 4500, "train_losses": 0.002017, "test_losses": 0.002017, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001374, "param_norms": 50.643646}, {"epoch": 4550, "train_losses": 0.001588, "test_losses": 0.001588, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.001097, "param_norms": 51.068871}, {"epoch": 4600, "train_losses": 0.001247, "test_losses": 0.001247, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000874, "param_norms": 51.492309}, {"epoch": 4650, "train_losses": 0.000977, "test_losses": 0.000977, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000695, "param_norms": 51.913958}, {"epoch": 4700, "train_losses": 0.000763, "test_losses": 0.000763, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000551, "param_norms": 52.333676}, {"epoch": 4750, "train_losses": 0.000595, "test_losses": 0.000595, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000435, "param_norms": 52.751372}, {"epoch": 4800, "train_losses": 0.000463, "test_losses": 0.000463, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000343, "param_norms": 53.166887}, {"epoch": 4850, "train_losses": 0.00036, "test_losses": 0.00036, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00027, "param_norms": 53.580091}, {"epoch": 4900, "train_losses": 0.000279, "test_losses": 0.000279, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000212, "param_norms": 53.990689}, {"epoch": 4950, "train_losses": 0.000216, "test_losses": 0.000216, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000166, "param_norms": 54.398488}, {"epoch": 4999, "train_losses": 0.000168, "test_losses": 0.000168, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.000131, "param_norms": 54.795093}], "total_epochs": 5000}, "quad_single_freq": {"config": {"prime": 31, "d_mlp": 931, "act_type": "Quad", "init_type": "single-freq", "init_scale": 0.02, "optimizer": "SGD", "lr": 0.1, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=31, run=quad_single_freq\n======================================================================\n\nConfiguration:\n  prime (p)       = 31\n  d_mlp           = 931\n  activation      = Quad\n  init_type       = single-freq\n  init_scale      = 0.02\n  optimizer       = SGD\n  learning_rate   = 0.1\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.433966      3.433966      0.0302      0.0302      0.0035       3.3977\n      50      3.433912      3.433912      0.0593      0.0593      0.0035       3.3979\n     100      3.433852      3.433852      0.1041      0.1041      0.0035       3.3984\n     150      3.433792      3.433792      0.1394      0.1394      0.0035       3.3991\n     200      3.433732      3.433732      0.1811      0.1811      0.0035       3.4001\n     250      3.433671      3.433671      0.2518      0.2518      0.0035       3.4013\n     300      3.433610      3.433610      0.3351      0.3351      0.0035       3.4029\n     350      3.433550      3.433550      0.4173      0.4173      0.0035       3.4046\n     400      3.433490      3.433490      0.5120      0.5120      0.0035       3.4067\n     450      3.433428      3.433428      0.6223      0.6223      0.0035       3.4090\n     500      3.433367      3.433367      0.7055      0.7055      0.0035       3.4116\n     550      3.433306      3.433306      0.7815      0.7815      0.0035       3.4145\n     600      3.433244      3.433244      0.8314      0.8314      0.0035       3.4176\n     650      3.433182      3.433182      0.8793      0.8793      0.0035       3.4210\n     700      3.433119      3.433119      0.9126      0.9126      0.0035       3.4247\n     750      3.433056      3.433056      0.9490      0.9490      0.0036       3.4286\n     800      3.432993      3.432993      0.9646      0.9646      0.0036       3.4328\n     850      3.432929      3.432929      0.9729      0.9729      0.0036       3.4373\n     900      3.432865      3.432865      0.9792      0.9792      0.0036       3.4420\n     950      3.432800      3.432800      0.9875      0.9875      0.0036       3.4471\n    1000      3.432735      3.432735      0.9917      0.9917      0.0036       3.4524\n    1050      3.432669      3.432669      0.9979      0.9979      0.0036       3.4579\n    1100      3.432603      3.432603      1.0000      1.0000      0.0037       3.4638\n    1150      3.432536      3.432536      1.0000      1.0000      0.0037       3.4699\n    1200      3.432468      3.432468      1.0000      1.0000      0.0037       3.4763\n    1250      3.432400      3.432400      1.0000      1.0000      0.0037       3.4830\n    1300      3.432331      3.432331      1.0000      1.0000      0.0037       3.4900\n    1350      3.432261      3.432261      1.0000      1.0000      0.0037       3.4973\n    1400      3.432190      3.432190      1.0000      1.0000      0.0038       3.5048\n    1450      3.432119      3.432119      1.0000      1.0000      0.0038       3.5126\n    1500      3.432047      3.432047      1.0000      1.0000      0.0038       3.5208\n    1550      3.431973      3.431973      1.0000      1.0000      0.0038       3.5292\n    1600      3.431899      3.431899      1.0000      1.0000      0.0039       3.5379\n    1650      3.431824      3.431824      1.0000      1.0000      0.0039       3.5469\n    1700      3.431748      3.431748      1.0000      1.0000      0.0039       3.5562\n    1750      3.431671      3.431671      1.0000      1.0000      0.0039       3.5658\n    1800      3.431592      3.431592      1.0000      1.0000      0.0040       3.5756\n    1850      3.431512      3.431512      1.0000      1.0000      0.0040       3.5858\n    1900      3.431432      3.431432      1.0000      1.0000      0.0040       3.5963\n    1950      3.431350      3.431350      1.0000      1.0000      0.0041       3.6072\n    2000      3.431267      3.431267      1.0000      1.0000      0.0041       3.6183\n    2050      3.431182      3.431182      1.0000      1.0000      0.0041       3.6297\n    2100      3.431096      3.431096      1.0000      1.0000      0.0042       3.6415\n    2150      3.431008      3.431008      1.0000      1.0000      0.0042       3.6535\n    2200      3.430919      3.430919      1.0000      1.0000      0.0042       3.6659\n    2250      3.430828      3.430828      1.0000      1.0000      0.0043       3.6786\n    2300      3.430736      3.430736      1.0000      1.0000      0.0043       3.6917\n    2350      3.430642      3.430642      1.0000      1.0000      0.0044       3.7050\n    2400      3.430547      3.430547      1.0000      1.0000      0.0044       3.7187\n    2450      3.430448      3.430448      1.0000      1.0000      0.0044       3.7328\n    2500      3.430349      3.430349      1.0000      1.0000      0.0045       3.7472\n    2550      3.430248      3.430248      1.0000      1.0000      0.0045       3.7619\n    2600      3.430144      3.430144      1.0000      1.0000      0.0046       3.7770\n    2650      3.430038      3.430038      1.0000      1.0000      0.0046       3.7924\n    2700      3.429930      3.429930      1.0000      1.0000      0.0047       3.8082\n    2750      3.429820      3.429820      1.0000      1.0000      0.0047       3.8244\n    2800      3.429707      3.429707      1.0000      1.0000      0.0048       3.8409\n    2850      3.429592      3.429592      1.0000      1.0000      0.0048       3.8578\n    2900      3.429475      3.429475      1.0000      1.0000      0.0049       3.8751\n    2950      3.429355      3.429355      1.0000      1.0000      0.0049       3.8927\n    3000      3.429231      3.429231      1.0000      1.0000      0.0050       3.9108\n    3050      3.429105      3.429105      1.0000      1.0000      0.0050       3.9292\n    3100      3.428977      3.428977      1.0000      1.0000      0.0051       3.9480\n    3150      3.428844      3.428844      1.0000      1.0000      0.0052       3.9673\n    3200      3.428710      3.428710      1.0000      1.0000      0.0052       3.9869\n    3250      3.428571      3.428571      1.0000      1.0000      0.0053       4.0070\n    3300      3.428429      3.428429      1.0000      1.0000      0.0054       4.0274\n    3350      3.428283      3.428283      1.0000      1.0000      0.0054       4.0484\n    3400      3.428134      3.428134      1.0000      1.0000      0.0055       4.0697\n    3450      3.427981      3.427981      1.0000      1.0000      0.0056       4.0915\n    3500      3.427823      3.427823      1.0000      1.0000      0.0056       4.1137\n    3550      3.427662      3.427662      1.0000      1.0000      0.0057       4.1364\n    3600      3.427496      3.427496      1.0000      1.0000      0.0058       4.1596\n    3650      3.427326      3.427326      1.0000      1.0000      0.0059       4.1832\n    3700      3.427151      3.427151      1.0000      1.0000      0.0060       4.2074\n    3750      3.426971      3.426971      1.0000      1.0000      0.0060       4.2320\n    3800      3.426786      3.426786      1.0000      1.0000      0.0061       4.2571\n    3850      3.426595      3.426595      1.0000      1.0000      0.0062       4.2827\n    3900      3.426400      3.426400      1.0000      1.0000      0.0063       4.3088\n    3950      3.426198      3.426198      1.0000      1.0000      0.0064       4.3355\n    4000      3.425990      3.425990      1.0000      1.0000      0.0065       4.3627\n    4050      3.425776      3.425776      1.0000      1.0000      0.0066       4.3905\n    4100      3.425555      3.425555      1.0000      1.0000      0.0067       4.4188\n    4150      3.425328      3.425328      1.0000      1.0000      0.0068       4.4477\n    4200      3.425094      3.425094      1.0000      1.0000      0.0069       4.4772\n    4250      3.424852      3.424852      1.0000      1.0000      0.0070       4.5073\n    4300      3.424603      3.424603      1.0000      1.0000      0.0071       4.5380\n    4350      3.424345      3.424345      1.0000      1.0000      0.0072       4.5693\n    4400      3.424079      3.424079      1.0000      1.0000      0.0074       4.6013\n    4450      3.423804      3.423804      1.0000      1.0000      0.0075       4.6339\n    4500      3.423519      3.423519      1.0000      1.0000      0.0076       4.6672\n    4550      3.423226      3.423226      1.0000      1.0000      0.0077       4.7011\n    4600      3.422922      3.422922      1.0000      1.0000      0.0079       4.7358\n    4650      3.422608      3.422608      1.0000      1.0000      0.0080       4.7712\n    4700      3.422282      3.422282      1.0000      1.0000      0.0081       4.8073\n    4750      3.421945      3.421945      1.0000      1.0000      0.0083       4.8442\n    4800      3.421596      3.421596      1.0000      1.0000      0.0084       4.8819\n    4850      3.421234      3.421234      1.0000      1.0000      0.0086       4.9203\n    4900      3.420859      3.420859      1.0000      1.0000      0.0087       4.9596\n    4950      3.420470      3.420470      1.0000      1.0000      0.0089       4.9997\n    4999      3.420075      3.420075      1.0000      1.0000      0.0091       5.0398\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 3.420075\n  Test Loss   = 3.420075\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 5.0398\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.433966, "test_losses": 3.433966, "train_accs": 0.030177, "test_accs": 0.030177, "grad_norms": 0.003466, "param_norms": 3.397703}, {"epoch": 50, "train_losses": 3.433912, "test_losses": 3.433912, "train_accs": 0.059313, "test_accs": 0.059313, "grad_norms": 0.003466, "param_norms": 3.397892}, {"epoch": 100, "train_losses": 3.433852, "test_losses": 3.433852, "train_accs": 0.104058, "test_accs": 0.104058, "grad_norms": 0.003468, "param_norms": 3.398355}, {"epoch": 150, "train_losses": 3.433792, "test_losses": 3.433792, "train_accs": 0.139438, "test_accs": 0.139438, "grad_norms": 0.00347, "param_norms": 3.399081}, {"epoch": 200, "train_losses": 3.433732, "test_losses": 3.433732, "train_accs": 0.181061, "test_accs": 0.181061, "grad_norms": 0.003472, "param_norms": 3.400075}, {"epoch": 250, "train_losses": 3.433671, "test_losses": 3.433671, "train_accs": 0.251821, "test_accs": 0.251821, "grad_norms": 0.003476, "param_norms": 3.401332}, {"epoch": 300, "train_losses": 3.43361, "test_losses": 3.43361, "train_accs": 0.335068, "test_accs": 0.335068, "grad_norms": 0.00348, "param_norms": 3.402855}, {"epoch": 350, "train_losses": 3.43355, "test_losses": 3.43355, "train_accs": 0.417274, "test_accs": 0.417274, "grad_norms": 0.003485, "param_norms": 3.404645}, {"epoch": 400, "train_losses": 3.43349, "test_losses": 3.43349, "train_accs": 0.511967, "test_accs": 0.511967, "grad_norms": 0.003491, "param_norms": 3.4067}, {"epoch": 450, "train_losses": 3.433428, "test_losses": 3.433428, "train_accs": 0.622268, "test_accs": 0.622268, "grad_norms": 0.003497, "param_norms": 3.409024}, {"epoch": 500, "train_losses": 3.433367, "test_losses": 3.433367, "train_accs": 0.705515, "test_accs": 0.705515, "grad_norms": 0.003505, "param_norms": 3.411617}, {"epoch": 550, "train_losses": 3.433306, "test_losses": 3.433306, "train_accs": 0.781478, "test_accs": 0.781478, "grad_norms": 0.003513, "param_norms": 3.414475}, {"epoch": 600, "train_losses": 3.433244, "test_losses": 3.433244, "train_accs": 0.831426, "test_accs": 0.831426, "grad_norms": 0.003521, "param_norms": 3.4176}, {"epoch": 650, "train_losses": 3.433182, "test_losses": 3.433182, "train_accs": 0.879292, "test_accs": 0.879292, "grad_norms": 0.003531, "param_norms": 3.420995}, {"epoch": 700, "train_losses": 3.433119, "test_losses": 3.433119, "train_accs": 0.912591, "test_accs": 0.912591, "grad_norms": 0.003541, "param_norms": 3.424663}, {"epoch": 750, "train_losses": 3.433056, "test_losses": 3.433056, "train_accs": 0.949011, "test_accs": 0.949011, "grad_norms": 0.003552, "param_norms": 3.428597}, {"epoch": 800, "train_losses": 3.432993, "test_losses": 3.432993, "train_accs": 0.96462, "test_accs": 0.96462, "grad_norms": 0.003564, "param_norms": 3.432805}, {"epoch": 850, "train_losses": 3.432929, "test_losses": 3.432929, "train_accs": 0.972945, "test_accs": 0.972945, "grad_norms": 0.003577, "param_norms": 3.437283}, {"epoch": 900, "train_losses": 3.432865, "test_losses": 3.432865, "train_accs": 0.979188, "test_accs": 0.979188, "grad_norms": 0.00359, "param_norms": 3.442035}, {"epoch": 950, "train_losses": 3.4328, "test_losses": 3.4328, "train_accs": 0.987513, "test_accs": 0.987513, "grad_norms": 0.003604, "param_norms": 3.447062}, {"epoch": 1000, "train_losses": 3.432735, "test_losses": 3.432735, "train_accs": 0.991675, "test_accs": 0.991675, "grad_norms": 0.003619, "param_norms": 3.452361}, {"epoch": 1050, "train_losses": 3.432669, "test_losses": 3.432669, "train_accs": 0.997919, "test_accs": 0.997919, "grad_norms": 0.003635, "param_norms": 3.457939}, {"epoch": 1100, "train_losses": 3.432603, "test_losses": 3.432603, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003652, "param_norms": 3.463793}, {"epoch": 1150, "train_losses": 3.432536, "test_losses": 3.432536, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003669, "param_norms": 3.469922}, {"epoch": 1200, "train_losses": 3.432468, "test_losses": 3.432468, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003688, "param_norms": 3.476335}, {"epoch": 1250, "train_losses": 3.4324, "test_losses": 3.4324, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003707, "param_norms": 3.483028}, {"epoch": 1300, "train_losses": 3.432331, "test_losses": 3.432331, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003727, "param_norms": 3.490004}, {"epoch": 1350, "train_losses": 3.432261, "test_losses": 3.432261, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003747, "param_norms": 3.497262}, {"epoch": 1400, "train_losses": 3.43219, "test_losses": 3.43219, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003769, "param_norms": 3.504806}, {"epoch": 1450, "train_losses": 3.432119, "test_losses": 3.432119, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003792, "param_norms": 3.512638}, {"epoch": 1500, "train_losses": 3.432047, "test_losses": 3.432047, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003815, "param_norms": 3.520757}, {"epoch": 1550, "train_losses": 3.431973, "test_losses": 3.431973, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003839, "param_norms": 3.529169}, {"epoch": 1600, "train_losses": 3.431899, "test_losses": 3.431899, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003864, "param_norms": 3.537873}, {"epoch": 1650, "train_losses": 3.431824, "test_losses": 3.431824, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00389, "param_norms": 3.546868}, {"epoch": 1700, "train_losses": 3.431748, "test_losses": 3.431748, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003917, "param_norms": 3.556164}, {"epoch": 1750, "train_losses": 3.431671, "test_losses": 3.431671, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003945, "param_norms": 3.565757}, {"epoch": 1800, "train_losses": 3.431592, "test_losses": 3.431592, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003974, "param_norms": 3.575649}, {"epoch": 1850, "train_losses": 3.431512, "test_losses": 3.431512, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004004, "param_norms": 3.585847}, {"epoch": 1900, "train_losses": 3.431432, "test_losses": 3.431432, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004035, "param_norms": 3.596346}, {"epoch": 1950, "train_losses": 3.43135, "test_losses": 3.43135, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004066, "param_norms": 3.607158}, {"epoch": 2000, "train_losses": 3.431267, "test_losses": 3.431267, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004099, "param_norms": 3.618276}, {"epoch": 2050, "train_losses": 3.431182, "test_losses": 3.431182, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004133, "param_norms": 3.629709}, {"epoch": 2100, "train_losses": 3.431096, "test_losses": 3.431096, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004167, "param_norms": 3.641458}, {"epoch": 2150, "train_losses": 3.431008, "test_losses": 3.431008, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004203, "param_norms": 3.653524}, {"epoch": 2200, "train_losses": 3.430919, "test_losses": 3.430919, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00424, "param_norms": 3.665913}, {"epoch": 2250, "train_losses": 3.430828, "test_losses": 3.430828, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004278, "param_norms": 3.678625}, {"epoch": 2300, "train_losses": 3.430736, "test_losses": 3.430736, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004317, "param_norms": 3.691668}, {"epoch": 2350, "train_losses": 3.430642, "test_losses": 3.430642, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004357, "param_norms": 3.705041}, {"epoch": 2400, "train_losses": 3.430547, "test_losses": 3.430547, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004398, "param_norms": 3.718747}, {"epoch": 2450, "train_losses": 3.430448, "test_losses": 3.430448, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004441, "param_norms": 3.732793}, {"epoch": 2500, "train_losses": 3.430349, "test_losses": 3.430349, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004484, "param_norms": 3.747178}, {"epoch": 2550, "train_losses": 3.430248, "test_losses": 3.430248, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004529, "param_norms": 3.761912}, {"epoch": 2600, "train_losses": 3.430144, "test_losses": 3.430144, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004575, "param_norms": 3.776994}, {"epoch": 2650, "train_losses": 3.430038, "test_losses": 3.430038, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004622, "param_norms": 3.792432}, {"epoch": 2700, "train_losses": 3.42993, "test_losses": 3.42993, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004671, "param_norms": 3.808226}, {"epoch": 2750, "train_losses": 3.42982, "test_losses": 3.42982, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00472, "param_norms": 3.824382}, {"epoch": 2800, "train_losses": 3.429707, "test_losses": 3.429707, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004771, "param_norms": 3.840907}, {"epoch": 2850, "train_losses": 3.429592, "test_losses": 3.429592, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004824, "param_norms": 3.857803}, {"epoch": 2900, "train_losses": 3.429475, "test_losses": 3.429475, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004878, "param_norms": 3.875073}, {"epoch": 2950, "train_losses": 3.429355, "test_losses": 3.429355, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004933, "param_norms": 3.892729}, {"epoch": 3000, "train_losses": 3.429231, "test_losses": 3.429231, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00499, "param_norms": 3.910768}, {"epoch": 3050, "train_losses": 3.429105, "test_losses": 3.429105, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005048, "param_norms": 3.9292}, {"epoch": 3100, "train_losses": 3.428977, "test_losses": 3.428977, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005108, "param_norms": 3.948029}, {"epoch": 3150, "train_losses": 3.428844, "test_losses": 3.428844, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005169, "param_norms": 3.967266}, {"epoch": 3200, "train_losses": 3.42871, "test_losses": 3.42871, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005232, "param_norms": 3.986909}, {"epoch": 3250, "train_losses": 3.428571, "test_losses": 3.428571, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005296, "param_norms": 4.00697}, {"epoch": 3300, "train_losses": 3.428429, "test_losses": 3.428429, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005363, "param_norms": 4.02745}, {"epoch": 3350, "train_losses": 3.428283, "test_losses": 3.428283, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005431, "param_norms": 4.04836}, {"epoch": 3400, "train_losses": 3.428134, "test_losses": 3.428134, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0055, "param_norms": 4.069706}, {"epoch": 3450, "train_losses": 3.427981, "test_losses": 3.427981, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005572, "param_norms": 4.091496}, {"epoch": 3500, "train_losses": 3.427823, "test_losses": 3.427823, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005645, "param_norms": 4.113736}, {"epoch": 3550, "train_losses": 3.427662, "test_losses": 3.427662, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00572, "param_norms": 4.136434}, {"epoch": 3600, "train_losses": 3.427496, "test_losses": 3.427496, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005798, "param_norms": 4.159597}, {"epoch": 3650, "train_losses": 3.427326, "test_losses": 3.427326, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005877, "param_norms": 4.183235}, {"epoch": 3700, "train_losses": 3.427151, "test_losses": 3.427151, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005958, "param_norms": 4.207352}, {"epoch": 3750, "train_losses": 3.426971, "test_losses": 3.426971, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006042, "param_norms": 4.231966}, {"epoch": 3800, "train_losses": 3.426786, "test_losses": 3.426786, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006127, "param_norms": 4.257076}, {"epoch": 3850, "train_losses": 3.426595, "test_losses": 3.426595, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006215, "param_norms": 4.282698}, {"epoch": 3900, "train_losses": 3.4264, "test_losses": 3.4264, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006306, "param_norms": 4.30884}, {"epoch": 3950, "train_losses": 3.426198, "test_losses": 3.426198, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006398, "param_norms": 4.335509}, {"epoch": 4000, "train_losses": 3.42599, "test_losses": 3.42599, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006494, "param_norms": 4.362721}, {"epoch": 4050, "train_losses": 3.425776, "test_losses": 3.425776, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006591, "param_norms": 4.390485}, {"epoch": 4100, "train_losses": 3.425555, "test_losses": 3.425555, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006692, "param_norms": 4.418808}, {"epoch": 4150, "train_losses": 3.425328, "test_losses": 3.425328, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006795, "param_norms": 4.447707}, {"epoch": 4200, "train_losses": 3.425094, "test_losses": 3.425094, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006901, "param_norms": 4.477195}, {"epoch": 4250, "train_losses": 3.424852, "test_losses": 3.424852, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00701, "param_norms": 4.507279}, {"epoch": 4300, "train_losses": 3.424603, "test_losses": 3.424603, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007122, "param_norms": 4.537976}, {"epoch": 4350, "train_losses": 3.424345, "test_losses": 3.424345, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007237, "param_norms": 4.569295}, {"epoch": 4400, "train_losses": 3.424079, "test_losses": 3.424079, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007355, "param_norms": 4.60126}, {"epoch": 4450, "train_losses": 3.423804, "test_losses": 3.423804, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007476, "param_norms": 4.633875}, {"epoch": 4500, "train_losses": 3.423519, "test_losses": 3.423519, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007601, "param_norms": 4.667164}, {"epoch": 4550, "train_losses": 3.423226, "test_losses": 3.423226, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00773, "param_norms": 4.701134}, {"epoch": 4600, "train_losses": 3.422922, "test_losses": 3.422922, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007862, "param_norms": 4.735807}, {"epoch": 4650, "train_losses": 3.422608, "test_losses": 3.422608, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007998, "param_norms": 4.771199}, {"epoch": 4700, "train_losses": 3.422282, "test_losses": 3.422282, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008138, "param_norms": 4.807327}, {"epoch": 4750, "train_losses": 3.421945, "test_losses": 3.421945, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008282, "param_norms": 4.844213}, {"epoch": 4800, "train_losses": 3.421596, "test_losses": 3.421596, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008431, "param_norms": 4.88187}, {"epoch": 4850, "train_losses": 3.421234, "test_losses": 3.421234, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008584, "param_norms": 4.920324}, {"epoch": 4900, "train_losses": 3.420859, "test_losses": 3.420859, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008741, "param_norms": 4.959588}, {"epoch": 4950, "train_losses": 3.42047, "test_losses": 3.42047, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008903, "param_norms": 4.999696}, {"epoch": 4999, "train_losses": 3.420075, "test_losses": 3.420075, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009067, "param_norms": 5.039828}], "total_epochs": 5000}, "relu_single_freq": {"config": {"prime": 31, "d_mlp": 931, "act_type": "ReLU", "init_type": "single-freq", "init_scale": 0.002, "optimizer": "SGD", "lr": 0.01, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=31, run=relu_single_freq\n======================================================================\n\nConfiguration:\n  prime (p)       = 31\n  d_mlp           = 931\n  activation      = ReLU\n  init_type       = single-freq\n  init_scale      = 0.002\n  optimizer       = SGD\n  learning_rate   = 0.01\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.433985      3.433985      0.0323      0.0323      0.0041       0.3398\n      50      3.433977      3.433977      0.0406      0.0406      0.0041       0.3398\n     100      3.433969      3.433969      0.0572      0.0572      0.0041       0.3398\n     150      3.433960      3.433960      0.0843      0.0843      0.0041       0.3399\n     200      3.433951      3.433951      0.1061      0.1061      0.0041       0.3400\n     250      3.433943      3.433943      0.1478      0.1478      0.0041       0.3401\n     300      3.433935      3.433935      0.1821      0.1821      0.0041       0.3403\n     350      3.433926      3.433926      0.2081      0.2081      0.0041       0.3404\n     400      3.433918      3.433918      0.2466      0.2466      0.0041       0.3406\n     450      3.433909      3.433909      0.2737      0.2737      0.0041       0.3408\n     500      3.433901      3.433901      0.3132      0.3132      0.0041       0.3411\n     550      3.433892      3.433892      0.3736      0.3736      0.0041       0.3413\n     600      3.433884      3.433884      0.4194      0.4194      0.0041       0.3416\n     650      3.433876      3.433876      0.4651      0.4651      0.0041       0.3419\n     700      3.433867      3.433867      0.5182      0.5182      0.0041       0.3423\n     750      3.433859      3.433859      0.5702      0.5702      0.0041       0.3426\n     800      3.433850      3.433850      0.6202      0.6202      0.0041       0.3430\n     850      3.433842      3.433842      0.6722      0.6722      0.0041       0.3434\n     900      3.433833      3.433833      0.7180      0.7180      0.0041       0.3439\n     950      3.433825      3.433825      0.7523      0.7523      0.0041       0.3443\n    1000      3.433817      3.433817      0.8044      0.8044      0.0041       0.3448\n    1050      3.433808      3.433808      0.8377      0.8377      0.0041       0.3453\n    1100      3.433800      3.433800      0.8585      0.8585      0.0041       0.3458\n    1150      3.433791      3.433791      0.8824      0.8824      0.0041       0.3464\n    1200      3.433783      3.433783      0.9011      0.9011      0.0041       0.3470\n    1250      3.433775      3.433775      0.9251      0.9251      0.0041       0.3476\n    1300      3.433766      3.433766      0.9355      0.9355      0.0041       0.3482\n    1350      3.433758      3.433758      0.9490      0.9490      0.0041       0.3488\n    1400      3.433749      3.433749      0.9584      0.9584      0.0041       0.3495\n    1450      3.433741      3.433741      0.9657      0.9657      0.0041       0.3502\n    1500      3.433733      3.433733      0.9761      0.9761      0.0041       0.3509\n    1550      3.433724      3.433724      0.9802      0.9802      0.0041       0.3517\n    1600      3.433716      3.433716      0.9844      0.9844      0.0041       0.3524\n    1650      3.433707      3.433707      0.9886      0.9886      0.0041       0.3532\n    1700      3.433699      3.433699      0.9906      0.9906      0.0041       0.3540\n    1750      3.433691      3.433691      0.9938      0.9938      0.0041       0.3548\n    1800      3.433682      3.433682      0.9938      0.9938      0.0041       0.3557\n    1850      3.433674      3.433674      0.9938      0.9938      0.0041       0.3565\n    1900      3.433665      3.433665      0.9938      0.9938      0.0041       0.3574\n    1950      3.433657      3.433657      0.9948      0.9948      0.0041       0.3583\n    2000      3.433649      3.433649      0.9969      0.9969      0.0041       0.3593\n    2050      3.433640      3.433640      0.9969      0.9969      0.0041       0.3602\n    2100      3.433632      3.433632      0.9969      0.9969      0.0042       0.3612\n    2150      3.433623      3.433623      0.9969      0.9969      0.0042       0.3622\n    2200      3.433615      3.433615      0.9969      0.9969      0.0042       0.3632\n    2250      3.433606      3.433606      0.9969      0.9969      0.0042       0.3642\n    2300      3.433598      3.433598      0.9969      0.9969      0.0042       0.3653\n    2350      3.433589      3.433589      0.9969      0.9969      0.0042       0.3664\n    2400      3.433581      3.433581      0.9969      0.9969      0.0042       0.3675\n    2450      3.433572      3.433572      1.0000      1.0000      0.0042       0.3686\n    2500      3.433564      3.433564      1.0000      1.0000      0.0042       0.3697\n    2550      3.433556      3.433556      1.0000      1.0000      0.0042       0.3709\n    2600      3.433547      3.433547      1.0000      1.0000      0.0042       0.3721\n    2650      3.433539      3.433539      1.0000      1.0000      0.0042       0.3732\n    2700      3.433530      3.433530      1.0000      1.0000      0.0042       0.3745\n    2750      3.433521      3.433521      1.0000      1.0000      0.0042       0.3757\n    2800      3.433513      3.433513      1.0000      1.0000      0.0042       0.3769\n    2850      3.433505      3.433505      1.0000      1.0000      0.0042       0.3782\n    2900      3.433496      3.433496      1.0000      1.0000      0.0042       0.3795\n    2950      3.433487      3.433487      1.0000      1.0000      0.0043       0.3808\n    3000      3.433478      3.433478      1.0000      1.0000      0.0043       0.3821\n    3050      3.433470      3.433470      1.0000      1.0000      0.0043       0.3835\n    3100      3.433461      3.433461      1.0000      1.0000      0.0043       0.3848\n    3150      3.433453      3.433453      1.0000      1.0000      0.0043       0.3862\n    3200      3.433444      3.433444      1.0000      1.0000      0.0043       0.3876\n    3250      3.433435      3.433435      1.0000      1.0000      0.0043       0.3890\n    3300      3.433426      3.433426      1.0000      1.0000      0.0043       0.3904\n    3350      3.433418      3.433418      1.0000      1.0000      0.0043       0.3919\n    3400      3.433409      3.433409      1.0000      1.0000      0.0044       0.3933\n    3450      3.433400      3.433400      1.0000      1.0000      0.0044       0.3948\n    3500      3.433391      3.433391      1.0000      1.0000      0.0044       0.3963\n    3550      3.433382      3.433382      1.0000      1.0000      0.0044       0.3978\n    3600      3.433373      3.433373      1.0000      1.0000      0.0044       0.3993\n    3650      3.433364      3.433364      1.0000      1.0000      0.0044       0.4009\n    3700      3.433356      3.433356      1.0000      1.0000      0.0044       0.4024\n    3750      3.433346      3.433346      1.0000      1.0000      0.0044       0.4040\n    3800      3.433337      3.433337      1.0000      1.0000      0.0045       0.4056\n    3850      3.433328      3.433328      1.0000      1.0000      0.0045       0.4072\n    3900      3.433319      3.433319      1.0000      1.0000      0.0045       0.4089\n    3950      3.433310      3.433310      1.0000      1.0000      0.0045       0.4105\n    4000      3.433300      3.433300      1.0000      1.0000      0.0045       0.4122\n    4050      3.433291      3.433291      1.0000      1.0000      0.0045       0.4138\n    4100      3.433282      3.433282      1.0000      1.0000      0.0045       0.4155\n    4150      3.433273      3.433273      1.0000      1.0000      0.0046       0.4172\n    4200      3.433264      3.433264      1.0000      1.0000      0.0046       0.4189\n    4250      3.433254      3.433254      1.0000      1.0000      0.0046       0.4207\n    4300      3.433244      3.433244      1.0000      1.0000      0.0046       0.4224\n    4350      3.433235      3.433235      1.0000      1.0000      0.0046       0.4242\n    4400      3.433225      3.433225      1.0000      1.0000      0.0047       0.4260\n    4450      3.433216      3.433216      1.0000      1.0000      0.0047       0.4278\n    4500      3.433206      3.433206      1.0000      1.0000      0.0047       0.4296\n    4550      3.433196      3.433196      1.0000      1.0000      0.0047       0.4314\n    4600      3.433187      3.433187      1.0000      1.0000      0.0047       0.4332\n    4650      3.433177      3.433177      1.0000      1.0000      0.0047       0.4351\n    4700      3.433167      3.433167      1.0000      1.0000      0.0048       0.4370\n    4750      3.433157      3.433157      1.0000      1.0000      0.0048       0.4389\n    4800      3.433147      3.433147      1.0000      1.0000      0.0048       0.4408\n    4850      3.433137      3.433137      1.0000      1.0000      0.0048       0.4427\n    4900      3.433127      3.433127      1.0000      1.0000      0.0048       0.4446\n    4950      3.433117      3.433117      1.0000      1.0000      0.0049       0.4465\n    4999      3.433106      3.433106      1.0000      1.0000      0.0049       0.4485\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 3.433106\n  Test Loss   = 3.433106\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 0.4485\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.433985, "test_losses": 3.433985, "train_accs": 0.032258, "test_accs": 0.032258, "grad_norms": 0.004112, "param_norms": 0.339771}, {"epoch": 50, "train_losses": 3.433977, "test_losses": 3.433977, "train_accs": 0.040583, "test_accs": 0.040583, "grad_norms": 0.004112, "param_norms": 0.339788}, {"epoch": 100, "train_losses": 3.433969, "test_losses": 3.433969, "train_accs": 0.057232, "test_accs": 0.057232, "grad_norms": 0.004114, "param_norms": 0.339831}, {"epoch": 150, "train_losses": 3.43396, "test_losses": 3.43396, "train_accs": 0.084287, "test_accs": 0.084287, "grad_norms": 0.004113, "param_norms": 0.339899}, {"epoch": 200, "train_losses": 3.433951, "test_losses": 3.433951, "train_accs": 0.106139, "test_accs": 0.106139, "grad_norms": 0.004109, "param_norms": 0.339992}, {"epoch": 250, "train_losses": 3.433943, "test_losses": 3.433943, "train_accs": 0.147763, "test_accs": 0.147763, "grad_norms": 0.004108, "param_norms": 0.340109}, {"epoch": 300, "train_losses": 3.433935, "test_losses": 3.433935, "train_accs": 0.182102, "test_accs": 0.182102, "grad_norms": 0.004106, "param_norms": 0.340252}, {"epoch": 350, "train_losses": 3.433926, "test_losses": 3.433926, "train_accs": 0.208117, "test_accs": 0.208117, "grad_norms": 0.004106, "param_norms": 0.340419}, {"epoch": 400, "train_losses": 3.433918, "test_losses": 3.433918, "train_accs": 0.246618, "test_accs": 0.246618, "grad_norms": 0.004106, "param_norms": 0.34061}, {"epoch": 450, "train_losses": 3.433909, "test_losses": 3.433909, "train_accs": 0.273673, "test_accs": 0.273673, "grad_norms": 0.004106, "param_norms": 0.340827}, {"epoch": 500, "train_losses": 3.433901, "test_losses": 3.433901, "train_accs": 0.313215, "test_accs": 0.313215, "grad_norms": 0.004107, "param_norms": 0.341068}, {"epoch": 550, "train_losses": 3.433892, "test_losses": 3.433892, "train_accs": 0.373569, "test_accs": 0.373569, "grad_norms": 0.004107, "param_norms": 0.341333}, {"epoch": 600, "train_losses": 3.433884, "test_losses": 3.433884, "train_accs": 0.419355, "test_accs": 0.419355, "grad_norms": 0.004107, "param_norms": 0.341623}, {"epoch": 650, "train_losses": 3.433876, "test_losses": 3.433876, "train_accs": 0.46514, "test_accs": 0.46514, "grad_norms": 0.004107, "param_norms": 0.341937}, {"epoch": 700, "train_losses": 3.433867, "test_losses": 3.433867, "train_accs": 0.51821, "test_accs": 0.51821, "grad_norms": 0.004107, "param_norms": 0.342276}, {"epoch": 750, "train_losses": 3.433859, "test_losses": 3.433859, "train_accs": 0.570239, "test_accs": 0.570239, "grad_norms": 0.004107, "param_norms": 0.342639}, {"epoch": 800, "train_losses": 3.43385, "test_losses": 3.43385, "train_accs": 0.620187, "test_accs": 0.620187, "grad_norms": 0.004107, "param_norms": 0.343026}, {"epoch": 850, "train_losses": 3.433842, "test_losses": 3.433842, "train_accs": 0.672216, "test_accs": 0.672216, "grad_norms": 0.004106, "param_norms": 0.343437}, {"epoch": 900, "train_losses": 3.433833, "test_losses": 3.433833, "train_accs": 0.718002, "test_accs": 0.718002, "grad_norms": 0.004106, "param_norms": 0.343872}, {"epoch": 950, "train_losses": 3.433825, "test_losses": 3.433825, "train_accs": 0.752341, "test_accs": 0.752341, "grad_norms": 0.004106, "param_norms": 0.344331}, {"epoch": 1000, "train_losses": 3.433817, "test_losses": 3.433817, "train_accs": 0.80437, "test_accs": 0.80437, "grad_norms": 0.004106, "param_norms": 0.344813}, {"epoch": 1050, "train_losses": 3.433808, "test_losses": 3.433808, "train_accs": 0.837669, "test_accs": 0.837669, "grad_norms": 0.004107, "param_norms": 0.34532}, {"epoch": 1100, "train_losses": 3.4338, "test_losses": 3.4338, "train_accs": 0.858481, "test_accs": 0.858481, "grad_norms": 0.004107, "param_norms": 0.34585}, {"epoch": 1150, "train_losses": 3.433791, "test_losses": 3.433791, "train_accs": 0.882414, "test_accs": 0.882414, "grad_norms": 0.004108, "param_norms": 0.346403}, {"epoch": 1200, "train_losses": 3.433783, "test_losses": 3.433783, "train_accs": 0.901145, "test_accs": 0.901145, "grad_norms": 0.004108, "param_norms": 0.34698}, {"epoch": 1250, "train_losses": 3.433775, "test_losses": 3.433775, "train_accs": 0.925078, "test_accs": 0.925078, "grad_norms": 0.00411, "param_norms": 0.34758}, {"epoch": 1300, "train_losses": 3.433766, "test_losses": 3.433766, "train_accs": 0.935484, "test_accs": 0.935484, "grad_norms": 0.004112, "param_norms": 0.348203}, {"epoch": 1350, "train_losses": 3.433758, "test_losses": 3.433758, "train_accs": 0.949011, "test_accs": 0.949011, "grad_norms": 0.004114, "param_norms": 0.348849}, {"epoch": 1400, "train_losses": 3.433749, "test_losses": 3.433749, "train_accs": 0.958377, "test_accs": 0.958377, "grad_norms": 0.004115, "param_norms": 0.349518}, {"epoch": 1450, "train_losses": 3.433741, "test_losses": 3.433741, "train_accs": 0.965661, "test_accs": 0.965661, "grad_norms": 0.004116, "param_norms": 0.35021}, {"epoch": 1500, "train_losses": 3.433733, "test_losses": 3.433733, "train_accs": 0.976067, "test_accs": 0.976067, "grad_norms": 0.004117, "param_norms": 0.350924}, {"epoch": 1550, "train_losses": 3.433724, "test_losses": 3.433724, "train_accs": 0.980229, "test_accs": 0.980229, "grad_norms": 0.004119, "param_norms": 0.351661}, {"epoch": 1600, "train_losses": 3.433716, "test_losses": 3.433716, "train_accs": 0.984391, "test_accs": 0.984391, "grad_norms": 0.00412, "param_norms": 0.352419}, {"epoch": 1650, "train_losses": 3.433707, "test_losses": 3.433707, "train_accs": 0.988554, "test_accs": 0.988554, "grad_norms": 0.004121, "param_norms": 0.353201}, {"epoch": 1700, "train_losses": 3.433699, "test_losses": 3.433699, "train_accs": 0.990635, "test_accs": 0.990635, "grad_norms": 0.004126, "param_norms": 0.354004}, {"epoch": 1750, "train_losses": 3.433691, "test_losses": 3.433691, "train_accs": 0.993757, "test_accs": 0.993757, "grad_norms": 0.004129, "param_norms": 0.354829}, {"epoch": 1800, "train_losses": 3.433682, "test_losses": 3.433682, "train_accs": 0.993757, "test_accs": 0.993757, "grad_norms": 0.004132, "param_norms": 0.355675}, {"epoch": 1850, "train_losses": 3.433674, "test_losses": 3.433674, "train_accs": 0.993757, "test_accs": 0.993757, "grad_norms": 0.004135, "param_norms": 0.356543}, {"epoch": 1900, "train_losses": 3.433665, "test_losses": 3.433665, "train_accs": 0.993757, "test_accs": 0.993757, "grad_norms": 0.004137, "param_norms": 0.357433}, {"epoch": 1950, "train_losses": 3.433657, "test_losses": 3.433657, "train_accs": 0.994797, "test_accs": 0.994797, "grad_norms": 0.00414, "param_norms": 0.358344}, {"epoch": 2000, "train_losses": 3.433649, "test_losses": 3.433649, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004144, "param_norms": 0.359276}, {"epoch": 2050, "train_losses": 3.43364, "test_losses": 3.43364, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004149, "param_norms": 0.360229}, {"epoch": 2100, "train_losses": 3.433632, "test_losses": 3.433632, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004152, "param_norms": 0.361202}, {"epoch": 2150, "train_losses": 3.433623, "test_losses": 3.433623, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004155, "param_norms": 0.362197}, {"epoch": 2200, "train_losses": 3.433615, "test_losses": 3.433615, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004159, "param_norms": 0.363211}, {"epoch": 2250, "train_losses": 3.433606, "test_losses": 3.433606, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004164, "param_norms": 0.364247}, {"epoch": 2300, "train_losses": 3.433598, "test_losses": 3.433598, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004172, "param_norms": 0.365302}, {"epoch": 2350, "train_losses": 3.433589, "test_losses": 3.433589, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004178, "param_norms": 0.366378}, {"epoch": 2400, "train_losses": 3.433581, "test_losses": 3.433581, "train_accs": 0.996878, "test_accs": 0.996878, "grad_norms": 0.004183, "param_norms": 0.367473}, {"epoch": 2450, "train_losses": 3.433572, "test_losses": 3.433572, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004188, "param_norms": 0.368588}, {"epoch": 2500, "train_losses": 3.433564, "test_losses": 3.433564, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004193, "param_norms": 0.369723}, {"epoch": 2550, "train_losses": 3.433556, "test_losses": 3.433556, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004199, "param_norms": 0.370877}, {"epoch": 2600, "train_losses": 3.433547, "test_losses": 3.433547, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004208, "param_norms": 0.37205}, {"epoch": 2650, "train_losses": 3.433539, "test_losses": 3.433539, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004213, "param_norms": 0.373242}, {"epoch": 2700, "train_losses": 3.43353, "test_losses": 3.43353, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004219, "param_norms": 0.374454}, {"epoch": 2750, "train_losses": 3.433521, "test_losses": 3.433521, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004228, "param_norms": 0.375684}, {"epoch": 2800, "train_losses": 3.433513, "test_losses": 3.433513, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004233, "param_norms": 0.376933}, {"epoch": 2850, "train_losses": 3.433505, "test_losses": 3.433505, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004241, "param_norms": 0.3782}, {"epoch": 2900, "train_losses": 3.433496, "test_losses": 3.433496, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004249, "param_norms": 0.379486}, {"epoch": 2950, "train_losses": 3.433487, "test_losses": 3.433487, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004258, "param_norms": 0.38079}, {"epoch": 3000, "train_losses": 3.433478, "test_losses": 3.433478, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00427, "param_norms": 0.382112}, {"epoch": 3050, "train_losses": 3.43347, "test_losses": 3.43347, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004282, "param_norms": 0.383452}, {"epoch": 3100, "train_losses": 3.433461, "test_losses": 3.433461, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004292, "param_norms": 0.38481}, {"epoch": 3150, "train_losses": 3.433453, "test_losses": 3.433453, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004305, "param_norms": 0.386185}, {"epoch": 3200, "train_losses": 3.433444, "test_losses": 3.433444, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004313, "param_norms": 0.387578}, {"epoch": 3250, "train_losses": 3.433435, "test_losses": 3.433435, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004324, "param_norms": 0.388989}, {"epoch": 3300, "train_losses": 3.433426, "test_losses": 3.433426, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004335, "param_norms": 0.390416}, {"epoch": 3350, "train_losses": 3.433418, "test_losses": 3.433418, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004344, "param_norms": 0.391861}, {"epoch": 3400, "train_losses": 3.433409, "test_losses": 3.433409, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004358, "param_norms": 0.393323}, {"epoch": 3450, "train_losses": 3.4334, "test_losses": 3.4334, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004371, "param_norms": 0.394802}, {"epoch": 3500, "train_losses": 3.433391, "test_losses": 3.433391, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004382, "param_norms": 0.396298}, {"epoch": 3550, "train_losses": 3.433382, "test_losses": 3.433382, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004392, "param_norms": 0.397811}, {"epoch": 3600, "train_losses": 3.433373, "test_losses": 3.433373, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004405, "param_norms": 0.39934}, {"epoch": 3650, "train_losses": 3.433364, "test_losses": 3.433364, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004416, "param_norms": 0.400885}, {"epoch": 3700, "train_losses": 3.433356, "test_losses": 3.433356, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004433, "param_norms": 0.402447}, {"epoch": 3750, "train_losses": 3.433346, "test_losses": 3.433346, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004443, "param_norms": 0.404025}, {"epoch": 3800, "train_losses": 3.433337, "test_losses": 3.433337, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004455, "param_norms": 0.40562}, {"epoch": 3850, "train_losses": 3.433328, "test_losses": 3.433328, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004471, "param_norms": 0.40723}, {"epoch": 3900, "train_losses": 3.433319, "test_losses": 3.433319, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004487, "param_norms": 0.408857}, {"epoch": 3950, "train_losses": 3.43331, "test_losses": 3.43331, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004499, "param_norms": 0.410499}, {"epoch": 4000, "train_losses": 3.4333, "test_losses": 3.4333, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004514, "param_norms": 0.412157}, {"epoch": 4050, "train_losses": 3.433291, "test_losses": 3.433291, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004528, "param_norms": 0.413831}, {"epoch": 4100, "train_losses": 3.433282, "test_losses": 3.433282, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00455, "param_norms": 0.415519}, {"epoch": 4150, "train_losses": 3.433273, "test_losses": 3.433273, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004566, "param_norms": 0.417224}, {"epoch": 4200, "train_losses": 3.433264, "test_losses": 3.433264, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004584, "param_norms": 0.418945}, {"epoch": 4250, "train_losses": 3.433254, "test_losses": 3.433254, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004598, "param_norms": 0.42068}, {"epoch": 4300, "train_losses": 3.433244, "test_losses": 3.433244, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004613, "param_norms": 0.42243}, {"epoch": 4350, "train_losses": 3.433235, "test_losses": 3.433235, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004636, "param_norms": 0.424196}, {"epoch": 4400, "train_losses": 3.433225, "test_losses": 3.433225, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004652, "param_norms": 0.425977}, {"epoch": 4450, "train_losses": 3.433216, "test_losses": 3.433216, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004671, "param_norms": 0.427773}, {"epoch": 4500, "train_losses": 3.433206, "test_losses": 3.433206, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004694, "param_norms": 0.429584}, {"epoch": 4550, "train_losses": 3.433196, "test_losses": 3.433196, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004707, "param_norms": 0.431409}, {"epoch": 4600, "train_losses": 3.433187, "test_losses": 3.433187, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004725, "param_norms": 0.43325}, {"epoch": 4650, "train_losses": 3.433177, "test_losses": 3.433177, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004739, "param_norms": 0.435105}, {"epoch": 4700, "train_losses": 3.433167, "test_losses": 3.433167, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004763, "param_norms": 0.436974}, {"epoch": 4750, "train_losses": 3.433157, "test_losses": 3.433157, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004785, "param_norms": 0.438859}, {"epoch": 4800, "train_losses": 3.433147, "test_losses": 3.433147, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004804, "param_norms": 0.440758}, {"epoch": 4850, "train_losses": 3.433137, "test_losses": 3.433137, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00482, "param_norms": 0.442671}, {"epoch": 4900, "train_losses": 3.433127, "test_losses": 3.433127, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004838, "param_norms": 0.444599}, {"epoch": 4950, "train_losses": 3.433117, "test_losses": 3.433117, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004859, "param_norms": 0.446541}, {"epoch": 4999, "train_losses": 3.433106, "test_losses": 3.433106, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004874, "param_norms": 0.448459}], "total_epochs": 5000}}