{"standard": {"config": {"prime": 23, "d_mlp": 512, "act_type": "ReLU", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 5e-05, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=23, run=standard\n======================================================================\n\nConfiguration:\n  prime (p)       = 23\n  d_mlp           = 512\n  activation      = ReLU\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 5e-05\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.135561      3.135561      0.0435      0.0435      0.0184       3.2092\n      50      3.131050      3.131050      0.1040      0.1040      0.0182       3.2376\n     100      3.125871      3.125871      0.2250      0.2250      0.0189       3.3492\n     150      3.120215      3.120215      0.3611      0.3611      0.0204       3.5243\n     200      3.113867      3.113867      0.5198      0.5198      0.0225       3.7451\n     250      3.106643      3.106643      0.6749      0.6749      0.0252       3.9974\n     300      3.098425      3.098425      0.7732      0.7732      0.0282       4.2711\n     350      3.089147      3.089147      0.8299      0.8299      0.0314       4.5595\n     400      3.078748      3.078748      0.8904      0.8904      0.0349       4.8586\n     450      3.067176      3.067176      0.9187      0.9187      0.0385       5.1660\n     500      3.054400      3.054400      0.9301      0.9301      0.0423       5.4795\n     550      3.040395      3.040395      0.9357      0.9357      0.0461       5.7975\n     600      3.025165      3.025165      0.9509      0.9509      0.0500       6.1191\n     650      3.008712      3.008712      0.9565      0.9565      0.0540       6.4440\n     700      2.991030      2.991030      0.9641      0.9641      0.0579       6.7716\n     750      2.972139      2.972139      0.9641      0.9641      0.0619       7.1017\n     800      2.952033      2.952033      0.9641      0.9641      0.0657       7.4341\n     850      2.930725      2.930725      0.9641      0.9641      0.0696       7.7684\n     900      2.908199      2.908199      0.9641      0.9641      0.0735       8.1045\n     950      2.884462      2.884462      0.9641      0.9641      0.0774       8.4420\n    1000      2.859511      2.859511      0.9641      0.9641      0.0814       8.7806\n    1050      2.833335      2.833335      0.9641      0.9641      0.0853       9.1202\n    1100      2.805946      2.805946      0.9622      0.9622      0.0894       9.4609\n    1150      2.777344      2.777344      0.9641      0.9641      0.0933       9.8024\n    1200      2.747575      2.747575      0.9641      0.9641      0.0972      10.1450\n    1250      2.716638      2.716638      0.9641      0.9641      0.1010      10.4883\n    1300      2.684527      2.684527      0.9641      0.9641      0.1049      10.8320\n    1350      2.651273      2.651273      0.9679      0.9679      0.1087      11.1758\n    1400      2.616875      2.616875      0.9660      0.9660      0.1125      11.5197\n    1450      2.581358      2.581358      0.9660      0.9660      0.1162      11.8638\n    1500      2.544763      2.544763      0.9660      0.9660      0.1198      12.2084\n    1550      2.507086      2.507086      0.9660      0.9660      0.1234      12.5533\n    1600      2.468355      2.468355      0.9660      0.9660      0.1270      12.8983\n    1650      2.428575      2.428575      0.9660      0.9660      0.1304      13.2431\n    1700      2.387768      2.387768      0.9679      0.9679      0.1339      13.5877\n    1750      2.345973      2.345973      0.9679      0.9679      0.1374      13.9325\n    1800      2.303205      2.303205      0.9679      0.9679      0.1405      14.2771\n    1850      2.259507      2.259507      0.9679      0.9679      0.1438      14.6216\n    1900      2.214904      2.214904      0.9679      0.9679      0.1467      14.9660\n    1950      2.169407      2.169407      0.9698      0.9698      0.1498      15.3102\n    2000      2.123073      2.123073      0.9698      0.9698      0.1526      15.6542\n    2050      2.075953      2.075953      0.9698      0.9698      0.1555      15.9978\n    2100      2.028098      2.028098      0.9698      0.9698      0.1580      16.3414\n    2150      1.979552      1.979552      0.9716      0.9716      0.1604      16.6850\n    2200      1.930340      1.930340      0.9716      0.9716      0.1627      17.0287\n    2250      1.880527      1.880527      0.9716      0.9716      0.1647      17.3721\n    2300      1.830198      1.830198      0.9735      0.9735      0.1670      17.7153\n    2350      1.779385      1.779385      0.9735      0.9735      0.1685      18.0576\n    2400      1.728140      1.728140      0.9735      0.9735      0.1701      18.3991\n    2450      1.676525      1.676525      0.9735      0.9735      0.1713      18.7401\n    2500      1.624630      1.624630      0.9754      0.9754      0.1726      19.0806\n    2550      1.572517      1.572517      0.9773      0.9773      0.1735      19.4206\n    2600      1.520268      1.520268      0.9773      0.9773      0.1740      19.7599\n    2650      1.467964      1.467964      0.9773      0.9773      0.1746      20.0984\n    2700      1.415667      1.415667      0.9773      0.9773      0.1746      20.4361\n    2750      1.363486      1.363486      0.9792      0.9792      0.1745      20.7733\n    2800      1.311482      1.311482      0.9811      0.9811      0.1740      21.1098\n    2850      1.259736      1.259736      0.9811      0.9811      0.1733      21.4456\n    2900      1.208353      1.208353      0.9830      0.9830      0.1722      21.7806\n    2950      1.157414      1.157414      0.9830      0.9830      0.1708      22.1147\n    3000      1.107026      1.107026      0.9849      0.9849      0.1695      22.4479\n    3050      1.057268      1.057268      0.9868      0.9868      0.1674      22.7802\n    3100      1.008225      1.008225      0.9887      0.9887      0.1652      23.1114\n    3150      0.960013      0.960013      0.9924      0.9924      0.1628      23.4415\n    3200      0.912727      0.912727      0.9924      0.9924      0.1598      23.7702\n    3250      0.866392      0.866392      0.9924      0.9924      0.1568      24.0974\n    3300      0.821100      0.821100      0.9924      0.9924      0.1534      24.4232\n    3350      0.776916      0.776916      0.9924      0.9924      0.1497      24.7476\n    3400      0.733911      0.733911      0.9943      0.9943      0.1459      25.0704\n    3450      0.692123      0.692123      0.9943      0.9943      0.1420      25.3917\n    3500      0.651614      0.651614      0.9962      0.9962      0.1377      25.7115\n    3550      0.612438      0.612438      0.9981      0.9981      0.1334      26.0299\n    3600      0.574631      0.574631      0.9981      0.9981      0.1288      26.3467\n    3650      0.538226      0.538226      0.9981      0.9981      0.1241      26.6622\n    3700      0.503222      0.503222      0.9981      0.9981      0.1194      26.9761\n    3750      0.469629      0.469629      0.9981      0.9981      0.1147      27.2886\n    3800      0.437465      0.437465      1.0000      1.0000      0.1098      27.5997\n    3850      0.406748      0.406748      1.0000      1.0000      0.1049      27.9094\n    3900      0.377463      0.377463      1.0000      1.0000      0.1001      28.2175\n    3950      0.349618      0.349618      1.0000      1.0000      0.0952      28.5245\n    4000      0.323210      0.323210      1.0000      1.0000      0.0904      28.8306\n    4050      0.298225      0.298225      1.0000      1.0000      0.0857      29.1358\n    4100      0.274650      0.274650      1.0000      1.0000      0.0810      29.4405\n    4150      0.252467      0.252467      1.0000      1.0000      0.0762      29.7449\n    4200      0.231657      0.231657      1.0000      1.0000      0.0717      30.0492\n    4250      0.212189      0.212189      1.0000      1.0000      0.0671      30.3535\n    4300      0.194017      0.194017      1.0000      1.0000      0.0627      30.6580\n    4350      0.177100      0.177100      1.0000      1.0000      0.0585      30.9627\n    4400      0.161399      0.161399      1.0000      1.0000      0.0543      31.2678\n    4450      0.146860      0.146860      1.0000      1.0000      0.0504      31.5730\n    4500      0.133428      0.133428      1.0000      1.0000      0.0467      31.8784\n    4550      0.121054      0.121054      1.0000      1.0000      0.0431      32.1839\n    4600      0.109677      0.109677      1.0000      1.0000      0.0397      32.4893\n    4650      0.099230      0.099230      1.0000      1.0000      0.0365      32.7946\n    4700      0.089664      0.089664      1.0000      1.0000      0.0335      33.0998\n    4750      0.080919      0.080919      1.0000      1.0000      0.0307      33.4048\n    4800      0.072938      0.072938      1.0000      1.0000      0.0281      33.7097\n    4850      0.065665      0.065665      1.0000      1.0000      0.0256      34.0143\n    4900      0.059048      0.059048      1.0000      1.0000      0.0233      34.3184\n    4950      0.053039      0.053039      1.0000      1.0000      0.0212      34.6220\n    4999      0.047694      0.047694      1.0000      1.0000      0.0194      34.9192\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.047694\n  Test Loss   = 0.047694\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 34.9192\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.135561, "test_losses": 3.135561, "train_accs": 0.043478, "test_accs": 0.043478, "grad_norms": 0.018382, "param_norms": 3.209196}, {"epoch": 50, "train_losses": 3.13105, "test_losses": 3.13105, "train_accs": 0.10397, "test_accs": 0.10397, "grad_norms": 0.018229, "param_norms": 3.23759}, {"epoch": 100, "train_losses": 3.125871, "test_losses": 3.125871, "train_accs": 0.224953, "test_accs": 0.224953, "grad_norms": 0.018947, "param_norms": 3.349206}, {"epoch": 150, "train_losses": 3.120215, "test_losses": 3.120215, "train_accs": 0.361059, "test_accs": 0.361059, "grad_norms": 0.020396, "param_norms": 3.524294}, {"epoch": 200, "train_losses": 3.113867, "test_losses": 3.113867, "train_accs": 0.519849, "test_accs": 0.519849, "grad_norms": 0.022541, "param_norms": 3.745119}, {"epoch": 250, "train_losses": 3.106643, "test_losses": 3.106643, "train_accs": 0.674858, "test_accs": 0.674858, "grad_norms": 0.025207, "param_norms": 3.997425}, {"epoch": 300, "train_losses": 3.098425, "test_losses": 3.098425, "train_accs": 0.773157, "test_accs": 0.773157, "grad_norms": 0.028205, "param_norms": 4.271121}, {"epoch": 350, "train_losses": 3.089147, "test_losses": 3.089147, "train_accs": 0.829868, "test_accs": 0.829868, "grad_norms": 0.03142, "param_norms": 4.559536}, {"epoch": 400, "train_losses": 3.078748, "test_losses": 3.078748, "train_accs": 0.890359, "test_accs": 0.890359, "grad_norms": 0.034894, "param_norms": 4.858556}, {"epoch": 450, "train_losses": 3.067176, "test_losses": 3.067176, "train_accs": 0.918715, "test_accs": 0.918715, "grad_norms": 0.038528, "param_norms": 5.165956}, {"epoch": 500, "train_losses": 3.0544, "test_losses": 3.0544, "train_accs": 0.930057, "test_accs": 0.930057, "grad_norms": 0.042286, "param_norms": 5.47954}, {"epoch": 550, "train_losses": 3.040395, "test_losses": 3.040395, "train_accs": 0.935728, "test_accs": 0.935728, "grad_norms": 0.046134, "param_norms": 5.797543}, {"epoch": 600, "train_losses": 3.025165, "test_losses": 3.025165, "train_accs": 0.950851, "test_accs": 0.950851, "grad_norms": 0.050038, "param_norms": 6.119105}, {"epoch": 650, "train_losses": 3.008712, "test_losses": 3.008712, "train_accs": 0.956522, "test_accs": 0.956522, "grad_norms": 0.054017, "param_norms": 6.443969}, {"epoch": 700, "train_losses": 2.99103, "test_losses": 2.99103, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.057903, "param_norms": 6.771608}, {"epoch": 750, "train_losses": 2.972139, "test_losses": 2.972139, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.061861, "param_norms": 7.101705}, {"epoch": 800, "train_losses": 2.952033, "test_losses": 2.952033, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.065718, "param_norms": 7.434052}, {"epoch": 850, "train_losses": 2.930725, "test_losses": 2.930725, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.069609, "param_norms": 7.76843}, {"epoch": 900, "train_losses": 2.908199, "test_losses": 2.908199, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.073522, "param_norms": 8.104532}, {"epoch": 950, "train_losses": 2.884462, "test_losses": 2.884462, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.077429, "param_norms": 8.44201}, {"epoch": 1000, "train_losses": 2.859511, "test_losses": 2.859511, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.081429, "param_norms": 8.780566}, {"epoch": 1050, "train_losses": 2.833335, "test_losses": 2.833335, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.08532, "param_norms": 9.120176}, {"epoch": 1100, "train_losses": 2.805946, "test_losses": 2.805946, "train_accs": 0.962193, "test_accs": 0.962193, "grad_norms": 0.089398, "param_norms": 9.460855}, {"epoch": 1150, "train_losses": 2.777344, "test_losses": 2.777344, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.093311, "param_norms": 9.802434}, {"epoch": 1200, "train_losses": 2.747575, "test_losses": 2.747575, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.097159, "param_norms": 10.145026}, {"epoch": 1250, "train_losses": 2.716638, "test_losses": 2.716638, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.101031, "param_norms": 10.488324}, {"epoch": 1300, "train_losses": 2.684527, "test_losses": 2.684527, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.104895, "param_norms": 10.831982}, {"epoch": 1350, "train_losses": 2.651273, "test_losses": 2.651273, "train_accs": 0.967864, "test_accs": 0.967864, "grad_norms": 0.108682, "param_norms": 11.17582}, {"epoch": 1400, "train_losses": 2.616875, "test_losses": 2.616875, "train_accs": 0.965974, "test_accs": 0.965974, "grad_norms": 0.112506, "param_norms": 11.519722}, {"epoch": 1450, "train_losses": 2.581358, "test_losses": 2.581358, "train_accs": 0.965974, "test_accs": 0.965974, "grad_norms": 0.116159, "param_norms": 11.863803}, {"epoch": 1500, "train_losses": 2.544763, "test_losses": 2.544763, "train_accs": 0.965974, "test_accs": 0.965974, "grad_norms": 0.119799, "param_norms": 12.208392}, {"epoch": 1550, "train_losses": 2.507086, "test_losses": 2.507086, "train_accs": 0.965974, "test_accs": 0.965974, "grad_norms": 0.123362, "param_norms": 12.553338}, {"epoch": 1600, "train_losses": 2.468355, "test_losses": 2.468355, "train_accs": 0.965974, "test_accs": 0.965974, "grad_norms": 0.127001, "param_norms": 12.898346}, {"epoch": 1650, "train_losses": 2.428575, "test_losses": 2.428575, "train_accs": 0.965974, "test_accs": 0.965974, "grad_norms": 0.130389, "param_norms": 13.243057}, {"epoch": 1700, "train_losses": 2.387768, "test_losses": 2.387768, "train_accs": 0.967864, "test_accs": 0.967864, "grad_norms": 0.133861, "param_norms": 13.587723}, {"epoch": 1750, "train_losses": 2.345973, "test_losses": 2.345973, "train_accs": 0.967864, "test_accs": 0.967864, "grad_norms": 0.137354, "param_norms": 13.932505}, {"epoch": 1800, "train_losses": 2.303205, "test_losses": 2.303205, "train_accs": 0.967864, "test_accs": 0.967864, "grad_norms": 0.140487, "param_norms": 14.277119}, {"epoch": 1850, "train_losses": 2.259507, "test_losses": 2.259507, "train_accs": 0.967864, "test_accs": 0.967864, "grad_norms": 0.143756, "param_norms": 14.621643}, {"epoch": 1900, "train_losses": 2.214904, "test_losses": 2.214904, "train_accs": 0.967864, "test_accs": 0.967864, "grad_norms": 0.146704, "param_norms": 14.965977}, {"epoch": 1950, "train_losses": 2.169407, "test_losses": 2.169407, "train_accs": 0.969754, "test_accs": 0.969754, "grad_norms": 0.149795, "param_norms": 15.310162}, {"epoch": 2000, "train_losses": 2.123073, "test_losses": 2.123073, "train_accs": 0.969754, "test_accs": 0.969754, "grad_norms": 0.152564, "param_norms": 15.654157}, {"epoch": 2050, "train_losses": 2.075953, "test_losses": 2.075953, "train_accs": 0.969754, "test_accs": 0.969754, "grad_norms": 0.155461, "param_norms": 15.99785}, {"epoch": 2100, "train_losses": 2.028098, "test_losses": 2.028098, "train_accs": 0.969754, "test_accs": 0.969754, "grad_norms": 0.157957, "param_norms": 16.341398}, {"epoch": 2150, "train_losses": 1.979552, "test_losses": 1.979552, "train_accs": 0.971645, "test_accs": 0.971645, "grad_norms": 0.160365, "param_norms": 16.685032}, {"epoch": 2200, "train_losses": 1.93034, "test_losses": 1.93034, "train_accs": 0.971645, "test_accs": 0.971645, "grad_norms": 0.162695, "param_norms": 17.028712}, {"epoch": 2250, "train_losses": 1.880527, "test_losses": 1.880527, "train_accs": 0.971645, "test_accs": 0.971645, "grad_norms": 0.164687, "param_norms": 17.372088}, {"epoch": 2300, "train_losses": 1.830198, "test_losses": 1.830198, "train_accs": 0.973535, "test_accs": 0.973535, "grad_norms": 0.167046, "param_norms": 17.715281}, {"epoch": 2350, "train_losses": 1.779385, "test_losses": 1.779385, "train_accs": 0.973535, "test_accs": 0.973535, "grad_norms": 0.168537, "param_norms": 18.057604}, {"epoch": 2400, "train_losses": 1.72814, "test_losses": 1.72814, "train_accs": 0.973535, "test_accs": 0.973535, "grad_norms": 0.17012, "param_norms": 18.399116}, {"epoch": 2450, "train_losses": 1.676525, "test_losses": 1.676525, "train_accs": 0.973535, "test_accs": 0.973535, "grad_norms": 0.171331, "param_norms": 18.740144}, {"epoch": 2500, "train_losses": 1.62463, "test_losses": 1.62463, "train_accs": 0.975425, "test_accs": 0.975425, "grad_norms": 0.17264, "param_norms": 19.080644}, {"epoch": 2550, "train_losses": 1.572517, "test_losses": 1.572517, "train_accs": 0.977316, "test_accs": 0.977316, "grad_norms": 0.173518, "param_norms": 19.420568}, {"epoch": 2600, "train_losses": 1.520268, "test_losses": 1.520268, "train_accs": 0.977316, "test_accs": 0.977316, "grad_norms": 0.174035, "param_norms": 19.759888}, {"epoch": 2650, "train_losses": 1.467964, "test_losses": 1.467964, "train_accs": 0.977316, "test_accs": 0.977316, "grad_norms": 0.174553, "param_norms": 20.098404}, {"epoch": 2700, "train_losses": 1.415667, "test_losses": 1.415667, "train_accs": 0.977316, "test_accs": 0.977316, "grad_norms": 0.174576, "param_norms": 20.43615}, {"epoch": 2750, "train_losses": 1.363486, "test_losses": 1.363486, "train_accs": 0.979206, "test_accs": 0.979206, "grad_norms": 0.174541, "param_norms": 20.77329}, {"epoch": 2800, "train_losses": 1.311482, "test_losses": 1.311482, "train_accs": 0.981096, "test_accs": 0.981096, "grad_norms": 0.173999, "param_norms": 21.109834}, {"epoch": 2850, "train_losses": 1.259736, "test_losses": 1.259736, "train_accs": 0.981096, "test_accs": 0.981096, "grad_norms": 0.17326, "param_norms": 21.445583}, {"epoch": 2900, "train_losses": 1.208353, "test_losses": 1.208353, "train_accs": 0.982987, "test_accs": 0.982987, "grad_norms": 0.172163, "param_norms": 21.780605}, {"epoch": 2950, "train_losses": 1.157414, "test_losses": 1.157414, "train_accs": 0.982987, "test_accs": 0.982987, "grad_norms": 0.170775, "param_norms": 22.114675}, {"epoch": 3000, "train_losses": 1.107026, "test_losses": 1.107026, "train_accs": 0.984877, "test_accs": 0.984877, "grad_norms": 0.169529, "param_norms": 22.447947}, {"epoch": 3050, "train_losses": 1.057268, "test_losses": 1.057268, "train_accs": 0.986767, "test_accs": 0.986767, "grad_norms": 0.167398, "param_norms": 22.780204}, {"epoch": 3100, "train_losses": 1.008225, "test_losses": 1.008225, "train_accs": 0.988658, "test_accs": 0.988658, "grad_norms": 0.165231, "param_norms": 23.111398}, {"epoch": 3150, "train_losses": 0.960013, "test_losses": 0.960013, "train_accs": 0.992439, "test_accs": 0.992439, "grad_norms": 0.162783, "param_norms": 23.441478}, {"epoch": 3200, "train_losses": 0.912727, "test_losses": 0.912727, "train_accs": 0.992439, "test_accs": 0.992439, "grad_norms": 0.15981, "param_norms": 23.770222}, {"epoch": 3250, "train_losses": 0.866392, "test_losses": 0.866392, "train_accs": 0.992439, "test_accs": 0.992439, "grad_norms": 0.156825, "param_norms": 24.097421}, {"epoch": 3300, "train_losses": 0.8211, "test_losses": 0.8211, "train_accs": 0.992439, "test_accs": 0.992439, "grad_norms": 0.153359, "param_norms": 24.4232}, {"epoch": 3350, "train_losses": 0.776916, "test_losses": 0.776916, "train_accs": 0.992439, "test_accs": 0.992439, "grad_norms": 0.149733, "param_norms": 24.747565}, {"epoch": 3400, "train_losses": 0.733911, "test_losses": 0.733911, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.145894, "param_norms": 25.070442}, {"epoch": 3450, "train_losses": 0.692123, "test_losses": 0.692123, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.142012, "param_norms": 25.391738}, {"epoch": 3500, "train_losses": 0.651614, "test_losses": 0.651614, "train_accs": 0.996219, "test_accs": 0.996219, "grad_norms": 0.137658, "param_norms": 25.711547}, {"epoch": 3550, "train_losses": 0.612438, "test_losses": 0.612438, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.133383, "param_norms": 26.029864}, {"epoch": 3600, "train_losses": 0.574631, "test_losses": 0.574631, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.128765, "param_norms": 26.346745}, {"epoch": 3650, "train_losses": 0.538226, "test_losses": 0.538226, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.124104, "param_norms": 26.662196}, {"epoch": 3700, "train_losses": 0.503222, "test_losses": 0.503222, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.119409, "param_norms": 26.976126}, {"epoch": 3750, "train_losses": 0.469629, "test_losses": 0.469629, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.114734, "param_norms": 27.288602}, {"epoch": 3800, "train_losses": 0.437465, "test_losses": 0.437465, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.10978, "param_norms": 27.599682}, {"epoch": 3850, "train_losses": 0.406748, "test_losses": 0.406748, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.104943, "param_norms": 27.909393}, {"epoch": 3900, "train_losses": 0.377463, "test_losses": 0.377463, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.100102, "param_norms": 28.217477}, {"epoch": 3950, "train_losses": 0.349618, "test_losses": 0.349618, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.095174, "param_norms": 28.524495}, {"epoch": 4000, "train_losses": 0.32321, "test_losses": 0.32321, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.090432, "param_norms": 28.830594}, {"epoch": 4050, "train_losses": 0.298225, "test_losses": 0.298225, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.085742, "param_norms": 29.135792}, {"epoch": 4100, "train_losses": 0.27465, "test_losses": 0.27465, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.08101, "param_norms": 29.440478}, {"epoch": 4150, "train_losses": 0.252467, "test_losses": 0.252467, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.076167, "param_norms": 29.744861}, {"epoch": 4200, "train_losses": 0.231657, "test_losses": 0.231657, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.071701, "param_norms": 30.049184}, {"epoch": 4250, "train_losses": 0.212189, "test_losses": 0.212189, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.067149, "param_norms": 30.353511}, {"epoch": 4300, "train_losses": 0.194017, "test_losses": 0.194017, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.062719, "param_norms": 30.658019}, {"epoch": 4350, "train_losses": 0.1771, "test_losses": 0.1771, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.058483, "param_norms": 30.962737}, {"epoch": 4400, "train_losses": 0.161399, "test_losses": 0.161399, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.054328, "param_norms": 31.267771}, {"epoch": 4450, "train_losses": 0.14686, "test_losses": 0.14686, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.050436, "param_norms": 31.573023}, {"epoch": 4500, "train_losses": 0.133428, "test_losses": 0.133428, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.046671, "param_norms": 31.878429}, {"epoch": 4550, "train_losses": 0.121054, "test_losses": 0.121054, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.043076, "param_norms": 32.183874}, {"epoch": 4600, "train_losses": 0.109677, "test_losses": 0.109677, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.039743, "param_norms": 32.489297}, {"epoch": 4650, "train_losses": 0.09923, "test_losses": 0.09923, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.036503, "param_norms": 32.794631}, {"epoch": 4700, "train_losses": 0.089664, "test_losses": 0.089664, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.033472, "param_norms": 33.099832}, {"epoch": 4750, "train_losses": 0.080919, "test_losses": 0.080919, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.030676, "param_norms": 33.404823}, {"epoch": 4800, "train_losses": 0.072938, "test_losses": 0.072938, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.028054, "param_norms": 33.709682}, {"epoch": 4850, "train_losses": 0.065665, "test_losses": 0.065665, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.025639, "param_norms": 34.014286}, {"epoch": 4900, "train_losses": 0.059048, "test_losses": 0.059048, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.023333, "param_norms": 34.318392}, {"epoch": 4950, "train_losses": 0.053039, "test_losses": 0.053039, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.021229, "param_norms": 34.621985}, {"epoch": 4999, "train_losses": 0.047694, "test_losses": 0.047694, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.019356, "param_norms": 34.91921}], "total_epochs": 5000}, "grokking": {"config": {"prime": 23, "d_mlp": 512, "act_type": "ReLU", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 0.0001, "weight_decay": 2.0, "frac_train": 0.75, "num_epochs": 50000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=23, run=grokking\n======================================================================\n\nConfiguration:\n  prime (p)       = 23\n  d_mlp           = 512\n  activation      = ReLU\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 0.0001\n  weight_decay    = 2.0\n  frac_train      = 0.75\n  num_epochs      = 50000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.135788      3.134885      0.0455      0.0376      0.0245       3.2092\n     240      3.044765      3.202619      0.3283      0.0075      0.0455       5.1706\n     480      2.863955      3.279887      0.6465      0.0075      0.0777       8.1716\n     720      2.609890      3.333790      0.7348      0.0150      0.1065      11.0874\n     960      2.304452      3.352303      0.7348      0.0226      0.1297      13.8591\n    1200      1.969475      3.319542      0.7601      0.0677      0.1460      16.4561\n    1440      1.627926      3.220526      0.8182      0.2180      0.1539      18.8593\n    1680      1.298562      3.041184      0.9242      0.5188      0.1532      21.0633\n    1920      0.996659      2.784536      0.9848      0.6917      0.1452      23.0892\n    2160      0.731639      2.478990      0.9975      0.7293      0.1305      24.9657\n    2400      0.512026      2.179595      1.0000      0.7368      0.1112      26.7355\n    2640      0.342097      1.927318      1.0000      0.7368      0.0882      28.4186\n    2880      0.219931      1.738867      1.0000      0.7368      0.0655      30.0205\n    3120      0.137737      1.609190      1.0000      0.7368      0.0459      31.5437\n    3360      0.084927      1.520576      1.0000      0.7368      0.0309      32.9898\n    3600      0.051950      1.454422      1.0000      0.7368      0.0203      34.3584\n    3840      0.031712      1.402243      1.0000      0.7368      0.0131      35.6569\n    4080      0.019389      1.358229      1.0000      0.7368      0.0084      36.8912\n    4320      0.011889      1.316100      1.0000      0.7368      0.0054      38.0660\n    4560      0.007326      1.274262      1.0000      0.7368      0.0034      39.1901\n    4800      0.004544      1.230890      1.0000      0.7368      0.0022      40.2672\n    5040      0.002838      1.184900      1.0000      0.7368      0.0014      41.2985\n    5280      0.001786      1.136832      1.0000      0.7368      0.0009      42.2839\n    5520      0.001133      1.089857      1.0000      0.7519      0.0006      43.2268\n    5760      0.000726      1.042044      1.0000      0.7820      0.0004      44.1282\n    6000      0.000470      0.996239      1.0000      0.7820      0.0003      44.9863\n    6240      0.000308      0.950361      1.0000      0.7820      0.0002      45.7979\n    6480      0.000204      0.903681      1.0000      0.7820      0.0001      46.5588\n    6720      0.000137      0.856100      1.0000      0.7820      0.0001      47.2684\n    6960      0.000094      0.809337      1.0000      0.7820      0.0001      47.9230\n    7200      0.000066      0.762877      1.0000      0.7970      0.0000      48.5171\n    7440      0.000047      0.716994      1.0000      0.8271      0.0000      49.0444\n    7680      0.000034      0.672391      1.0000      0.8421      0.0000      49.4989\n    7920      0.000026      0.628780      1.0000      0.8872      0.0000      49.8764\n    8160      0.000020      0.587391      1.0000      0.9023      0.0000      50.1779\n    8400      0.000017      0.547753      1.0000      0.9023      0.0000      50.4054\n    8640      0.000014      0.510860      1.0000      0.9023      0.0000      50.5641\n    8880      0.000012      0.477347      1.0000      0.9323      0.0000      50.6625\n    9120      0.000011      0.448372      1.0000      0.9323      0.0000      50.7125\n    9360      0.000010      0.422513      1.0000      0.9323      0.0000      50.7227\n    9600      0.000009      0.399118      1.0000      0.9624      0.0000      50.7035\n    9840      0.000009      0.377931      1.0000      0.9624      0.0000      50.6601\n   10080      0.000008      0.359181      1.0000      0.9624      0.0000      50.6000\n   10320      0.000008      0.342359      1.0000      0.9624      0.0000      50.5295\n   10560      0.000008      0.326894      1.0000      0.9624      0.0000      50.4519\n   10800      0.000008      0.312899      1.0000      0.9624      0.0000      50.3706\n   11040      0.000008      0.300169      1.0000      0.9624      0.0000      50.2871\n   11280      0.000008      0.288715      1.0000      0.9624      0.0000      50.2032\n   11520      0.000007      0.278343      1.0000      0.9624      0.0000      50.1218\n   11760      0.000007      0.268877      1.0000      0.9624      0.0000      50.0417\n   12000      0.000007      0.260429      1.0000      0.9624      0.0000      49.9645\n   12240      0.000007      0.253031      1.0000      0.9624      0.0000      49.8904\n   12480      0.000007      0.246451      1.0000      0.9624      0.0000      49.8196\n   12720      0.000007      0.240401      1.0000      0.9624      0.0000      49.7522\n   12960      0.000007      0.234875      1.0000      0.9624      0.0000      49.6880\n   13200      0.000007      0.229973      1.0000      0.9624      0.0000      49.6270\n   13440      0.000007      0.225422      1.0000      0.9624      0.0000      49.5693\n   13680      0.000007      0.221136      1.0000      0.9624      0.0000      49.5139\n   13920      0.000007      0.217057      1.0000      0.9624      0.0000      49.4611\n   14160      0.000007      0.213229      1.0000      0.9624      0.0000      49.4110\n   14400      0.000007      0.209691      1.0000      0.9624      0.0000      49.3633\n   14640      0.000007      0.206428      1.0000      0.9624      0.0000      49.3184\n   14880      0.000007      0.203423      1.0000      0.9624      0.0000      49.2759\n   15120      0.000007      0.200598      1.0000      0.9624      0.0000      49.2350\n   15360      0.000007      0.197951      1.0000      0.9624      0.0000      49.1963\n   15600      0.000007      0.195402      1.0000      0.9624      0.0000      49.1593\n   15840      0.000007      0.192916      1.0000      0.9624      0.0000      49.1234\n   16080      0.000007      0.190596      1.0000      0.9624      0.0000      49.0885\n   16320      0.000007      0.188481      1.0000      0.9624      0.0000      49.0551\n   16560      0.000007      0.186484      1.0000      0.9624      0.0000      49.0239\n   16800      0.000007      0.184701      1.0000      0.9624      0.0000      48.9942\n   17040      0.000007      0.183305      1.0000      0.9624      0.0000      48.9664\n   17280      0.000007      0.181962      1.0000      0.9624      0.0000      48.9390\n   17520      0.000007      0.180803      1.0000      0.9624      0.0000      48.9128\n   17760      0.000007      0.179860      1.0000      0.9624      0.0000      48.8874\n   18000      0.000007      0.178967      1.0000      0.9624      0.0000      48.8635\n   18240      0.000007      0.178177      1.0000      0.9624      0.0000      48.8408\n   18480      0.000007      0.177541      1.0000      0.9624      0.0000      48.8189\n   18720      0.000007      0.176824      1.0000      0.9624      0.0000      48.7977\n   18960      0.000007      0.176174      1.0000      0.9624      0.0000      48.7773\n   19200      0.000007      0.175552      1.0000      0.9624      0.0000      48.7575\n   19440      0.000007      0.175008      1.0000      0.9624      0.0000      48.7388\n   19680      0.000007      0.174522      1.0000      0.9624      0.0000      48.7213\n   19920      0.000007      0.174109      1.0000      0.9624      0.0000      48.7044\n   20160      0.000007      0.173762      1.0000      0.9624      0.0000      48.6882\n   20400      0.000007      0.173414      1.0000      0.9624      0.0000      48.6724\n   20640      0.000007      0.173033      1.0000      0.9624      0.0000      48.6575\n   20880      0.000007      0.172714      1.0000      0.9624      0.0000      48.6437\n   21120      0.000007      0.172355      1.0000      0.9624      0.0000      48.6304\n   21360      0.000007      0.172068      1.0000      0.9624      0.0000      48.6172\n   21600      0.000007      0.171768      1.0000      0.9624      0.0000      48.6046\n   21840      0.000007      0.171573      1.0000      0.9624      0.0000      48.5925\n   22080      0.000007      0.171403      1.0000      0.9624      0.0000      48.5811\n   22320      0.000007      0.171239      1.0000      0.9624      0.0000      48.5700\n   22560      0.000007      0.170982      1.0000      0.9624      0.0000      48.5596\n   22800      0.000007      0.170657      1.0000      0.9624      0.0000      48.5500\n   23040      0.000007      0.170401      1.0000      0.9624      0.0000      48.5408\n   23280      0.000007      0.170223      1.0000      0.9624      0.0000      48.5320\n   23520      0.000007      0.170149      1.0000      0.9624      0.0000      48.5235\n   23760      0.000007      0.170044      1.0000      0.9624      0.0000      48.5154\n   24000      0.000007      0.169871      1.0000      0.9624      0.0000      48.5074\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.000007\n  Test Loss   = 0.169871\n  Train Acc   = 1.0000\n  Test Acc    = 0.9624\n  Param Norm  = 48.5074\n\nTotal epochs trained: 24001\n", "table": [{"epoch": 0, "train_losses": 3.135788, "test_losses": 3.134885, "train_accs": 0.045455, "test_accs": 0.037594, "grad_norms": 0.024501, "param_norms": 3.209196}, {"epoch": 240, "train_losses": 3.044765, "test_losses": 3.202619, "train_accs": 0.328283, "test_accs": 0.007519, "grad_norms": 0.045498, "param_norms": 5.170639}, {"epoch": 480, "train_losses": 2.863955, "test_losses": 3.279887, "train_accs": 0.646465, "test_accs": 0.007519, "grad_norms": 0.077704, "param_norms": 8.171587}, {"epoch": 720, "train_losses": 2.60989, "test_losses": 3.33379, "train_accs": 0.734848, "test_accs": 0.015038, "grad_norms": 0.106509, "param_norms": 11.087385}, {"epoch": 960, "train_losses": 2.304452, "test_losses": 3.352303, "train_accs": 0.734848, "test_accs": 0.022556, "grad_norms": 0.129684, "param_norms": 13.859101}, {"epoch": 1200, "train_losses": 1.969475, "test_losses": 3.319542, "train_accs": 0.760101, "test_accs": 0.067669, "grad_norms": 0.145989, "param_norms": 16.456113}, {"epoch": 1440, "train_losses": 1.627926, "test_losses": 3.220526, "train_accs": 0.818182, "test_accs": 0.218045, "grad_norms": 0.1539, "param_norms": 18.859278}, {"epoch": 1680, "train_losses": 1.298562, "test_losses": 3.041184, "train_accs": 0.924242, "test_accs": 0.518797, "grad_norms": 0.153235, "param_norms": 21.063257}, {"epoch": 1920, "train_losses": 0.996659, "test_losses": 2.784536, "train_accs": 0.984848, "test_accs": 0.691729, "grad_norms": 0.145246, "param_norms": 23.089182}, {"epoch": 2160, "train_losses": 0.731639, "test_losses": 2.47899, "train_accs": 0.997475, "test_accs": 0.729323, "grad_norms": 0.130524, "param_norms": 24.965661}, {"epoch": 2400, "train_losses": 0.512026, "test_losses": 2.179595, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.111192, "param_norms": 26.735498}, {"epoch": 2640, "train_losses": 0.342097, "test_losses": 1.927318, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.088204, "param_norms": 28.418615}, {"epoch": 2880, "train_losses": 0.219931, "test_losses": 1.738867, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.065494, "param_norms": 30.020465}, {"epoch": 3120, "train_losses": 0.137737, "test_losses": 1.60919, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.045894, "param_norms": 31.54374}, {"epoch": 3360, "train_losses": 0.084927, "test_losses": 1.520576, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.030866, "param_norms": 32.989789}, {"epoch": 3600, "train_losses": 0.05195, "test_losses": 1.454422, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.020278, "param_norms": 34.358401}, {"epoch": 3840, "train_losses": 0.031712, "test_losses": 1.402243, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.013111, "param_norms": 35.65687}, {"epoch": 4080, "train_losses": 0.019389, "test_losses": 1.358229, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.008417, "param_norms": 36.891184}, {"epoch": 4320, "train_losses": 0.011889, "test_losses": 1.3161, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.005381, "param_norms": 38.066032}, {"epoch": 4560, "train_losses": 0.007326, "test_losses": 1.274262, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.003444, "param_norms": 39.190069}, {"epoch": 4800, "train_losses": 0.004544, "test_losses": 1.23089, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.002209, "param_norms": 40.267207}, {"epoch": 5040, "train_losses": 0.002838, "test_losses": 1.1849, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.001425, "param_norms": 41.298473}, {"epoch": 5280, "train_losses": 0.001786, "test_losses": 1.136832, "train_accs": 1.0, "test_accs": 0.736842, "grad_norms": 0.000925, "param_norms": 42.283926}, {"epoch": 5520, "train_losses": 0.001133, "test_losses": 1.089857, "train_accs": 1.0, "test_accs": 0.75188, "grad_norms": 0.000603, "param_norms": 43.226838}, {"epoch": 5760, "train_losses": 0.000726, "test_losses": 1.042044, "train_accs": 1.0, "test_accs": 0.781955, "grad_norms": 0.000396, "param_norms": 44.12818}, {"epoch": 6000, "train_losses": 0.00047, "test_losses": 0.996239, "train_accs": 1.0, "test_accs": 0.781955, "grad_norms": 0.000263, "param_norms": 44.986346}, {"epoch": 6240, "train_losses": 0.000308, "test_losses": 0.950361, "train_accs": 1.0, "test_accs": 0.781955, "grad_norms": 0.000176, "param_norms": 45.797871}, {"epoch": 6480, "train_losses": 0.000204, "test_losses": 0.903681, "train_accs": 1.0, "test_accs": 0.781955, "grad_norms": 0.000119, "param_norms": 46.558775}, {"epoch": 6720, "train_losses": 0.000137, "test_losses": 0.8561, "train_accs": 1.0, "test_accs": 0.781955, "grad_norms": 8.2e-05, "param_norms": 47.26838}, {"epoch": 6960, "train_losses": 9.4e-05, "test_losses": 0.809337, "train_accs": 1.0, "test_accs": 0.781955, "grad_norms": 5.7e-05, "param_norms": 47.923043}, {"epoch": 7200, "train_losses": 6.6e-05, "test_losses": 0.762877, "train_accs": 1.0, "test_accs": 0.796992, "grad_norms": 4e-05, "param_norms": 48.517097}, {"epoch": 7440, "train_losses": 4.7e-05, "test_losses": 0.716994, "train_accs": 1.0, "test_accs": 0.827068, "grad_norms": 2.9e-05, "param_norms": 49.044407}, {"epoch": 7680, "train_losses": 3.4e-05, "test_losses": 0.672391, "train_accs": 1.0, "test_accs": 0.842105, "grad_norms": 2.2e-05, "param_norms": 49.498946}, {"epoch": 7920, "train_losses": 2.6e-05, "test_losses": 0.62878, "train_accs": 1.0, "test_accs": 0.887218, "grad_norms": 1.7e-05, "param_norms": 49.876448}, {"epoch": 8160, "train_losses": 2e-05, "test_losses": 0.587391, "train_accs": 1.0, "test_accs": 0.902256, "grad_norms": 1.3e-05, "param_norms": 50.177861}, {"epoch": 8400, "train_losses": 1.7e-05, "test_losses": 0.547753, "train_accs": 1.0, "test_accs": 0.902256, "grad_norms": 1.1e-05, "param_norms": 50.405356}, {"epoch": 8640, "train_losses": 1.4e-05, "test_losses": 0.51086, "train_accs": 1.0, "test_accs": 0.902256, "grad_norms": 9e-06, "param_norms": 50.564097}, {"epoch": 8880, "train_losses": 1.2e-05, "test_losses": 0.477347, "train_accs": 1.0, "test_accs": 0.932331, "grad_norms": 8e-06, "param_norms": 50.662506}, {"epoch": 9120, "train_losses": 1.1e-05, "test_losses": 0.448372, "train_accs": 1.0, "test_accs": 0.932331, "grad_norms": 7e-06, "param_norms": 50.71249}, {"epoch": 9360, "train_losses": 1e-05, "test_losses": 0.422513, "train_accs": 1.0, "test_accs": 0.932331, "grad_norms": 7e-06, "param_norms": 50.72268}, {"epoch": 9600, "train_losses": 9e-06, "test_losses": 0.399118, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 6e-06, "param_norms": 50.703518}, {"epoch": 9840, "train_losses": 9e-06, "test_losses": 0.377931, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 6e-06, "param_norms": 50.660147}, {"epoch": 10080, "train_losses": 8e-06, "test_losses": 0.359181, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 6e-06, "param_norms": 50.599973}, {"epoch": 10320, "train_losses": 8e-06, "test_losses": 0.342359, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 50.529532}, {"epoch": 10560, "train_losses": 8e-06, "test_losses": 0.326894, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 50.451947}, {"epoch": 10800, "train_losses": 8e-06, "test_losses": 0.312899, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 50.370559}, {"epoch": 11040, "train_losses": 8e-06, "test_losses": 0.300169, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 50.287064}, {"epoch": 11280, "train_losses": 8e-06, "test_losses": 0.288715, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 50.203159}, {"epoch": 11520, "train_losses": 7e-06, "test_losses": 0.278343, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 50.121817}, {"epoch": 11760, "train_losses": 7e-06, "test_losses": 0.268877, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 50.041665}, {"epoch": 12000, "train_losses": 7e-06, "test_losses": 0.260429, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.964537}, {"epoch": 12240, "train_losses": 7e-06, "test_losses": 0.253031, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.890351}, {"epoch": 12480, "train_losses": 7e-06, "test_losses": 0.246451, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.819632}, {"epoch": 12720, "train_losses": 7e-06, "test_losses": 0.240401, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.75224}, {"epoch": 12960, "train_losses": 7e-06, "test_losses": 0.234875, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.688046}, {"epoch": 13200, "train_losses": 7e-06, "test_losses": 0.229973, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.62701}, {"epoch": 13440, "train_losses": 7e-06, "test_losses": 0.225422, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.569317}, {"epoch": 13680, "train_losses": 7e-06, "test_losses": 0.221136, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.513881}, {"epoch": 13920, "train_losses": 7e-06, "test_losses": 0.217057, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.461146}, {"epoch": 14160, "train_losses": 7e-06, "test_losses": 0.213229, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.411044}, {"epoch": 14400, "train_losses": 7e-06, "test_losses": 0.209691, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.363345}, {"epoch": 14640, "train_losses": 7e-06, "test_losses": 0.206428, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.318399}, {"epoch": 14880, "train_losses": 7e-06, "test_losses": 0.203423, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.275882}, {"epoch": 15120, "train_losses": 7e-06, "test_losses": 0.200598, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.234982}, {"epoch": 15360, "train_losses": 7e-06, "test_losses": 0.197951, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.196311}, {"epoch": 15600, "train_losses": 7e-06, "test_losses": 0.195402, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.159335}, {"epoch": 15840, "train_losses": 7e-06, "test_losses": 0.192916, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.123401}, {"epoch": 16080, "train_losses": 7e-06, "test_losses": 0.190596, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.088531}, {"epoch": 16320, "train_losses": 7e-06, "test_losses": 0.188481, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.055143}, {"epoch": 16560, "train_losses": 7e-06, "test_losses": 0.186484, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 49.023948}, {"epoch": 16800, "train_losses": 7e-06, "test_losses": 0.184701, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.994215}, {"epoch": 17040, "train_losses": 7e-06, "test_losses": 0.183305, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.966429}, {"epoch": 17280, "train_losses": 7e-06, "test_losses": 0.181962, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.939035}, {"epoch": 17520, "train_losses": 7e-06, "test_losses": 0.180803, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.912822}, {"epoch": 17760, "train_losses": 7e-06, "test_losses": 0.17986, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.887409}, {"epoch": 18000, "train_losses": 7e-06, "test_losses": 0.178967, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.863513}, {"epoch": 18240, "train_losses": 7e-06, "test_losses": 0.178177, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.840795}, {"epoch": 18480, "train_losses": 7e-06, "test_losses": 0.177541, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.818923}, {"epoch": 18720, "train_losses": 7e-06, "test_losses": 0.176824, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.797704}, {"epoch": 18960, "train_losses": 7e-06, "test_losses": 0.176174, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.7773}, {"epoch": 19200, "train_losses": 7e-06, "test_losses": 0.175552, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.757507}, {"epoch": 19440, "train_losses": 7e-06, "test_losses": 0.175008, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.73885}, {"epoch": 19680, "train_losses": 7e-06, "test_losses": 0.174522, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.721279}, {"epoch": 19920, "train_losses": 7e-06, "test_losses": 0.174109, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.704442}, {"epoch": 20160, "train_losses": 7e-06, "test_losses": 0.173762, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.688196}, {"epoch": 20400, "train_losses": 7e-06, "test_losses": 0.173414, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.672445}, {"epoch": 20640, "train_losses": 7e-06, "test_losses": 0.173033, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.657508}, {"epoch": 20880, "train_losses": 7e-06, "test_losses": 0.172714, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.643698}, {"epoch": 21120, "train_losses": 7e-06, "test_losses": 0.172355, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.630356}, {"epoch": 21360, "train_losses": 7e-06, "test_losses": 0.172068, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.617172}, {"epoch": 21600, "train_losses": 7e-06, "test_losses": 0.171768, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.604572}, {"epoch": 21840, "train_losses": 7e-06, "test_losses": 0.171573, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.592474}, {"epoch": 22080, "train_losses": 7e-06, "test_losses": 0.171403, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.58108}, {"epoch": 22320, "train_losses": 7e-06, "test_losses": 0.171239, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.57003}, {"epoch": 22560, "train_losses": 7e-06, "test_losses": 0.170982, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.559644}, {"epoch": 22800, "train_losses": 7e-06, "test_losses": 0.170657, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.550028}, {"epoch": 23040, "train_losses": 7e-06, "test_losses": 0.170401, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.540756}, {"epoch": 23280, "train_losses": 7e-06, "test_losses": 0.170223, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.531966}, {"epoch": 23520, "train_losses": 7e-06, "test_losses": 0.170149, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.523459}, {"epoch": 23760, "train_losses": 7e-06, "test_losses": 0.170044, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.51536}, {"epoch": 24000, "train_losses": 7e-06, "test_losses": 0.169871, "train_accs": 1.0, "test_accs": 0.962406, "grad_norms": 5e-06, "param_norms": 48.507386}], "total_epochs": 24001}, "quad_random": {"config": {"prime": 23, "d_mlp": 512, "act_type": "Quad", "init_type": "random", "init_scale": 0.1, "optimizer": "AdamW", "lr": 5e-05, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=23, run=quad_random\n======================================================================\n\nConfiguration:\n  prime (p)       = 23\n  d_mlp           = 512\n  activation      = Quad\n  init_type       = random\n  init_scale      = 0.1\n  optimizer       = AdamW\n  learning_rate   = 5e-05\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.135497      3.135497      0.0454      0.0454      0.0021       3.2092\n      50      3.134939      3.134939      0.1645      0.1645      0.0022       3.2503\n     100      3.134207      3.134207      0.3459      0.3459      0.0025       3.3973\n     150      3.133266      3.133266      0.5841      0.5841      0.0031       3.6183\n     200      3.132037      3.132037      0.7429      0.7429      0.0040       3.8868\n     250      3.130458      3.130458      0.8507      0.8507      0.0050       4.1848\n     300      3.128469      3.128469      0.9149      0.9149      0.0062       4.5005\n     350      3.126014      3.126014      0.9641      0.9641      0.0076       4.8274\n     400      3.123039      3.123039      0.9830      0.9830      0.0092       5.1621\n     450      3.119489      3.119489      0.9830      0.9830      0.0109       5.5020\n     500      3.115309      3.115309      0.9868      0.9868      0.0128       5.8459\n     550      3.110447      3.110447      0.9868      0.9868      0.0149       6.1931\n     600      3.104849      3.104849      0.9905      0.9905      0.0171       6.5432\n     650      3.098467      3.098467      0.9905      0.9905      0.0195       6.8957\n     700      3.091248      3.091248      0.9905      0.9905      0.0221       7.2504\n     750      3.083141      3.083141      0.9905      0.9905      0.0248       7.6072\n     800      3.074099      3.074099      0.9905      0.9905      0.0277       7.9657\n     850      3.064072      3.064072      0.9905      0.9905      0.0308       8.3256\n     900      3.053009      3.053009      0.9905      0.9905      0.0340       8.6867\n     950      3.040863      3.040863      0.9905      0.9905      0.0373       9.0489\n    1000      3.027585      3.027585      0.9924      0.9924      0.0408       9.4123\n    1050      3.013130      3.013130      0.9924      0.9924      0.0445       9.7765\n    1100      2.997449      2.997449      0.9943      0.9943      0.0482      10.1418\n    1150      2.980499      2.980499      0.9943      0.9943      0.0522      10.5078\n    1200      2.962233      2.962233      0.9943      0.9943      0.0563      10.8744\n    1250      2.942608      2.942608      0.9943      0.9943      0.0605      11.2416\n    1300      2.921581      2.921581      0.9943      0.9943      0.0649      11.6094\n    1350      2.899109      2.899109      0.9943      0.9943      0.0694      11.9777\n    1400      2.875152      2.875152      0.9943      0.9943      0.0740      12.3467\n    1450      2.849672      2.849672      0.9943      0.9943      0.0787      12.7161\n    1500      2.822629      2.822629      0.9943      0.9943      0.0836      13.0860\n    1550      2.793987      2.793987      0.9943      0.9943      0.0886      13.4564\n    1600      2.763711      2.763711      0.9962      0.9962      0.0937      13.8273\n    1650      2.731771      2.731771      0.9962      0.9962      0.0989      14.1987\n    1700      2.698134      2.698134      0.9981      0.9981      0.1042      14.5703\n    1750      2.662773      2.662773      0.9981      0.9981      0.1096      14.9420\n    1800      2.625660      2.625660      0.9981      0.9981      0.1151      15.3140\n    1850      2.586775      2.586775      0.9981      0.9981      0.1207      15.6861\n    1900      2.546099      2.546099      0.9981      0.9981      0.1263      16.0583\n    1950      2.503616      2.503616      0.9981      0.9981      0.1320      16.4306\n    2000      2.459314      2.459314      0.9981      0.9981      0.1378      16.8031\n    2050      2.413188      2.413188      0.9981      0.9981      0.1435      17.1757\n    2100      2.365236      2.365236      0.9981      0.9981      0.1493      17.5484\n    2150      2.315463      2.315463      0.9981      0.9981      0.1551      17.9210\n    2200      2.263879      2.263879      1.0000      1.0000      0.1608      18.2934\n    2250      2.210502      2.210502      1.0000      1.0000      0.1666      18.6656\n    2300      2.155358      2.155358      1.0000      1.0000      0.1722      19.0376\n    2350      2.098480      2.098480      1.0000      1.0000      0.1778      19.4094\n    2400      2.039916      2.039916      1.0000      1.0000      0.1832      19.7809\n    2450      1.979719      1.979719      1.0000      1.0000      0.1885      20.1520\n    2500      1.917958      1.917958      1.0000      1.0000      0.1936      20.5227\n    2550      1.854713      1.854713      1.0000      1.0000      0.1985      20.8930\n    2600      1.790080      1.790080      1.0000      1.0000      0.2031      21.2628\n    2650      1.724170      1.724170      1.0000      1.0000      0.2073      21.6321\n    2700      1.657112      1.657112      1.0000      1.0000      0.2112      22.0007\n    2750      1.589050      1.589050      1.0000      1.0000      0.2147      22.3686\n    2800      1.520149      1.520149      1.0000      1.0000      0.2177      22.7359\n    2850      1.450594      1.450594      1.0000      1.0000      0.2202      23.1021\n    2900      1.380585      1.380585      1.0000      1.0000      0.2221      23.4671\n    2950      1.310344      1.310344      1.0000      1.0000      0.2233      23.8307\n    3000      1.240111      1.240111      1.0000      1.0000      0.2238      24.1930\n    3050      1.170140      1.170140      1.0000      1.0000      0.2236      24.5537\n    3100      1.100701      1.100701      1.0000      1.0000      0.2226      24.9125\n    3150      1.032074      1.032074      1.0000      1.0000      0.2207      25.2693\n    3200      0.964543      0.964543      1.0000      1.0000      0.2180      25.6239\n    3250      0.898394      0.898394      1.0000      1.0000      0.2144      25.9761\n    3300      0.833906      0.833906      1.0000      1.0000      0.2100      26.3256\n    3350      0.771348      0.771348      1.0000      1.0000      0.2048      26.6719\n    3400      0.710964      0.710964      1.0000      1.0000      0.1988      27.0149\n    3450      0.652978      0.652978      1.0000      1.0000      0.1920      27.3541\n    3500      0.597577      0.597577      1.0000      1.0000      0.1847      27.6889\n    3550      0.544912      0.544912      1.0000      1.0000      0.1768      28.0192\n    3600      0.495093      0.495093      1.0000      1.0000      0.1685      28.3447\n    3650      0.448192      0.448192      1.0000      1.0000      0.1598      28.6652\n    3700      0.404242      0.404242      1.0000      1.0000      0.1509      28.9804\n    3750      0.363241      0.363241      1.0000      1.0000      0.1418      29.2903\n    3800      0.325157      0.325157      1.0000      1.0000      0.1327      29.5950\n    3850      0.289938      0.289938      1.0000      1.0000      0.1236      29.8949\n    3900      0.257519      0.257519      1.0000      1.0000      0.1145      30.1908\n    3950      0.227820      0.227820      1.0000      1.0000      0.1055      30.4833\n    4000      0.200754      0.200754      1.0000      1.0000      0.0967      30.7734\n    4050      0.176218      0.176218      1.0000      1.0000      0.0880      31.0620\n    4100      0.154098      0.154098      1.0000      1.0000      0.0797      31.3502\n    4150      0.134266      0.134266      1.0000      1.0000      0.0718      31.6389\n    4200      0.116578      0.116578      1.0000      1.0000      0.0642      31.9288\n    4250      0.100882      0.100882      1.0000      1.0000      0.0572      32.2199\n    4300      0.087019      0.087019      1.0000      1.0000      0.0507      32.5121\n    4350      0.074828      0.074828      1.0000      1.0000      0.0448      32.8051\n    4400      0.064154      0.064154      1.0000      1.0000      0.0393      33.0989\n    4450      0.054845      0.054845      1.0000      1.0000      0.0344      33.3930\n    4500      0.046756      0.046756      1.0000      1.0000      0.0300      33.6873\n    4550      0.039752      0.039752      1.0000      1.0000      0.0261      33.9818\n    4600      0.033709      0.033709      1.0000      1.0000      0.0226      34.2761\n    4650      0.028512      0.028512      1.0000      1.0000      0.0195      34.5704\n    4700      0.024056      0.024056      1.0000      1.0000      0.0167      34.8644\n    4750      0.020247      0.020247      1.0000      1.0000      0.0143      35.1581\n    4800      0.017001      0.017001      1.0000      1.0000      0.0122      35.4513\n    4850      0.014242      0.014242      1.0000      1.0000      0.0104      35.7442\n    4900      0.011904      0.011904      1.0000      1.0000      0.0089      36.0365\n    4950      0.009927      0.009927      1.0000      1.0000      0.0075      36.3283\n    4999      0.008291      0.008291      1.0000      1.0000      0.0064      36.6137\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 0.008291\n  Test Loss   = 0.008291\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 36.6137\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.135497, "test_losses": 3.135497, "train_accs": 0.045369, "test_accs": 0.045369, "grad_norms": 0.002064, "param_norms": 3.209196}, {"epoch": 50, "train_losses": 3.134939, "test_losses": 3.134939, "train_accs": 0.164461, "test_accs": 0.164461, "grad_norms": 0.002166, "param_norms": 3.2503}, {"epoch": 100, "train_losses": 3.134207, "test_losses": 3.134207, "train_accs": 0.345936, "test_accs": 0.345936, "grad_norms": 0.002538, "param_norms": 3.397335}, {"epoch": 150, "train_losses": 3.133266, "test_losses": 3.133266, "train_accs": 0.584121, "test_accs": 0.584121, "grad_norms": 0.003145, "param_norms": 3.618321}, {"epoch": 200, "train_losses": 3.132037, "test_losses": 3.132037, "train_accs": 0.742911, "test_accs": 0.742911, "grad_norms": 0.003966, "param_norms": 3.886802}, {"epoch": 250, "train_losses": 3.130458, "test_losses": 3.130458, "train_accs": 0.850662, "test_accs": 0.850662, "grad_norms": 0.004986, "param_norms": 4.184755}, {"epoch": 300, "train_losses": 3.128469, "test_losses": 3.128469, "train_accs": 0.914934, "test_accs": 0.914934, "grad_norms": 0.006195, "param_norms": 4.500452}, {"epoch": 350, "train_losses": 3.126014, "test_losses": 3.126014, "train_accs": 0.964083, "test_accs": 0.964083, "grad_norms": 0.007584, "param_norms": 4.827423}, {"epoch": 400, "train_losses": 3.123039, "test_losses": 3.123039, "train_accs": 0.982987, "test_accs": 0.982987, "grad_norms": 0.009151, "param_norms": 5.162057}, {"epoch": 450, "train_losses": 3.119489, "test_losses": 3.119489, "train_accs": 0.982987, "test_accs": 0.982987, "grad_norms": 0.010891, "param_norms": 5.502047}, {"epoch": 500, "train_losses": 3.115309, "test_losses": 3.115309, "train_accs": 0.986767, "test_accs": 0.986767, "grad_norms": 0.012803, "param_norms": 5.845938}, {"epoch": 550, "train_losses": 3.110447, "test_losses": 3.110447, "train_accs": 0.986767, "test_accs": 0.986767, "grad_norms": 0.014883, "param_norms": 6.193145}, {"epoch": 600, "train_losses": 3.104849, "test_losses": 3.104849, "train_accs": 0.990548, "test_accs": 0.990548, "grad_norms": 0.017128, "param_norms": 6.543198}, {"epoch": 650, "train_losses": 3.098467, "test_losses": 3.098467, "train_accs": 0.990548, "test_accs": 0.990548, "grad_norms": 0.019535, "param_norms": 6.895717}, {"epoch": 700, "train_losses": 3.091248, "test_losses": 3.091248, "train_accs": 0.990548, "test_accs": 0.990548, "grad_norms": 0.022104, "param_norms": 7.250431}, {"epoch": 750, "train_losses": 3.083141, "test_losses": 3.083141, "train_accs": 0.990548, "test_accs": 0.990548, "grad_norms": 0.024832, "param_norms": 7.607174}, {"epoch": 800, "train_losses": 3.074099, "test_losses": 3.074099, "train_accs": 0.990548, "test_accs": 0.990548, "grad_norms": 0.027718, "param_norms": 7.965668}, {"epoch": 850, "train_losses": 3.064072, "test_losses": 3.064072, "train_accs": 0.990548, "test_accs": 0.990548, "grad_norms": 0.03076, "param_norms": 8.325594}, {"epoch": 900, "train_losses": 3.053009, "test_losses": 3.053009, "train_accs": 0.990548, "test_accs": 0.990548, "grad_norms": 0.033956, "param_norms": 8.686682}, {"epoch": 950, "train_losses": 3.040863, "test_losses": 3.040863, "train_accs": 0.990548, "test_accs": 0.990548, "grad_norms": 0.037305, "param_norms": 9.048921}, {"epoch": 1000, "train_losses": 3.027585, "test_losses": 3.027585, "train_accs": 0.992439, "test_accs": 0.992439, "grad_norms": 0.040805, "param_norms": 9.412255}, {"epoch": 1050, "train_losses": 3.01313, "test_losses": 3.01313, "train_accs": 0.992439, "test_accs": 0.992439, "grad_norms": 0.044454, "param_norms": 9.776544}, {"epoch": 1100, "train_losses": 2.997449, "test_losses": 2.997449, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.04825, "param_norms": 10.141751}, {"epoch": 1150, "train_losses": 2.980499, "test_losses": 2.980499, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.05219, "param_norms": 10.507788}, {"epoch": 1200, "train_losses": 2.962233, "test_losses": 2.962233, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.056273, "param_norms": 10.874437}, {"epoch": 1250, "train_losses": 2.942608, "test_losses": 2.942608, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.060497, "param_norms": 11.24161}, {"epoch": 1300, "train_losses": 2.921581, "test_losses": 2.921581, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.064857, "param_norms": 11.609382}, {"epoch": 1350, "train_losses": 2.899109, "test_losses": 2.899109, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.069351, "param_norms": 11.977746}, {"epoch": 1400, "train_losses": 2.875152, "test_losses": 2.875152, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.073975, "param_norms": 12.346698}, {"epoch": 1450, "train_losses": 2.849672, "test_losses": 2.849672, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.078726, "param_norms": 12.71614}, {"epoch": 1500, "train_losses": 2.822629, "test_losses": 2.822629, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.083599, "param_norms": 13.086018}, {"epoch": 1550, "train_losses": 2.793987, "test_losses": 2.793987, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.08859, "param_norms": 13.456398}, {"epoch": 1600, "train_losses": 2.763711, "test_losses": 2.763711, "train_accs": 0.996219, "test_accs": 0.996219, "grad_norms": 0.093694, "param_norms": 13.827311}, {"epoch": 1650, "train_losses": 2.731771, "test_losses": 2.731771, "train_accs": 0.996219, "test_accs": 0.996219, "grad_norms": 0.098905, "param_norms": 14.198698}, {"epoch": 1700, "train_losses": 2.698134, "test_losses": 2.698134, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.104217, "param_norms": 14.570294}, {"epoch": 1750, "train_losses": 2.662773, "test_losses": 2.662773, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.109623, "param_norms": 14.942018}, {"epoch": 1800, "train_losses": 2.62566, "test_losses": 2.62566, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.115115, "param_norms": 15.313954}, {"epoch": 1850, "train_losses": 2.586775, "test_losses": 2.586775, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.120684, "param_norms": 15.686056}, {"epoch": 1900, "train_losses": 2.546099, "test_losses": 2.546099, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.126321, "param_norms": 16.058303}, {"epoch": 1950, "train_losses": 2.503616, "test_losses": 2.503616, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.132015, "param_norms": 16.430622}, {"epoch": 2000, "train_losses": 2.459314, "test_losses": 2.459314, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.137753, "param_norms": 16.80308}, {"epoch": 2050, "train_losses": 2.413188, "test_losses": 2.413188, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.143521, "param_norms": 17.175684}, {"epoch": 2100, "train_losses": 2.365236, "test_losses": 2.365236, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.149305, "param_norms": 17.548385}, {"epoch": 2150, "train_losses": 2.315463, "test_losses": 2.315463, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.155087, "param_norms": 17.921001}, {"epoch": 2200, "train_losses": 2.263879, "test_losses": 2.263879, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.160848, "param_norms": 18.293438}, {"epoch": 2250, "train_losses": 2.210502, "test_losses": 2.210502, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.166568, "param_norms": 18.665635}, {"epoch": 2300, "train_losses": 2.155358, "test_losses": 2.155358, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.172221, "param_norms": 19.03761}, {"epoch": 2350, "train_losses": 2.09848, "test_losses": 2.09848, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.177782, "param_norms": 19.409383}, {"epoch": 2400, "train_losses": 2.039916, "test_losses": 2.039916, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.183221, "param_norms": 19.780875}, {"epoch": 2450, "train_losses": 1.979719, "test_losses": 1.979719, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.188507, "param_norms": 20.152025}, {"epoch": 2500, "train_losses": 1.917958, "test_losses": 1.917958, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.193602, "param_norms": 20.522738}, {"epoch": 2550, "train_losses": 1.854713, "test_losses": 1.854713, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.198468, "param_norms": 20.893015}, {"epoch": 2600, "train_losses": 1.79008, "test_losses": 1.79008, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.203061, "param_norms": 21.262809}, {"epoch": 2650, "train_losses": 1.72417, "test_losses": 1.72417, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.207336, "param_norms": 21.63205}, {"epoch": 2700, "train_losses": 1.657112, "test_losses": 1.657112, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.211242, "param_norms": 22.00066}, {"epoch": 2750, "train_losses": 1.58905, "test_losses": 1.58905, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.214726, "param_norms": 22.368644}, {"epoch": 2800, "train_losses": 1.520149, "test_losses": 1.520149, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.217733, "param_norms": 22.735871}, {"epoch": 2850, "train_losses": 1.450594, "test_losses": 1.450594, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.220205, "param_norms": 23.102076}, {"epoch": 2900, "train_losses": 1.380585, "test_losses": 1.380585, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.222084, "param_norms": 23.467057}, {"epoch": 2950, "train_losses": 1.310344, "test_losses": 1.310344, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.223313, "param_norms": 23.830739}, {"epoch": 3000, "train_losses": 1.240111, "test_losses": 1.240111, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.223838, "param_norms": 24.193004}, {"epoch": 3050, "train_losses": 1.17014, "test_losses": 1.17014, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.223608, "param_norms": 24.553654}, {"epoch": 3100, "train_losses": 1.100701, "test_losses": 1.100701, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.222581, "param_norms": 24.912474}, {"epoch": 3150, "train_losses": 1.032074, "test_losses": 1.032074, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.220722, "param_norms": 25.269304}, {"epoch": 3200, "train_losses": 0.964543, "test_losses": 0.964543, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.218011, "param_norms": 25.623904}, {"epoch": 3250, "train_losses": 0.898394, "test_losses": 0.898394, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.21444, "param_norms": 25.976077}, {"epoch": 3300, "train_losses": 0.833906, "test_losses": 0.833906, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.210021, "param_norms": 26.325561}, {"epoch": 3350, "train_losses": 0.771348, "test_losses": 0.771348, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.20478, "param_norms": 26.671937}, {"epoch": 3400, "train_losses": 0.710964, "test_losses": 0.710964, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.198765, "param_norms": 27.014869}, {"epoch": 3450, "train_losses": 0.652978, "test_losses": 0.652978, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.19204, "param_norms": 27.354052}, {"epoch": 3500, "train_losses": 0.597577, "test_losses": 0.597577, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.184685, "param_norms": 27.688947}, {"epoch": 3550, "train_losses": 0.544912, "test_losses": 0.544912, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.176794, "param_norms": 28.019226}, {"epoch": 3600, "train_losses": 0.495093, "test_losses": 0.495093, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.168466, "param_norms": 28.344722}, {"epoch": 3650, "train_losses": 0.448192, "test_losses": 0.448192, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.159801, "param_norms": 28.665199}, {"epoch": 3700, "train_losses": 0.404242, "test_losses": 0.404242, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.150898, "param_norms": 28.980401}, {"epoch": 3750, "train_losses": 0.363241, "test_losses": 0.363241, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.141846, "param_norms": 29.290319}, {"epoch": 3800, "train_losses": 0.325157, "test_losses": 0.325157, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.13272, "param_norms": 29.595034}, {"epoch": 3850, "train_losses": 0.289938, "test_losses": 0.289938, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.123584, "param_norms": 29.894933}, {"epoch": 3900, "train_losses": 0.257519, "test_losses": 0.257519, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.114493, "param_norms": 30.19077}, {"epoch": 3950, "train_losses": 0.22782, "test_losses": 0.22782, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.105499, "param_norms": 30.483319}, {"epoch": 4000, "train_losses": 0.200754, "test_losses": 0.200754, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.096659, "param_norms": 30.77341}, {"epoch": 4050, "train_losses": 0.176218, "test_losses": 0.176218, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.088038, "param_norms": 31.062013}, {"epoch": 4100, "train_losses": 0.154098, "test_losses": 0.154098, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.079712, "param_norms": 31.350198}, {"epoch": 4150, "train_losses": 0.134266, "test_losses": 0.134266, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.071759, "param_norms": 31.638934}, {"epoch": 4200, "train_losses": 0.116578, "test_losses": 0.116578, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.064248, "param_norms": 31.928773}, {"epoch": 4250, "train_losses": 0.100882, "test_losses": 0.100882, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.057229, "param_norms": 32.219885}, {"epoch": 4300, "train_losses": 0.087019, "test_losses": 0.087019, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.050732, "param_norms": 32.512081}, {"epoch": 4350, "train_losses": 0.074828, "test_losses": 0.074828, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.044767, "param_norms": 32.805146}, {"epoch": 4400, "train_losses": 0.064154, "test_losses": 0.064154, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.039334, "param_norms": 33.098862}, {"epoch": 4450, "train_losses": 0.054845, "test_losses": 0.054845, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.03442, "param_norms": 33.392995}, {"epoch": 4500, "train_losses": 0.046756, "test_losses": 0.046756, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.030004, "param_norms": 33.687347}, {"epoch": 4550, "train_losses": 0.039752, "test_losses": 0.039752, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.026058, "param_norms": 33.981756}, {"epoch": 4600, "train_losses": 0.033709, "test_losses": 0.033709, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.022551, "param_norms": 34.276148}, {"epoch": 4650, "train_losses": 0.028512, "test_losses": 0.028512, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.019451, "param_norms": 34.570387}, {"epoch": 4700, "train_losses": 0.024056, "test_losses": 0.024056, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016723, "param_norms": 34.864401}, {"epoch": 4750, "train_losses": 0.020247, "test_losses": 0.020247, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014333, "param_norms": 35.15807}, {"epoch": 4800, "train_losses": 0.017001, "test_losses": 0.017001, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012248, "param_norms": 35.451341}, {"epoch": 4850, "train_losses": 0.014242, "test_losses": 0.014242, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010437, "param_norms": 35.744169}, {"epoch": 4900, "train_losses": 0.011904, "test_losses": 0.011904, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008869, "param_norms": 36.036479}, {"epoch": 4950, "train_losses": 0.009927, "test_losses": 0.009927, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007516, "param_norms": 36.328265}, {"epoch": 4999, "train_losses": 0.008291, "test_losses": 0.008291, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006374, "param_norms": 36.613659}], "total_epochs": 5000}, "quad_single_freq": {"config": {"prime": 23, "d_mlp": 512, "act_type": "Quad", "init_type": "single-freq", "init_scale": 0.02, "optimizer": "SGD", "lr": 0.1, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=23, run=quad_single_freq\n======================================================================\n\nConfiguration:\n  prime (p)       = 23\n  d_mlp           = 512\n  activation      = Quad\n  init_type       = single-freq\n  init_scale      = 0.02\n  optimizer       = SGD\n  learning_rate   = 0.1\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.135525      3.135525      0.0227      0.0227      0.0030       2.1703\n      50      3.135485      3.135485      0.0397      0.0397      0.0030       2.1703\n     100      3.135441      3.135441      0.0529      0.0529      0.0030       2.1705\n     150      3.135396      3.135396      0.0907      0.0907      0.0030       2.1710\n     200      3.135351      3.135351      0.1323      0.1323      0.0030       2.1718\n     250      3.135307      3.135307      0.2079      0.2079      0.0030       2.1730\n     300      3.135262      3.135262      0.2741      0.2741      0.0030       2.1744\n     350      3.135217      3.135217      0.3686      0.3686      0.0030       2.1762\n     400      3.135171      3.135171      0.4329      0.4329      0.0030       2.1783\n     450      3.135126      3.135126      0.5217      0.5217      0.0030       2.1806\n     500      3.135080      3.135080      0.5822      0.5822      0.0030       2.1833\n     550      3.135033      3.135033      0.6635      0.6635      0.0030       2.1863\n     600      3.134987      3.134987      0.7183      0.7183      0.0031       2.1896\n     650      3.134940      3.134940      0.7977      0.7977      0.0031       2.1933\n     700      3.134892      3.134892      0.8582      0.8582      0.0031       2.1972\n     750      3.134844      3.134844      0.8922      0.8922      0.0031       2.2015\n     800      3.134796      3.134796      0.9433      0.9433      0.0031       2.2061\n     850      3.134747      3.134747      0.9735      0.9735      0.0031       2.2110\n     900      3.134697      3.134697      0.9887      0.9887      0.0032       2.2162\n     950      3.134647      3.134647      0.9924      0.9924      0.0032       2.2218\n    1000      3.134596      3.134596      0.9962      0.9962      0.0032       2.2276\n    1050      3.134544      3.134544      1.0000      1.0000      0.0032       2.2339\n    1100      3.134491      3.134491      1.0000      1.0000      0.0033       2.2404\n    1150      3.134438      3.134438      1.0000      1.0000      0.0033       2.2473\n    1200      3.134382      3.134382      1.0000      1.0000      0.0033       2.2545\n    1250      3.134327      3.134327      1.0000      1.0000      0.0033       2.2621\n    1300      3.134271      3.134271      1.0000      1.0000      0.0034       2.2700\n    1350      3.134213      3.134213      1.0000      1.0000      0.0034       2.2782\n    1400      3.134155      3.134155      1.0000      1.0000      0.0034       2.2868\n    1450      3.134095      3.134095      1.0000      1.0000      0.0035       2.2958\n    1500      3.134033      3.134033      1.0000      1.0000      0.0035       2.3051\n    1550      3.133971      3.133971      1.0000      1.0000      0.0036       2.3148\n    1600      3.133907      3.133907      1.0000      1.0000      0.0036       2.3249\n    1650      3.133842      3.133842      1.0000      1.0000      0.0036       2.3353\n    1700      3.133775      3.133775      1.0000      1.0000      0.0037       2.3461\n    1750      3.133706      3.133706      1.0000      1.0000      0.0037       2.3573\n    1800      3.133635      3.133635      1.0000      1.0000      0.0038       2.3688\n    1850      3.133564      3.133564      1.0000      1.0000      0.0038       2.3808\n    1900      3.133489      3.133489      1.0000      1.0000      0.0039       2.3932\n    1950      3.133413      3.133413      1.0000      1.0000      0.0039       2.4059\n    2000      3.133335      3.133335      1.0000      1.0000      0.0040       2.4191\n    2050      3.133254      3.133254      1.0000      1.0000      0.0040       2.4327\n    2100      3.133171      3.133171      1.0000      1.0000      0.0041       2.4467\n    2150      3.133086      3.133086      1.0000      1.0000      0.0042       2.4612\n    2200      3.132998      3.132998      1.0000      1.0000      0.0042       2.4761\n    2250      3.132907      3.132907      1.0000      1.0000      0.0043       2.4914\n    2300      3.132814      3.132814      1.0000      1.0000      0.0044       2.5072\n    2350      3.132717      3.132717      1.0000      1.0000      0.0044       2.5235\n    2400      3.132617      3.132617      1.0000      1.0000      0.0045       2.5402\n    2450      3.132514      3.132514      1.0000      1.0000      0.0046       2.5575\n    2500      3.132407      3.132407      1.0000      1.0000      0.0047       2.5752\n    2550      3.132297      3.132297      1.0000      1.0000      0.0047       2.5934\n    2600      3.132183      3.132183      1.0000      1.0000      0.0048       2.6121\n    2650      3.132064      3.132064      1.0000      1.0000      0.0049       2.6314\n    2700      3.131942      3.131942      1.0000      1.0000      0.0050       2.6512\n    2750      3.131815      3.131815      1.0000      1.0000      0.0051       2.6716\n    2800      3.131682      3.131682      1.0000      1.0000      0.0052       2.6925\n    2850      3.131545      3.131545      1.0000      1.0000      0.0053       2.7141\n    2900      3.131402      3.131402      1.0000      1.0000      0.0054       2.7362\n    2950      3.131254      3.131254      1.0000      1.0000      0.0055       2.7589\n    3000      3.131100      3.131100      1.0000      1.0000      0.0056       2.7823\n    3050      3.130939      3.130939      1.0000      1.0000      0.0057       2.8063\n    3100      3.130772      3.130772      1.0000      1.0000      0.0058       2.8310\n    3150      3.130597      3.130597      1.0000      1.0000      0.0060       2.8563\n    3200      3.130415      3.130415      1.0000      1.0000      0.0061       2.8824\n    3250      3.130225      3.130225      1.0000      1.0000      0.0062       2.9092\n    3300      3.130027      3.130027      1.0000      1.0000      0.0064       2.9367\n    3350      3.129819      3.129819      1.0000      1.0000      0.0065       2.9650\n    3400      3.129602      3.129602      1.0000      1.0000      0.0067       2.9941\n    3450      3.129375      3.129375      1.0000      1.0000      0.0068       3.0240\n    3500      3.129137      3.129137      1.0000      1.0000      0.0070       3.0548\n    3550      3.128887      3.128887      1.0000      1.0000      0.0072       3.0864\n    3600      3.128625      3.128625      1.0000      1.0000      0.0073       3.1190\n    3650      3.128350      3.128350      1.0000      1.0000      0.0075       3.1525\n    3700      3.128061      3.128061      1.0000      1.0000      0.0077       3.1869\n    3750      3.127757      3.127757      1.0000      1.0000      0.0079       3.2224\n    3800      3.127438      3.127438      1.0000      1.0000      0.0081       3.2589\n    3850      3.127101      3.127101      1.0000      1.0000      0.0083       3.2966\n    3900      3.126746      3.126746      1.0000      1.0000      0.0085       3.3353\n    3950      3.126372      3.126372      1.0000      1.0000      0.0088       3.3752\n    4000      3.125977      3.125977      1.0000      1.0000      0.0090       3.4163\n    4050      3.125559      3.125559      1.0000      1.0000      0.0093       3.4587\n    4100      3.125117      3.125117      1.0000      1.0000      0.0095       3.5025\n    4150      3.124649      3.124649      1.0000      1.0000      0.0098       3.5476\n    4200      3.124152      3.124152      1.0000      1.0000      0.0101       3.5941\n    4250      3.123626      3.123626      1.0000      1.0000      0.0104       3.6422\n    4300      3.123067      3.123067      1.0000      1.0000      0.0107       3.6918\n    4350      3.122473      3.122473      1.0000      1.0000      0.0111       3.7431\n    4400      3.121840      3.121840      1.0000      1.0000      0.0114       3.7961\n    4450      3.121166      3.121166      1.0000      1.0000      0.0118       3.8510\n    4500      3.120447      3.120447      1.0000      1.0000      0.0122       3.9077\n    4550      3.119678      3.119678      1.0000      1.0000      0.0126       3.9664\n    4600      3.118857      3.118857      1.0000      1.0000      0.0130       4.0272\n    4650      3.117977      3.117977      1.0000      1.0000      0.0135       4.0903\n    4700      3.117033      3.117033      1.0000      1.0000      0.0140       4.1556\n    4750      3.116020      3.116020      1.0000      1.0000      0.0145       4.2234\n    4800      3.114931      3.114931      1.0000      1.0000      0.0150       4.2938\n    4850      3.113758      3.113758      1.0000      1.0000      0.0156       4.3670\n    4900      3.112492      3.112492      1.0000      1.0000      0.0162       4.4430\n    4950      3.111125      3.111125      1.0000      1.0000      0.0169       4.5221\n    4999      3.109675      3.109675      1.0000      1.0000      0.0175       4.6028\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 3.109675\n  Test Loss   = 3.109675\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 4.6028\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.135525, "test_losses": 3.135525, "train_accs": 0.022684, "test_accs": 0.022684, "grad_norms": 0.002984, "param_norms": 2.170346}, {"epoch": 50, "train_losses": 3.135485, "test_losses": 3.135485, "train_accs": 0.039698, "test_accs": 0.039698, "grad_norms": 0.002984, "param_norms": 2.170277}, {"epoch": 100, "train_losses": 3.135441, "test_losses": 3.135441, "train_accs": 0.05293, "test_accs": 0.05293, "grad_norms": 0.002985, "param_norms": 2.170492}, {"epoch": 150, "train_losses": 3.135396, "test_losses": 3.135396, "train_accs": 0.090737, "test_accs": 0.090737, "grad_norms": 0.002987, "param_norms": 2.171013}, {"epoch": 200, "train_losses": 3.135351, "test_losses": 3.135351, "train_accs": 0.132325, "test_accs": 0.132325, "grad_norms": 0.00299, "param_norms": 2.171842}, {"epoch": 250, "train_losses": 3.135307, "test_losses": 3.135307, "train_accs": 0.20794, "test_accs": 0.20794, "grad_norms": 0.002995, "param_norms": 2.172981}, {"epoch": 300, "train_losses": 3.135262, "test_losses": 3.135262, "train_accs": 0.274102, "test_accs": 0.274102, "grad_norms": 0.003001, "param_norms": 2.174427}, {"epoch": 350, "train_losses": 3.135217, "test_losses": 3.135217, "train_accs": 0.36862, "test_accs": 0.36862, "grad_norms": 0.003008, "param_norms": 2.176184}, {"epoch": 400, "train_losses": 3.135171, "test_losses": 3.135171, "train_accs": 0.432892, "test_accs": 0.432892, "grad_norms": 0.003016, "param_norms": 2.178251}, {"epoch": 450, "train_losses": 3.135126, "test_losses": 3.135126, "train_accs": 0.521739, "test_accs": 0.521739, "grad_norms": 0.003025, "param_norms": 2.180628}, {"epoch": 500, "train_losses": 3.13508, "test_losses": 3.13508, "train_accs": 0.582231, "test_accs": 0.582231, "grad_norms": 0.003036, "param_norms": 2.183317}, {"epoch": 550, "train_losses": 3.135033, "test_losses": 3.135033, "train_accs": 0.663516, "test_accs": 0.663516, "grad_norms": 0.003047, "param_norms": 2.186318}, {"epoch": 600, "train_losses": 3.134987, "test_losses": 3.134987, "train_accs": 0.718336, "test_accs": 0.718336, "grad_norms": 0.00306, "param_norms": 2.189635}, {"epoch": 650, "train_losses": 3.13494, "test_losses": 3.13494, "train_accs": 0.797732, "test_accs": 0.797732, "grad_norms": 0.003074, "param_norms": 2.193265}, {"epoch": 700, "train_losses": 3.134892, "test_losses": 3.134892, "train_accs": 0.858223, "test_accs": 0.858223, "grad_norms": 0.00309, "param_norms": 2.197213}, {"epoch": 750, "train_losses": 3.134844, "test_losses": 3.134844, "train_accs": 0.89225, "test_accs": 0.89225, "grad_norms": 0.003107, "param_norms": 2.201479}, {"epoch": 800, "train_losses": 3.134796, "test_losses": 3.134796, "train_accs": 0.943289, "test_accs": 0.943289, "grad_norms": 0.003125, "param_norms": 2.206065}, {"epoch": 850, "train_losses": 3.134747, "test_losses": 3.134747, "train_accs": 0.973535, "test_accs": 0.973535, "grad_norms": 0.003144, "param_norms": 2.210972}, {"epoch": 900, "train_losses": 3.134697, "test_losses": 3.134697, "train_accs": 0.988658, "test_accs": 0.988658, "grad_norms": 0.003164, "param_norms": 2.216203}, {"epoch": 950, "train_losses": 3.134647, "test_losses": 3.134647, "train_accs": 0.992439, "test_accs": 0.992439, "grad_norms": 0.003186, "param_norms": 2.22176}, {"epoch": 1000, "train_losses": 3.134596, "test_losses": 3.134596, "train_accs": 0.996219, "test_accs": 0.996219, "grad_norms": 0.003209, "param_norms": 2.227644}, {"epoch": 1050, "train_losses": 3.134544, "test_losses": 3.134544, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003234, "param_norms": 2.233859}, {"epoch": 1100, "train_losses": 3.134491, "test_losses": 3.134491, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003259, "param_norms": 2.240406}, {"epoch": 1150, "train_losses": 3.134438, "test_losses": 3.134438, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003287, "param_norms": 2.247289}, {"epoch": 1200, "train_losses": 3.134382, "test_losses": 3.134382, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003315, "param_norms": 2.25451}, {"epoch": 1250, "train_losses": 3.134327, "test_losses": 3.134327, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003345, "param_norms": 2.262073}, {"epoch": 1300, "train_losses": 3.134271, "test_losses": 3.134271, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003377, "param_norms": 2.26998}, {"epoch": 1350, "train_losses": 3.134213, "test_losses": 3.134213, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00341, "param_norms": 2.278235}, {"epoch": 1400, "train_losses": 3.134155, "test_losses": 3.134155, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003444, "param_norms": 2.286841}, {"epoch": 1450, "train_losses": 3.134095, "test_losses": 3.134095, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00348, "param_norms": 2.295802}, {"epoch": 1500, "train_losses": 3.134033, "test_losses": 3.134033, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003517, "param_norms": 2.305123}, {"epoch": 1550, "train_losses": 3.133971, "test_losses": 3.133971, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003556, "param_norms": 2.314807}, {"epoch": 1600, "train_losses": 3.133907, "test_losses": 3.133907, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003597, "param_norms": 2.324858}, {"epoch": 1650, "train_losses": 3.133842, "test_losses": 3.133842, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003639, "param_norms": 2.335281}, {"epoch": 1700, "train_losses": 3.133775, "test_losses": 3.133775, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003684, "param_norms": 2.346082}, {"epoch": 1750, "train_losses": 3.133706, "test_losses": 3.133706, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003729, "param_norms": 2.357266}, {"epoch": 1800, "train_losses": 3.133635, "test_losses": 3.133635, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003777, "param_norms": 2.368835}, {"epoch": 1850, "train_losses": 3.133564, "test_losses": 3.133564, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003826, "param_norms": 2.380799}, {"epoch": 1900, "train_losses": 3.133489, "test_losses": 3.133489, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003877, "param_norms": 2.393161}, {"epoch": 1950, "train_losses": 3.133413, "test_losses": 3.133413, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00393, "param_norms": 2.405928}, {"epoch": 2000, "train_losses": 3.133335, "test_losses": 3.133335, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003985, "param_norms": 2.419106}, {"epoch": 2050, "train_losses": 3.133254, "test_losses": 3.133254, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004043, "param_norms": 2.432703}, {"epoch": 2100, "train_losses": 3.133171, "test_losses": 3.133171, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004102, "param_norms": 2.446725}, {"epoch": 2150, "train_losses": 3.133086, "test_losses": 3.133086, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004163, "param_norms": 2.461179}, {"epoch": 2200, "train_losses": 3.132998, "test_losses": 3.132998, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004226, "param_norms": 2.476074}, {"epoch": 2250, "train_losses": 3.132907, "test_losses": 3.132907, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004292, "param_norms": 2.491417}, {"epoch": 2300, "train_losses": 3.132814, "test_losses": 3.132814, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00436, "param_norms": 2.507217}, {"epoch": 2350, "train_losses": 3.132717, "test_losses": 3.132717, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004431, "param_norms": 2.523483}, {"epoch": 2400, "train_losses": 3.132617, "test_losses": 3.132617, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004504, "param_norms": 2.540225}, {"epoch": 2450, "train_losses": 3.132514, "test_losses": 3.132514, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00458, "param_norms": 2.557452}, {"epoch": 2500, "train_losses": 3.132407, "test_losses": 3.132407, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004658, "param_norms": 2.575174}, {"epoch": 2550, "train_losses": 3.132297, "test_losses": 3.132297, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004739, "param_norms": 2.593402}, {"epoch": 2600, "train_losses": 3.132183, "test_losses": 3.132183, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004823, "param_norms": 2.612147}, {"epoch": 2650, "train_losses": 3.132064, "test_losses": 3.132064, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00491, "param_norms": 2.631421}, {"epoch": 2700, "train_losses": 3.131942, "test_losses": 3.131942, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005, "param_norms": 2.651237}, {"epoch": 2750, "train_losses": 3.131815, "test_losses": 3.131815, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005093, "param_norms": 2.671607}, {"epoch": 2800, "train_losses": 3.131682, "test_losses": 3.131682, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00519, "param_norms": 2.692546}, {"epoch": 2850, "train_losses": 3.131545, "test_losses": 3.131545, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00529, "param_norms": 2.714067}, {"epoch": 2900, "train_losses": 3.131402, "test_losses": 3.131402, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005393, "param_norms": 2.736185}, {"epoch": 2950, "train_losses": 3.131254, "test_losses": 3.131254, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005501, "param_norms": 2.758916}, {"epoch": 3000, "train_losses": 3.1311, "test_losses": 3.1311, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005612, "param_norms": 2.782277}, {"epoch": 3050, "train_losses": 3.130939, "test_losses": 3.130939, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005727, "param_norms": 2.806284}, {"epoch": 3100, "train_losses": 3.130772, "test_losses": 3.130772, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005847, "param_norms": 2.830956}, {"epoch": 3150, "train_losses": 3.130597, "test_losses": 3.130597, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.005971, "param_norms": 2.856313}, {"epoch": 3200, "train_losses": 3.130415, "test_losses": 3.130415, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0061, "param_norms": 2.882375}, {"epoch": 3250, "train_losses": 3.130225, "test_losses": 3.130225, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006233, "param_norms": 2.90916}, {"epoch": 3300, "train_losses": 3.130027, "test_losses": 3.130027, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006372, "param_norms": 2.936694}, {"epoch": 3350, "train_losses": 3.129819, "test_losses": 3.129819, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006516, "param_norms": 2.964997}, {"epoch": 3400, "train_losses": 3.129602, "test_losses": 3.129602, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006666, "param_norms": 2.994096}, {"epoch": 3450, "train_losses": 3.129375, "test_losses": 3.129375, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006821, "param_norms": 3.024017}, {"epoch": 3500, "train_losses": 3.129137, "test_losses": 3.129137, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.006983, "param_norms": 3.054785}, {"epoch": 3550, "train_losses": 3.128887, "test_losses": 3.128887, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007151, "param_norms": 3.086432}, {"epoch": 3600, "train_losses": 3.128625, "test_losses": 3.128625, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007326, "param_norms": 3.118985}, {"epoch": 3650, "train_losses": 3.12835, "test_losses": 3.12835, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007507, "param_norms": 3.152479}, {"epoch": 3700, "train_losses": 3.128061, "test_losses": 3.128061, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007697, "param_norms": 3.186945}, {"epoch": 3750, "train_losses": 3.127757, "test_losses": 3.127757, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.007894, "param_norms": 3.222421}, {"epoch": 3800, "train_losses": 3.127438, "test_losses": 3.127438, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0081, "param_norms": 3.258944}, {"epoch": 3850, "train_losses": 3.127101, "test_losses": 3.127101, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008314, "param_norms": 3.296554}, {"epoch": 3900, "train_losses": 3.126746, "test_losses": 3.126746, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008538, "param_norms": 3.335292}, {"epoch": 3950, "train_losses": 3.126372, "test_losses": 3.126372, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.008772, "param_norms": 3.375205}, {"epoch": 4000, "train_losses": 3.125977, "test_losses": 3.125977, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009016, "param_norms": 3.416338}, {"epoch": 4050, "train_losses": 3.125559, "test_losses": 3.125559, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009271, "param_norms": 3.458744}, {"epoch": 4100, "train_losses": 3.125117, "test_losses": 3.125117, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009538, "param_norms": 3.502474}, {"epoch": 4150, "train_losses": 3.124649, "test_losses": 3.124649, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.009817, "param_norms": 3.547587}, {"epoch": 4200, "train_losses": 3.124152, "test_losses": 3.124152, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010109, "param_norms": 3.594144}, {"epoch": 4250, "train_losses": 3.123626, "test_losses": 3.123626, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010416, "param_norms": 3.642205}, {"epoch": 4300, "train_losses": 3.123067, "test_losses": 3.123067, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.010737, "param_norms": 3.691845}, {"epoch": 4350, "train_losses": 3.122473, "test_losses": 3.122473, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011074, "param_norms": 3.743132}, {"epoch": 4400, "train_losses": 3.12184, "test_losses": 3.12184, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011428, "param_norms": 3.796146}, {"epoch": 4450, "train_losses": 3.121166, "test_losses": 3.121166, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.011801, "param_norms": 3.85097}, {"epoch": 4500, "train_losses": 3.120447, "test_losses": 3.120447, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012192, "param_norms": 3.907697}, {"epoch": 4550, "train_losses": 3.119678, "test_losses": 3.119678, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.012605, "param_norms": 3.966417}, {"epoch": 4600, "train_losses": 3.118857, "test_losses": 3.118857, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01304, "param_norms": 4.027236}, {"epoch": 4650, "train_losses": 3.117977, "test_losses": 3.117977, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013498, "param_norms": 4.090264}, {"epoch": 4700, "train_losses": 3.117033, "test_losses": 3.117033, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.013983, "param_norms": 4.15562}, {"epoch": 4750, "train_losses": 3.11602, "test_losses": 3.11602, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.014495, "param_norms": 4.223428}, {"epoch": 4800, "train_losses": 3.114931, "test_losses": 3.114931, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.015036, "param_norms": 4.293832}, {"epoch": 4850, "train_losses": 3.113758, "test_losses": 3.113758, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.01561, "param_norms": 4.366976}, {"epoch": 4900, "train_losses": 3.112492, "test_losses": 3.112492, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016219, "param_norms": 4.44302}, {"epoch": 4950, "train_losses": 3.111125, "test_losses": 3.111125, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.016866, "param_norms": 4.522143}, {"epoch": 4999, "train_losses": 3.109675, "test_losses": 3.109675, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.017539, "param_norms": 4.602847}], "total_epochs": 5000}, "relu_single_freq": {"config": {"prime": 23, "d_mlp": 512, "act_type": "ReLU", "init_type": "single-freq", "init_scale": 0.002, "optimizer": "SGD", "lr": 0.01, "weight_decay": 0, "frac_train": 1.0, "num_epochs": 5000, "seed": 42}, "log_text": "======================================================================\nTraining Log: p=23, run=relu_single_freq\n======================================================================\n\nConfiguration:\n  prime (p)       = 23\n  d_mlp           = 512\n  activation      = ReLU\n  init_type       = single-freq\n  init_scale      = 0.002\n  optimizer       = SGD\n  learning_rate   = 0.01\n  weight_decay    = 0\n  frac_train      = 1.0\n  num_epochs      = 5000\n  batch_style     = full\n  seed            = 42\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Epoch    Train Loss     Test Loss   Train Acc    Test Acc   Grad Norm   Param Norm\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       0      3.135498      3.135498      0.0208      0.0208      0.0035       0.2170\n      50      3.135493      3.135493      0.0302      0.0302      0.0035       0.2170\n     100      3.135487      3.135487      0.0416      0.0416      0.0035       0.2170\n     150      3.135480      3.135480      0.0643      0.0643      0.0035       0.2171\n     200      3.135474      3.135474      0.0851      0.0851      0.0035       0.2172\n     250      3.135468      3.135468      0.1078      0.1078      0.0035       0.2173\n     300      3.135462      3.135462      0.1229      0.1229      0.0035       0.2174\n     350      3.135456      3.135456      0.1607      0.1607      0.0035       0.2176\n     400      3.135450      3.135450      0.2023      0.2023      0.0035       0.2178\n     450      3.135443      3.135443      0.2420      0.2420      0.0035       0.2180\n     500      3.135437      3.135437      0.3214      0.3214      0.0035       0.2182\n     550      3.135431      3.135431      0.3951      0.3951      0.0035       0.2185\n     600      3.135425      3.135425      0.4518      0.4518      0.0035       0.2188\n     650      3.135419      3.135419      0.5009      0.5009      0.0035       0.2191\n     700      3.135413      3.135413      0.5633      0.5633      0.0035       0.2195\n     750      3.135406      3.135406      0.6163      0.6163      0.0035       0.2199\n     800      3.135400      3.135400      0.6635      0.6635      0.0035       0.2203\n     850      3.135394      3.135394      0.7202      0.7202      0.0035       0.2207\n     900      3.135388      3.135388      0.7524      0.7524      0.0035       0.2212\n     950      3.135382      3.135382      0.7996      0.7996      0.0035       0.2217\n    1000      3.135376      3.135376      0.8393      0.8393      0.0035       0.2222\n    1050      3.135370      3.135370      0.8715      0.8715      0.0035       0.2228\n    1100      3.135364      3.135364      0.8866      0.8866      0.0035       0.2233\n    1150      3.135358      3.135358      0.8941      0.8941      0.0035       0.2239\n    1200      3.135351      3.135351      0.9263      0.9263      0.0035       0.2246\n    1250      3.135345      3.135345      0.9357      0.9357      0.0035       0.2252\n    1300      3.135339      3.135339      0.9414      0.9414      0.0035       0.2259\n    1350      3.135333      3.135333      0.9527      0.9527      0.0035       0.2266\n    1400      3.135327      3.135327      0.9603      0.9603      0.0035       0.2273\n    1450      3.135321      3.135321      0.9735      0.9735      0.0035       0.2281\n    1500      3.135314      3.135314      0.9773      0.9773      0.0035       0.2288\n    1550      3.135308      3.135308      0.9830      0.9830      0.0036       0.2296\n    1600      3.135302      3.135302      0.9905      0.9905      0.0036       0.2304\n    1650      3.135296      3.135296      0.9905      0.9905      0.0036       0.2313\n    1700      3.135290      3.135290      0.9943      0.9943      0.0036       0.2322\n    1750      3.135284      3.135284      0.9943      0.9943      0.0036       0.2331\n    1800      3.135277      3.135277      0.9943      0.9943      0.0036       0.2340\n    1850      3.135271      3.135271      0.9981      0.9981      0.0036       0.2349\n    1900      3.135265      3.135265      0.9981      0.9981      0.0036       0.2359\n    1950      3.135258      3.135258      1.0000      1.0000      0.0036       0.2369\n    2000      3.135252      3.135252      1.0000      1.0000      0.0036       0.2379\n    2050      3.135246      3.135246      1.0000      1.0000      0.0036       0.2389\n    2100      3.135240      3.135240      1.0000      1.0000      0.0036       0.2399\n    2150      3.135234      3.135234      1.0000      1.0000      0.0036       0.2410\n    2200      3.135227      3.135227      1.0000      1.0000      0.0036       0.2421\n    2250      3.135221      3.135221      1.0000      1.0000      0.0037       0.2432\n    2300      3.135215      3.135215      1.0000      1.0000      0.0037       0.2443\n    2350      3.135208      3.135208      1.0000      1.0000      0.0037       0.2455\n    2400      3.135202      3.135202      1.0000      1.0000      0.0037       0.2467\n    2450      3.135195      3.135195      1.0000      1.0000      0.0037       0.2479\n    2500      3.135189      3.135189      1.0000      1.0000      0.0037       0.2491\n    2550      3.135183      3.135183      1.0000      1.0000      0.0037       0.2503\n    2600      3.135176      3.135176      1.0000      1.0000      0.0038       0.2516\n    2650      3.135170      3.135170      1.0000      1.0000      0.0038       0.2528\n    2700      3.135163      3.135163      1.0000      1.0000      0.0038       0.2541\n    2750      3.135156      3.135156      1.0000      1.0000      0.0038       0.2555\n    2800      3.135150      3.135150      1.0000      1.0000      0.0038       0.2568\n    2850      3.135143      3.135143      1.0000      1.0000      0.0038       0.2581\n    2900      3.135136      3.135136      1.0000      1.0000      0.0038       0.2595\n    2950      3.135129      3.135129      1.0000      1.0000      0.0039       0.2609\n    3000      3.135123      3.135123      1.0000      1.0000      0.0039       0.2623\n    3050      3.135116      3.135116      1.0000      1.0000      0.0039       0.2637\n    3100      3.135109      3.135109      1.0000      1.0000      0.0039       0.2652\n    3150      3.135102      3.135102      1.0000      1.0000      0.0039       0.2666\n    3200      3.135095      3.135095      1.0000      1.0000      0.0040       0.2681\n    3250      3.135088      3.135088      1.0000      1.0000      0.0040       0.2696\n    3300      3.135081      3.135081      1.0000      1.0000      0.0040       0.2711\n    3350      3.135074      3.135074      1.0000      1.0000      0.0040       0.2727\n    3400      3.135067      3.135067      1.0000      1.0000      0.0040       0.2742\n    3450      3.135060      3.135060      1.0000      1.0000      0.0041       0.2758\n    3500      3.135052      3.135052      1.0000      1.0000      0.0041       0.2774\n    3550      3.135045      3.135045      1.0000      1.0000      0.0041       0.2790\n    3600      3.135038      3.135038      1.0000      1.0000      0.0041       0.2806\n    3650      3.135030      3.135030      1.0000      1.0000      0.0041       0.2822\n    3700      3.135023      3.135023      1.0000      1.0000      0.0042       0.2839\n    3750      3.135015      3.135015      1.0000      1.0000      0.0042       0.2855\n    3800      3.135007      3.135007      1.0000      1.0000      0.0042       0.2872\n    3850      3.135000      3.135000      1.0000      1.0000      0.0042       0.2889\n    3900      3.134992      3.134992      1.0000      1.0000      0.0043       0.2906\n    3950      3.134985      3.134985      1.0000      1.0000      0.0043       0.2924\n    4000      3.134977      3.134977      1.0000      1.0000      0.0043       0.2941\n    4050      3.134969      3.134969      1.0000      1.0000      0.0043       0.2959\n    4100      3.134961      3.134961      1.0000      1.0000      0.0044       0.2977\n    4150      3.134953      3.134953      1.0000      1.0000      0.0044       0.2995\n    4200      3.134944      3.134944      1.0000      1.0000      0.0044       0.3013\n    4250      3.134936      3.134936      1.0000      1.0000      0.0044       0.3031\n    4300      3.134928      3.134928      1.0000      1.0000      0.0045       0.3050\n    4350      3.134919      3.134919      1.0000      1.0000      0.0045       0.3068\n    4400      3.134911      3.134911      1.0000      1.0000      0.0045       0.3087\n    4450      3.134902      3.134902      1.0000      1.0000      0.0045       0.3106\n    4500      3.134893      3.134893      1.0000      1.0000      0.0046       0.3125\n    4550      3.134885      3.134885      1.0000      1.0000      0.0046       0.3145\n    4600      3.134876      3.134876      1.0000      1.0000      0.0046       0.3164\n    4650      3.134867      3.134867      1.0000      1.0000      0.0047       0.3184\n    4700      3.134858      3.134858      1.0000      1.0000      0.0047       0.3203\n    4750      3.134849      3.134849      1.0000      1.0000      0.0047       0.3223\n    4800      3.134840      3.134840      1.0000      1.0000      0.0048       0.3244\n    4850      3.134831      3.134831      1.0000      1.0000      0.0048       0.3264\n    4900      3.134821      3.134821      1.0000      1.0000      0.0048       0.3284\n    4950      3.134812      3.134812      1.0000      1.0000      0.0049       0.3305\n    4999      3.134802      3.134802      1.0000      1.0000      0.0049       0.3325\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nFinal Results:\n  Train Loss  = 3.134802\n  Test Loss   = 3.134802\n  Train Acc   = 1.0000\n  Test Acc    = 1.0000\n  Param Norm  = 0.3325\n\nTotal epochs trained: 5000\n", "table": [{"epoch": 0, "train_losses": 3.135498, "test_losses": 3.135498, "train_accs": 0.020794, "test_accs": 0.020794, "grad_norms": 0.003529, "param_norms": 0.217035}, {"epoch": 50, "train_losses": 3.135493, "test_losses": 3.135493, "train_accs": 0.030246, "test_accs": 0.030246, "grad_norms": 0.003524, "param_norms": 0.217029}, {"epoch": 100, "train_losses": 3.135487, "test_losses": 3.135487, "train_accs": 0.041588, "test_accs": 0.041588, "grad_norms": 0.003519, "param_norms": 0.21705}, {"epoch": 150, "train_losses": 3.13548, "test_losses": 3.13548, "train_accs": 0.064272, "test_accs": 0.064272, "grad_norms": 0.003519, "param_norms": 0.217099}, {"epoch": 200, "train_losses": 3.135474, "test_losses": 3.135474, "train_accs": 0.085066, "test_accs": 0.085066, "grad_norms": 0.003517, "param_norms": 0.217176}, {"epoch": 250, "train_losses": 3.135468, "test_losses": 3.135468, "train_accs": 0.10775, "test_accs": 0.10775, "grad_norms": 0.003513, "param_norms": 0.217283}, {"epoch": 300, "train_losses": 3.135462, "test_losses": 3.135462, "train_accs": 0.122873, "test_accs": 0.122873, "grad_norms": 0.00351, "param_norms": 0.217417}, {"epoch": 350, "train_losses": 3.135456, "test_losses": 3.135456, "train_accs": 0.160681, "test_accs": 0.160681, "grad_norms": 0.003507, "param_norms": 0.21758}, {"epoch": 400, "train_losses": 3.13545, "test_losses": 3.13545, "train_accs": 0.202268, "test_accs": 0.202268, "grad_norms": 0.003508, "param_norms": 0.21777}, {"epoch": 450, "train_losses": 3.135443, "test_losses": 3.135443, "train_accs": 0.241966, "test_accs": 0.241966, "grad_norms": 0.003509, "param_norms": 0.217989}, {"epoch": 500, "train_losses": 3.135437, "test_losses": 3.135437, "train_accs": 0.321361, "test_accs": 0.321361, "grad_norms": 0.003511, "param_norms": 0.218236}, {"epoch": 550, "train_losses": 3.135431, "test_losses": 3.135431, "train_accs": 0.395085, "test_accs": 0.395085, "grad_norms": 0.003512, "param_norms": 0.21851}, {"epoch": 600, "train_losses": 3.135425, "test_losses": 3.135425, "train_accs": 0.451796, "test_accs": 0.451796, "grad_norms": 0.003511, "param_norms": 0.218813}, {"epoch": 650, "train_losses": 3.135419, "test_losses": 3.135419, "train_accs": 0.500945, "test_accs": 0.500945, "grad_norms": 0.003511, "param_norms": 0.219143}, {"epoch": 700, "train_losses": 3.135413, "test_losses": 3.135413, "train_accs": 0.563327, "test_accs": 0.563327, "grad_norms": 0.00351, "param_norms": 0.219501}, {"epoch": 750, "train_losses": 3.135406, "test_losses": 3.135406, "train_accs": 0.616257, "test_accs": 0.616257, "grad_norms": 0.00351, "param_norms": 0.219886}, {"epoch": 800, "train_losses": 3.1354, "test_losses": 3.1354, "train_accs": 0.663516, "test_accs": 0.663516, "grad_norms": 0.00351, "param_norms": 0.220298}, {"epoch": 850, "train_losses": 3.135394, "test_losses": 3.135394, "train_accs": 0.720227, "test_accs": 0.720227, "grad_norms": 0.00351, "param_norms": 0.220737}, {"epoch": 900, "train_losses": 3.135388, "test_losses": 3.135388, "train_accs": 0.752363, "test_accs": 0.752363, "grad_norms": 0.003512, "param_norms": 0.221203}, {"epoch": 950, "train_losses": 3.135382, "test_losses": 3.135382, "train_accs": 0.799622, "test_accs": 0.799622, "grad_norms": 0.003513, "param_norms": 0.221696}, {"epoch": 1000, "train_losses": 3.135376, "test_losses": 3.135376, "train_accs": 0.839319, "test_accs": 0.839319, "grad_norms": 0.003515, "param_norms": 0.222215}, {"epoch": 1050, "train_losses": 3.13537, "test_losses": 3.13537, "train_accs": 0.871456, "test_accs": 0.871456, "grad_norms": 0.003517, "param_norms": 0.222761}, {"epoch": 1100, "train_losses": 3.135364, "test_losses": 3.135364, "train_accs": 0.886578, "test_accs": 0.886578, "grad_norms": 0.00352, "param_norms": 0.223333}, {"epoch": 1150, "train_losses": 3.135358, "test_losses": 3.135358, "train_accs": 0.89414, "test_accs": 0.89414, "grad_norms": 0.003521, "param_norms": 0.223931}, {"epoch": 1200, "train_losses": 3.135351, "test_losses": 3.135351, "train_accs": 0.926276, "test_accs": 0.926276, "grad_norms": 0.003526, "param_norms": 0.224554}, {"epoch": 1250, "train_losses": 3.135345, "test_losses": 3.135345, "train_accs": 0.935728, "test_accs": 0.935728, "grad_norms": 0.003527, "param_norms": 0.225203}, {"epoch": 1300, "train_losses": 3.135339, "test_losses": 3.135339, "train_accs": 0.941399, "test_accs": 0.941399, "grad_norms": 0.003529, "param_norms": 0.225878}, {"epoch": 1350, "train_losses": 3.135333, "test_losses": 3.135333, "train_accs": 0.952741, "test_accs": 0.952741, "grad_norms": 0.003531, "param_norms": 0.226577}, {"epoch": 1400, "train_losses": 3.135327, "test_losses": 3.135327, "train_accs": 0.960302, "test_accs": 0.960302, "grad_norms": 0.00354, "param_norms": 0.227302}, {"epoch": 1450, "train_losses": 3.135321, "test_losses": 3.135321, "train_accs": 0.973535, "test_accs": 0.973535, "grad_norms": 0.003544, "param_norms": 0.228051}, {"epoch": 1500, "train_losses": 3.135314, "test_losses": 3.135314, "train_accs": 0.977316, "test_accs": 0.977316, "grad_norms": 0.003548, "param_norms": 0.228825}, {"epoch": 1550, "train_losses": 3.135308, "test_losses": 3.135308, "train_accs": 0.982987, "test_accs": 0.982987, "grad_norms": 0.003553, "param_norms": 0.229623}, {"epoch": 1600, "train_losses": 3.135302, "test_losses": 3.135302, "train_accs": 0.990548, "test_accs": 0.990548, "grad_norms": 0.003558, "param_norms": 0.230445}, {"epoch": 1650, "train_losses": 3.135296, "test_losses": 3.135296, "train_accs": 0.990548, "test_accs": 0.990548, "grad_norms": 0.003564, "param_norms": 0.23129}, {"epoch": 1700, "train_losses": 3.13529, "test_losses": 3.13529, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.003568, "param_norms": 0.232159}, {"epoch": 1750, "train_losses": 3.135284, "test_losses": 3.135284, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.003573, "param_norms": 0.233052}, {"epoch": 1800, "train_losses": 3.135277, "test_losses": 3.135277, "train_accs": 0.994329, "test_accs": 0.994329, "grad_norms": 0.003578, "param_norms": 0.233967}, {"epoch": 1850, "train_losses": 3.135271, "test_losses": 3.135271, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.003583, "param_norms": 0.234906}, {"epoch": 1900, "train_losses": 3.135265, "test_losses": 3.135265, "train_accs": 0.99811, "test_accs": 0.99811, "grad_norms": 0.003592, "param_norms": 0.235867}, {"epoch": 1950, "train_losses": 3.135258, "test_losses": 3.135258, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003597, "param_norms": 0.23685}, {"epoch": 2000, "train_losses": 3.135252, "test_losses": 3.135252, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003611, "param_norms": 0.237856}, {"epoch": 2050, "train_losses": 3.135246, "test_losses": 3.135246, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003623, "param_norms": 0.238883}, {"epoch": 2100, "train_losses": 3.13524, "test_losses": 3.13524, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003633, "param_norms": 0.239932}, {"epoch": 2150, "train_losses": 3.135234, "test_losses": 3.135234, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003643, "param_norms": 0.241003}, {"epoch": 2200, "train_losses": 3.135227, "test_losses": 3.135227, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00365, "param_norms": 0.242095}, {"epoch": 2250, "train_losses": 3.135221, "test_losses": 3.135221, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00366, "param_norms": 0.243208}, {"epoch": 2300, "train_losses": 3.135215, "test_losses": 3.135215, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003676, "param_norms": 0.244341}, {"epoch": 2350, "train_losses": 3.135208, "test_losses": 3.135208, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003687, "param_norms": 0.245496}, {"epoch": 2400, "train_losses": 3.135202, "test_losses": 3.135202, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.0037, "param_norms": 0.246671}, {"epoch": 2450, "train_losses": 3.135195, "test_losses": 3.135195, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00371, "param_norms": 0.247866}, {"epoch": 2500, "train_losses": 3.135189, "test_losses": 3.135189, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003725, "param_norms": 0.249081}, {"epoch": 2550, "train_losses": 3.135183, "test_losses": 3.135183, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003741, "param_norms": 0.250316}, {"epoch": 2600, "train_losses": 3.135176, "test_losses": 3.135176, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003757, "param_norms": 0.251571}, {"epoch": 2650, "train_losses": 3.13517, "test_losses": 3.13517, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003765, "param_norms": 0.252846}, {"epoch": 2700, "train_losses": 3.135163, "test_losses": 3.135163, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003783, "param_norms": 0.25414}, {"epoch": 2750, "train_losses": 3.135156, "test_losses": 3.135156, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003794, "param_norms": 0.255453}, {"epoch": 2800, "train_losses": 3.13515, "test_losses": 3.13515, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003806, "param_norms": 0.256786}, {"epoch": 2850, "train_losses": 3.135143, "test_losses": 3.135143, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003825, "param_norms": 0.258137}, {"epoch": 2900, "train_losses": 3.135136, "test_losses": 3.135136, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003843, "param_norms": 0.259507}, {"epoch": 2950, "train_losses": 3.135129, "test_losses": 3.135129, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003864, "param_norms": 0.260895}, {"epoch": 3000, "train_losses": 3.135123, "test_losses": 3.135123, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003882, "param_norms": 0.262303}, {"epoch": 3050, "train_losses": 3.135116, "test_losses": 3.135116, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003898, "param_norms": 0.263728}, {"epoch": 3100, "train_losses": 3.135109, "test_losses": 3.135109, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003921, "param_norms": 0.265171}, {"epoch": 3150, "train_losses": 3.135102, "test_losses": 3.135102, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003939, "param_norms": 0.266633}, {"epoch": 3200, "train_losses": 3.135095, "test_losses": 3.135095, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003963, "param_norms": 0.268112}, {"epoch": 3250, "train_losses": 3.135088, "test_losses": 3.135088, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.003982, "param_norms": 0.269609}, {"epoch": 3300, "train_losses": 3.135081, "test_losses": 3.135081, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004003, "param_norms": 0.271124}, {"epoch": 3350, "train_losses": 3.135074, "test_losses": 3.135074, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004019, "param_norms": 0.272656}, {"epoch": 3400, "train_losses": 3.135067, "test_losses": 3.135067, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004041, "param_norms": 0.274206}, {"epoch": 3450, "train_losses": 3.13506, "test_losses": 3.13506, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00406, "param_norms": 0.275773}, {"epoch": 3500, "train_losses": 3.135052, "test_losses": 3.135052, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004082, "param_norms": 0.277357}, {"epoch": 3550, "train_losses": 3.135045, "test_losses": 3.135045, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004106, "param_norms": 0.278959}, {"epoch": 3600, "train_losses": 3.135038, "test_losses": 3.135038, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004124, "param_norms": 0.280577}, {"epoch": 3650, "train_losses": 3.13503, "test_losses": 3.13503, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00415, "param_norms": 0.282212}, {"epoch": 3700, "train_losses": 3.135023, "test_losses": 3.135023, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004172, "param_norms": 0.283865}, {"epoch": 3750, "train_losses": 3.135015, "test_losses": 3.135015, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004191, "param_norms": 0.285534}, {"epoch": 3800, "train_losses": 3.135007, "test_losses": 3.135007, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004215, "param_norms": 0.287219}, {"epoch": 3850, "train_losses": 3.135, "test_losses": 3.135, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004245, "param_norms": 0.288922}, {"epoch": 3900, "train_losses": 3.134992, "test_losses": 3.134992, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.00426, "param_norms": 0.290641}, {"epoch": 3950, "train_losses": 3.134985, "test_losses": 3.134985, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004289, "param_norms": 0.292376}, {"epoch": 4000, "train_losses": 3.134977, "test_losses": 3.134977, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004315, "param_norms": 0.294128}, {"epoch": 4050, "train_losses": 3.134969, "test_losses": 3.134969, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004339, "param_norms": 0.295896}, {"epoch": 4100, "train_losses": 3.134961, "test_losses": 3.134961, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004362, "param_norms": 0.29768}, {"epoch": 4150, "train_losses": 3.134953, "test_losses": 3.134953, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004387, "param_norms": 0.299481}, {"epoch": 4200, "train_losses": 3.134944, "test_losses": 3.134944, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004414, "param_norms": 0.301298}, {"epoch": 4250, "train_losses": 3.134936, "test_losses": 3.134936, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004435, "param_norms": 0.303131}, {"epoch": 4300, "train_losses": 3.134928, "test_losses": 3.134928, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004463, "param_norms": 0.30498}, {"epoch": 4350, "train_losses": 3.134919, "test_losses": 3.134919, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004489, "param_norms": 0.306845}, {"epoch": 4400, "train_losses": 3.134911, "test_losses": 3.134911, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004515, "param_norms": 0.308726}, {"epoch": 4450, "train_losses": 3.134902, "test_losses": 3.134902, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004543, "param_norms": 0.310623}, {"epoch": 4500, "train_losses": 3.134893, "test_losses": 3.134893, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004571, "param_norms": 0.312536}, {"epoch": 4550, "train_losses": 3.134885, "test_losses": 3.134885, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004602, "param_norms": 0.314465}, {"epoch": 4600, "train_losses": 3.134876, "test_losses": 3.134876, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004631, "param_norms": 0.31641}, {"epoch": 4650, "train_losses": 3.134867, "test_losses": 3.134867, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004665, "param_norms": 0.318371}, {"epoch": 4700, "train_losses": 3.134858, "test_losses": 3.134858, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004686, "param_norms": 0.320348}, {"epoch": 4750, "train_losses": 3.134849, "test_losses": 3.134849, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004727, "param_norms": 0.322341}, {"epoch": 4800, "train_losses": 3.13484, "test_losses": 3.13484, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004753, "param_norms": 0.32435}, {"epoch": 4850, "train_losses": 3.134831, "test_losses": 3.134831, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004784, "param_norms": 0.326375}, {"epoch": 4900, "train_losses": 3.134821, "test_losses": 3.134821, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004813, "param_norms": 0.328416}, {"epoch": 4950, "train_losses": 3.134812, "test_losses": 3.134812, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004854, "param_norms": 0.330473}, {"epoch": 4999, "train_losses": 3.134802, "test_losses": 3.134802, "train_accs": 1.0, "test_accs": 1.0, "grad_norms": 0.004884, "param_norms": 0.332504}], "total_epochs": 5000}}